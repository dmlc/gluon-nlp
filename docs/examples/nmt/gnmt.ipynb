{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google NMT on IWSLT 2015 English-Vietnamese Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we going to train Google NMT on IWSLT 2015 English-Vietnamese Dataset. The building prcoess includes: 1) load and process dataset and 2) create sampler and DataLoader and 3) build model and 4) write training epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MXNET and Gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import io\n",
    "import logging\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "from mxnet.gluon.data import ArrayDataset, SimpleDataset\n",
    "from mxnet.gluon.data import DataLoader\n",
    "import gluonnlp.data.batchify as btf\n",
    "from gluonnlp.data import ExpWidthBucket, FixedBucketSampler, IWSLT2015\n",
    "from gluonnlp.model import BeamSearchScorer\n",
    "from scripts.nmt.gnmt import get_gnmt_encoder_decoder\n",
    "from scripts.nmt.translation import NMTModel, BeamSearchTranslator\n",
    "from scripts.nmt.loss import SoftmaxCEMaskedLoss\n",
    "from scripts.nmt.utils import logging_config\n",
    "from scripts.nmt.bleu import compute_bleu\n",
    "import scripts.nmt._constants as _C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Logs will be saved to gnmt_en_vi_u512/<ipython-input-2-c89b0965bf79>.log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gnmt_en_vi_u512'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "mx.random.seed(10000)\n",
    "ctx = mx.gpu(0)\n",
    "\n",
    "# parameters for dataset\n",
    "dataset = 'IWSLT2015'\n",
    "src_lang = 'en'\n",
    "tgt_lang = 'vi'\n",
    "src_max_len = 50\n",
    "tgt_max_len = 50\n",
    "\n",
    "# parameters for model\n",
    "num_hidden = 512\n",
    "num_layers = 2\n",
    "num_bi_layers = 1\n",
    "dropout = 0.2\n",
    "\n",
    "# parameters for training\n",
    "batch_size = 128\n",
    "test_batch_size = 32\n",
    "num_buckets = 5\n",
    "epochs = 2\n",
    "clip = 5\n",
    "lr = 0.001\n",
    "lr_update_factor = 0.5\n",
    "log_interval = 10\n",
    "save_dir = 'gnmt_en_vi_u512'\n",
    "\n",
    "#parameters for testing\n",
    "beam_size = 10\n",
    "lp_alpha = 1.0\n",
    "lp_k = 5\n",
    "\n",
    "logging_config(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shows how to process the dataset and cache the processed dataset for the future use. The processing steps include: 1) clip the source and target sequences and 2) split the string input to a list of tokens and 3) map the string token into its index in the vocabulary and 4) append EOS token to source sentence and add BOS and EOS tokens to target sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load cached data from /home/ubuntu/gluon-nlp-1/scripts/nmt/cached/IWSLT2015_en_vi_50_50_train.npz\n",
      "Load cached data from /home/ubuntu/gluon-nlp-1/scripts/nmt/cached/IWSLT2015_en_vi_50_50_val.npz\n",
      "Load cached data from /home/ubuntu/gluon-nlp-1/scripts/nmt/cached/IWSLT2015_en_vi_50_50_test.npz\n"
     ]
    }
   ],
   "source": [
    "def cache_dataset(dataset, prefix):\n",
    "    \"\"\"Cache the processed npy dataset  the dataset into a npz\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : SimpleDataset\n",
    "    file_path : str\n",
    "    \"\"\"\n",
    "    if not os.path.exists(_C.CACHE_PATH):\n",
    "        os.makedirs(_C.CACHE_PATH)\n",
    "    src_data = np.array([ele[0] for ele in dataset])\n",
    "    tgt_data = np.array([ele[1] for ele in dataset])\n",
    "    np.savez(os.path.join(_C.CACHE_PATH, prefix + '.npz'), src_data=src_data, tgt_data=tgt_data)\n",
    "\n",
    "\n",
    "def load_cached_dataset(prefix):\n",
    "    cached_file_path = os.path.join(_C.CACHE_PATH, prefix + '.npz')\n",
    "    if os.path.exists(cached_file_path):\n",
    "        print('Load cached data from {}'.format(cached_file_path))\n",
    "        dat = np.load(cached_file_path)\n",
    "        return ArrayDataset(np.array(dat['src_data']), np.array(dat['tgt_data']))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "class TrainValDataTransform(object):\n",
    "    \"\"\"Transform the machine translation dataset.\n",
    "\n",
    "    Clip source and the target sentences to the maximum length. For the source sentence, append the\n",
    "    EOS. For the target sentence, append BOS and EOS.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    src_vocab : Vocab\n",
    "    tgt_vocab : Vocab\n",
    "    src_max_len : int\n",
    "    tgt_max_len : int\n",
    "    \"\"\"\n",
    "    def __init__(self, src_vocab, tgt_vocab, src_max_len, tgt_max_len):\n",
    "        self._src_vocab = src_vocab\n",
    "        self._tgt_vocab = tgt_vocab\n",
    "        self._src_max_len = src_max_len\n",
    "        self._tgt_max_len = tgt_max_len\n",
    "\n",
    "    def __call__(self, src, tgt):\n",
    "        if self._src_max_len > 0:\n",
    "            src_sentence = self._src_vocab[src.split()[:self._src_max_len]]\n",
    "        else:\n",
    "            src_sentence = self._src_vocab[src.split()]\n",
    "        if self._tgt_max_len > 0:\n",
    "            tgt_sentence = self._tgt_vocab[tgt.split()[:self._tgt_max_len]]\n",
    "        else:\n",
    "            tgt_sentence = self._tgt_vocab[tgt.split()]\n",
    "        src_sentence.append(self._src_vocab[self._src_vocab.eos_token])\n",
    "        tgt_sentence.insert(0, self._tgt_vocab[self._tgt_vocab.bos_token])\n",
    "        tgt_sentence.append(self._tgt_vocab[self._tgt_vocab.eos_token])\n",
    "        src_npy = np.array(src_sentence, dtype=np.int32)\n",
    "        tgt_npy = np.array(tgt_sentence, dtype=np.int32)\n",
    "        return src_npy, tgt_npy\n",
    "\n",
    "\n",
    "def process_dataset(dataset, src_vocab, tgt_vocab, src_max_len=-1, tgt_max_len=-1):\n",
    "    start = time.time()\n",
    "    dataset_processed = dataset.transform(TrainValDataTransform(src_vocab, tgt_vocab,\n",
    "                                                                src_max_len,\n",
    "                                                                tgt_max_len), lazy=False)\n",
    "    end = time.time()\n",
    "    print('Processing Time spent: {}'.format(end - start))\n",
    "    return dataset_processed\n",
    "\n",
    "\n",
    "def load_translation_data(dataset, src_lang='en', tgt_lang='vi'):\n",
    "    \"\"\"Load translation dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : str\n",
    "    src_lang : str, default 'en'\n",
    "    tgt_lang : str, default 'vi'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    common_prefix = 'IWSLT2015_{}_{}_{}_{}'.format(src_lang, tgt_lang,\n",
    "                                                   src_max_len, tgt_max_len)\n",
    "    data_train = IWSLT2015('train', src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "    data_val = IWSLT2015('val', src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "    data_test = IWSLT2015('test', src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "    src_vocab, tgt_vocab = data_train.src_vocab, data_train.tgt_vocab\n",
    "    data_train_processed = load_cached_dataset(common_prefix + '_train')\n",
    "    if not data_train_processed:\n",
    "        data_train_processed = process_dataset(data_train, src_vocab, tgt_vocab,\n",
    "                                               src_max_len, tgt_max_len)\n",
    "        cache_dataset(data_train_processed, common_prefix + '_train')\n",
    "    data_val_processed = load_cached_dataset(common_prefix + '_val')\n",
    "    if not data_val_processed:\n",
    "        data_val_processed = process_dataset(data_val, src_vocab, tgt_vocab)\n",
    "        cache_dataset(data_val_processed, common_prefix + '_val')\n",
    "    data_test_processed = load_cached_dataset(common_prefix + '_test')\n",
    "    if not data_test_processed:\n",
    "        data_test_processed = process_dataset(data_test, src_vocab, tgt_vocab)\n",
    "        cache_dataset(data_test_processed, common_prefix + '_test')\n",
    "    fetch_tgt_sentence = lambda src, tgt: tgt.split()\n",
    "    val_tgt_sentences = list(data_val.transform(fetch_tgt_sentence))\n",
    "    test_tgt_sentences = list(data_test.transform(fetch_tgt_sentence))\n",
    "    return data_train_processed, data_val_processed, data_test_processed, \\\n",
    "           val_tgt_sentences, test_tgt_sentences, src_vocab, tgt_vocab\n",
    "\n",
    "\n",
    "def get_data_lengths(dataset):\n",
    "    return list(dataset.transform(lambda srg, tgt: (len(srg), len(tgt))))\n",
    "\n",
    "\n",
    "data_train, data_val, data_test, val_tgt_sentences, test_tgt_sentences, src_vocab, tgt_vocab\\\n",
    "    = load_translation_data(dataset=dataset, src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "data_train_lengths = get_data_lengths(data_train)\n",
    "data_val_lengths = get_data_lengths(data_val)\n",
    "data_test_lengths = get_data_lengths(data_test)\n",
    "\n",
    "with io.open(os.path.join(save_dir, 'val_gt.txt'), 'w', encoding='utf-8') as of:\n",
    "    for ele in val_tgt_sentences:\n",
    "        of.write(' '.join(ele) + '\\n')\n",
    "\n",
    "with io.open(os.path.join(save_dir, 'test_gt.txt'), 'w', encoding='utf-8') as of:\n",
    "    for ele in test_tgt_sentences:\n",
    "        of.write(' '.join(ele) + '\\n')\n",
    "\n",
    "\n",
    "data_train = data_train.transform(lambda src, tgt: (src, tgt, len(src), len(tgt)), lazy=False)\n",
    "data_val = SimpleDataset([(ele[0], ele[1], len(ele[0]), len(ele[1]), i)\n",
    "                          for i, ele in enumerate(data_val)])\n",
    "data_test = SimpleDataset([(ele[0], ele[1], len(ele[0]), len(ele[1]), i)\n",
    "                           for i, ele in enumerate(data_test)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sampler and DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have obtained `data_train`, `data_val`, and `data_test`. The next step is to construct sampler and DataLoader. The first step is to construct batchify function, which pads and stacks sequences to form mini-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batchify_fn = btf.Tuple(btf.Pad(), btf.Pad(),\n",
    "                              btf.Stack(dtype='float32'), btf.Stack(dtype='float32'))\n",
    "test_batchify_fn = btf.Tuple(btf.Pad(), btf.Pad(),\n",
    "                             btf.Stack(dtype='float32'), btf.Stack(dtype='float32'),\n",
    "                             btf.Stack())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then construct bucketing samplers, which generate batches by grouping sequences with similar lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 00:35:50,872 - root - Train Batch Sampler:\n",
      "FixedBucketSampler:\n",
      "  sample_num=133166, batch_num=1043\n",
      "  key=[(9, 10), (16, 17), (26, 27), (37, 38), (51, 52)]\n",
      "  cnt=[11414, 34897, 37760, 23480, 25615]\n",
      "  batch_size=[128, 128, 128, 128, 128]\n",
      "2018-08-09 00:35:50,875 - root - Valid Batch Sampler:\n",
      "FixedBucketSampler:\n",
      "  sample_num=1553, batch_num=51\n",
      "  key=[(15, 16), (24, 25), (33, 34), (42, 43), (51, 52)]\n",
      "  cnt=[511, 484, 266, 153, 139]\n",
      "  batch_size=[32, 32, 32, 32, 32]\n",
      "2018-08-09 00:35:50,877 - root - Test Batch Sampler:\n",
      "FixedBucketSampler:\n",
      "  sample_num=1268, batch_num=42\n",
      "  key=[(15, 16), (24, 25), (33, 34), (42, 43), (51, 52)]\n",
      "  cnt=[338, 334, 235, 145, 216]\n",
      "  batch_size=[32, 32, 32, 32, 32]\n"
     ]
    }
   ],
   "source": [
    "train_batch_sampler = FixedBucketSampler(lengths=data_train_lengths,\n",
    "                                             batch_size=batch_size,\n",
    "                                             num_buckets=num_buckets,\n",
    "                                             shuffle=True,\n",
    "                                             bucket_scheme=ExpWidthBucket(bucket_len_step=1.2))\n",
    "logging.info('Train Batch Sampler:\\n{}'.format(train_batch_sampler.stats()))\n",
    "val_batch_sampler = FixedBucketSampler(lengths=data_val_lengths,\n",
    "                                       batch_size=test_batch_size,\n",
    "                                       num_buckets=num_buckets,\n",
    "                                       shuffle=False)\n",
    "logging.info('Valid Batch Sampler:\\n{}'.format(val_batch_sampler.stats()))\n",
    "test_batch_sampler = FixedBucketSampler(lengths=data_test_lengths,\n",
    "                                        batch_size=test_batch_size,\n",
    "                                        num_buckets=num_buckets,\n",
    "                                        shuffle=False)\n",
    "logging.info('Test Batch Sampler:\\n{}'.format(test_batch_sampler.stats()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the samplers, we can create DataLoader, which is iterable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(data_train,\n",
    "                               batch_sampler=train_batch_sampler,\n",
    "                               batchify_fn=train_batchify_fn,\n",
    "                               num_workers=4)\n",
    "val_data_loader = DataLoader(data_val,\n",
    "                             batch_sampler=val_batch_sampler,\n",
    "                             batchify_fn=test_batchify_fn,\n",
    "                             num_workers=4)\n",
    "test_data_loader = DataLoader(data_test,\n",
    "                              batch_sampler=test_batch_sampler,\n",
    "                              batchify_fn=test_batchify_fn,\n",
    "                              num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build GNMT Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After obtaining DataLoader, we can build the model. The GNTM encoder and decoder can be easily obtained by calling `get_gnmt_encoder_decoder` function. Then, we feed encoder and decoder to `NMTModel` to construct the GNMT model. `model.hybridize` allows computation to be done using symbolic backend. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 00:36:01,295 - root - NMTModel(\n",
      "  (src_embed): HybridSequential(\n",
      "    (0): Embedding(17191 -> 512, float32)\n",
      "    (1): Dropout(p = 0.0, axes=())\n",
      "  )\n",
      "  (decoder): GNMTDecoder(\n",
      "    (rnn_cells): HybridSequential(\n",
      "      (0): LSTMCell(None -> 2048)\n",
      "      (1): LSTMCell(None -> 2048)\n",
      "    )\n",
      "    (attention_cell): DotProductAttentionCell(\n",
      "      (_proj_query): Dense(None -> 512, linear)\n",
      "      (_dropout_layer): Dropout(p = 0.0, axes=())\n",
      "    )\n",
      "    (dropout_layer): Dropout(p = 0.2, axes=())\n",
      "  )\n",
      "  (tgt_embed): HybridSequential(\n",
      "    (0): Embedding(7709 -> 512, float32)\n",
      "    (1): Dropout(p = 0.0, axes=())\n",
      "  )\n",
      "  (tgt_proj): Dense(None -> 7709, linear)\n",
      "  (encoder): GNMTEncoder(\n",
      "    (rnn_cells): HybridSequential(\n",
      "      (0): BidirectionalCell(forward=LSTMCell(None -> 2048), backward=LSTMCell(None -> 2048))\n",
      "      (1): LSTMCell(None -> 2048)\n",
      "    )\n",
      "    (dropout_layer): Dropout(p = 0.2, axes=())\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "encoder, decoder = get_gnmt_encoder_decoder(hidden_size=num_hidden,\n",
    "                                            dropout=dropout,\n",
    "                                            num_layers=num_layers,\n",
    "                                            num_bi_layers=num_bi_layers)\n",
    "model = NMTModel(src_vocab=src_vocab, tgt_vocab=tgt_vocab, encoder=encoder, decoder=decoder,\n",
    "                 embed_size=num_hidden, prefix='gnmt_')\n",
    "model.initialize(init=mx.init.Uniform(0.1), ctx=ctx)\n",
    "static_alloc = True\n",
    "model.hybridize(static_alloc=static_alloc)\n",
    "logging.info(model)\n",
    "\n",
    "loss_function = SoftmaxCEMaskedLoss()\n",
    "loss_function.hybridize(static_alloc=static_alloc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also build the translator the beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 00:36:05,021 - root - Use beam_size=10, alpha=1.0, K=5\n"
     ]
    }
   ],
   "source": [
    "translator = BeamSearchTranslator(model=model, beam_size=beam_size,\n",
    "                                  scorer=BeamSearchScorer(alpha=lp_alpha,\n",
    "                                                          K=lp_k),\n",
    "                                  max_length=tgt_max_len + 100)\n",
    "logging.info('Use beam_size={}, alpha={}, K={}'.format(beam_size, lp_alpha, lp_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define evaluation function as follows. The `evaluate` function use beam search translator to generate outputs for the validation and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader):\n",
    "    \"\"\"Evaluate given the data loader\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_loader : DataLoader\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    avg_loss : float\n",
    "        Average loss\n",
    "    real_translation_out : list of list of str\n",
    "        The translation output\n",
    "    \"\"\"\n",
    "    translation_out = []\n",
    "    all_inst_ids = []\n",
    "    avg_loss_denom = 0\n",
    "    avg_loss = 0.0\n",
    "    for _, (src_seq, tgt_seq, src_valid_length, tgt_valid_length, inst_ids) \\\n",
    "            in enumerate(data_loader):\n",
    "        src_seq = src_seq.as_in_context(ctx)\n",
    "        tgt_seq = tgt_seq.as_in_context(ctx)\n",
    "        src_valid_length = src_valid_length.as_in_context(ctx)\n",
    "        tgt_valid_length = tgt_valid_length.as_in_context(ctx)\n",
    "        # Calculating Loss\n",
    "        out, _ = model(src_seq, tgt_seq[:, :-1], src_valid_length, tgt_valid_length - 1)\n",
    "        loss = loss_function(out, tgt_seq[:, 1:], tgt_valid_length - 1).mean().asscalar()\n",
    "        all_inst_ids.extend(inst_ids.asnumpy().astype(np.int32).tolist())\n",
    "        avg_loss += loss * (tgt_seq.shape[1] - 1)\n",
    "        avg_loss_denom += (tgt_seq.shape[1] - 1)\n",
    "        # Translate\n",
    "        samples, _, sample_valid_length =\\\n",
    "            translator.translate(src_seq=src_seq, src_valid_length=src_valid_length)\n",
    "        max_score_sample = samples[:, 0, :].asnumpy()\n",
    "        sample_valid_length = sample_valid_length[:, 0].asnumpy()\n",
    "        for i in range(max_score_sample.shape[0]):\n",
    "            translation_out.append(\n",
    "                [tgt_vocab.idx_to_token[ele] for ele in\n",
    "                 max_score_sample[i][1:(sample_valid_length[i] - 1)]])\n",
    "    avg_loss = avg_loss / avg_loss_denom\n",
    "    real_translation_out = [None for _ in range(len(all_inst_ids))]\n",
    "    for ind, sentence in zip(all_inst_ids, translation_out):\n",
    "        real_translation_out[ind] = sentence\n",
    "    return avg_loss, real_translation_out\n",
    "\n",
    "\n",
    "def write_sentences(sentences, file_path):\n",
    "    with io.open(file_path, 'w', encoding='utf-8') as of:\n",
    "        for sent in sentences:\n",
    "            of.write(' '.join(sent) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before entering the training stage, we need to create trainer for updating the parameter. In the following example, we create a trainer that uses ADAM optimzier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(model.collect_params(), 'adam', {'learning_rate': lr})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then write the training iteration. During the training, we perform the one evaluation on validation and testing dataset every epoch, and record the parameters that give the hightest BLEU score on validation dataset. Before performing forward and backward, we first use `as_in_context` function to copy the mini-batch to GPU. The statement `with mx.autograd.record()` tell Gluon backend to compute the gradients for the part inside the block. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 00:36:17,893 - root - [Epoch 0 Batch 10/1043] loss=7.7375, ppl=2292.6596, gnorm=1.4907, throughput=13.26K wps, wc=54.27K\n",
      "2018-08-09 00:36:20,161 - root - [Epoch 0 Batch 20/1043] loss=6.3590, ppl=577.6419, gnorm=1.5744, throughput=22.16K wps, wc=50.20K\n",
      "2018-08-09 00:36:22,992 - root - [Epoch 0 Batch 30/1043] loss=6.3708, ppl=584.5321, gnorm=0.8044, throughput=23.95K wps, wc=67.78K\n",
      "2018-08-09 00:36:25,673 - root - [Epoch 0 Batch 40/1043] loss=6.1795, ppl=482.7611, gnorm=0.6211, throughput=23.58K wps, wc=63.19K\n",
      "2018-08-09 00:36:28,401 - root - [Epoch 0 Batch 50/1043] loss=6.1892, ppl=487.4410, gnorm=0.4055, throughput=22.71K wps, wc=61.93K\n",
      "2018-08-09 00:36:30,953 - root - [Epoch 0 Batch 60/1043] loss=6.1059, ppl=448.5020, gnorm=0.6838, throughput=23.21K wps, wc=59.19K\n",
      "2018-08-09 00:36:34,068 - root - [Epoch 0 Batch 70/1043] loss=6.1564, ppl=471.7430, gnorm=0.4561, throughput=23.44K wps, wc=72.99K\n",
      "2018-08-09 00:36:36,870 - root - [Epoch 0 Batch 80/1043] loss=6.0698, ppl=432.6081, gnorm=0.4128, throughput=23.05K wps, wc=64.58K\n",
      "2018-08-09 00:36:39,250 - root - [Epoch 0 Batch 90/1043] loss=5.9379, ppl=379.1364, gnorm=0.3576, throughput=22.29K wps, wc=53.02K\n",
      "2018-08-09 00:36:41,760 - root - [Epoch 0 Batch 100/1043] loss=5.8766, ppl=356.5934, gnorm=0.3973, throughput=23.69K wps, wc=59.42K\n",
      "2018-08-09 00:36:44,462 - root - [Epoch 0 Batch 110/1043] loss=5.8715, ppl=354.7781, gnorm=0.3595, throughput=24.25K wps, wc=65.50K\n",
      "2018-08-09 00:36:47,020 - root - [Epoch 0 Batch 120/1043] loss=5.8697, ppl=354.1486, gnorm=0.3507, throughput=22.85K wps, wc=58.43K\n",
      "2018-08-09 00:36:49,878 - root - [Epoch 0 Batch 130/1043] loss=5.9376, ppl=379.0390, gnorm=0.3592, throughput=20.79K wps, wc=59.39K\n",
      "2018-08-09 00:36:52,548 - root - [Epoch 0 Batch 140/1043] loss=5.8872, ppl=360.3881, gnorm=0.2931, throughput=22.93K wps, wc=61.18K\n",
      "2018-08-09 00:36:55,078 - root - [Epoch 0 Batch 150/1043] loss=5.8202, ppl=337.0510, gnorm=0.2910, throughput=22.28K wps, wc=56.34K\n",
      "2018-08-09 00:36:57,781 - root - [Epoch 0 Batch 160/1043] loss=5.7485, ppl=313.7332, gnorm=0.3798, throughput=21.44K wps, wc=57.93K\n",
      "2018-08-09 00:37:00,579 - root - [Epoch 0 Batch 170/1043] loss=5.7716, ppl=321.0453, gnorm=0.2964, throughput=23.03K wps, wc=64.42K\n",
      "2018-08-09 00:37:02,683 - root - [Epoch 0 Batch 180/1043] loss=5.4653, ppl=236.3463, gnorm=0.3290, throughput=21.08K wps, wc=44.31K\n",
      "2018-08-09 00:37:05,500 - root - [Epoch 0 Batch 190/1043] loss=5.6214, ppl=276.2662, gnorm=0.3634, throughput=22.18K wps, wc=62.45K\n",
      "2018-08-09 00:37:07,959 - root - [Epoch 0 Batch 200/1043] loss=5.5123, ppl=247.7112, gnorm=0.3481, throughput=22.07K wps, wc=54.24K\n",
      "2018-08-09 00:37:10,357 - root - [Epoch 0 Batch 210/1043] loss=5.3340, ppl=207.2624, gnorm=0.4326, throughput=21.98K wps, wc=52.68K\n",
      "2018-08-09 00:37:12,690 - root - [Epoch 0 Batch 220/1043] loss=5.3151, ppl=203.3878, gnorm=0.3865, throughput=21.64K wps, wc=50.47K\n",
      "2018-08-09 00:37:15,368 - root - [Epoch 0 Batch 230/1043] loss=5.3216, ppl=204.7110, gnorm=0.4554, throughput=23.03K wps, wc=61.65K\n",
      "2018-08-09 00:37:17,874 - root - [Epoch 0 Batch 240/1043] loss=5.3135, ppl=203.0627, gnorm=0.3558, throughput=23.28K wps, wc=58.32K\n",
      "2018-08-09 00:37:20,903 - root - [Epoch 0 Batch 250/1043] loss=5.4301, ppl=228.1620, gnorm=0.2876, throughput=23.40K wps, wc=70.84K\n",
      "2018-08-09 00:37:23,539 - root - [Epoch 0 Batch 260/1043] loss=5.2831, ppl=196.9847, gnorm=0.3609, throughput=22.86K wps, wc=60.22K\n",
      "2018-08-09 00:37:26,638 - root - [Epoch 0 Batch 270/1043] loss=5.3842, ppl=217.9430, gnorm=0.2854, throughput=23.55K wps, wc=72.96K\n",
      "2018-08-09 00:37:29,370 - root - [Epoch 0 Batch 280/1043] loss=5.2570, ppl=191.9062, gnorm=0.2639, throughput=22.26K wps, wc=60.80K\n",
      "2018-08-09 00:37:31,493 - root - [Epoch 0 Batch 290/1043] loss=4.9827, ppl=145.8668, gnorm=0.3416, throughput=21.58K wps, wc=45.79K\n",
      "2018-08-09 00:37:33,990 - root - [Epoch 0 Batch 300/1043] loss=5.0931, ppl=162.8862, gnorm=0.3531, throughput=23.66K wps, wc=59.05K\n",
      "2018-08-09 00:37:36,621 - root - [Epoch 0 Batch 310/1043] loss=5.1109, ppl=165.8201, gnorm=0.2904, throughput=23.42K wps, wc=61.58K\n",
      "2018-08-09 00:37:38,982 - root - [Epoch 0 Batch 320/1043] loss=4.9833, ppl=145.9487, gnorm=0.2993, throughput=22.50K wps, wc=53.10K\n",
      "2018-08-09 00:37:41,612 - root - [Epoch 0 Batch 330/1043] loss=5.0428, ppl=154.9032, gnorm=0.2951, throughput=23.34K wps, wc=61.37K\n",
      "2018-08-09 00:37:44,170 - root - [Epoch 0 Batch 340/1043] loss=5.0641, ppl=158.2321, gnorm=0.2660, throughput=22.24K wps, wc=56.88K\n",
      "2018-08-09 00:37:46,579 - root - [Epoch 0 Batch 350/1043] loss=4.9226, ppl=137.3587, gnorm=0.3184, throughput=22.79K wps, wc=54.86K\n",
      "2018-08-09 00:37:49,268 - root - [Epoch 0 Batch 360/1043] loss=5.0471, ppl=155.5713, gnorm=0.2936, throughput=24.01K wps, wc=64.55K\n",
      "2018-08-09 00:37:52,077 - root - [Epoch 0 Batch 370/1043] loss=4.9074, ppl=135.2821, gnorm=0.3009, throughput=23.85K wps, wc=66.97K\n",
      "2018-08-09 00:37:54,462 - root - [Epoch 0 Batch 380/1043] loss=4.8215, ppl=124.1515, gnorm=0.3029, throughput=22.15K wps, wc=52.79K\n",
      "2018-08-09 00:37:56,762 - root - [Epoch 0 Batch 390/1043] loss=4.7910, ppl=120.4176, gnorm=0.3268, throughput=22.17K wps, wc=50.94K\n",
      "2018-08-09 00:37:58,914 - root - [Epoch 0 Batch 400/1043] loss=4.6422, ppl=103.7682, gnorm=0.3452, throughput=22.41K wps, wc=48.22K\n",
      "2018-08-09 00:38:01,191 - root - [Epoch 0 Batch 410/1043] loss=4.8162, ppl=123.4975, gnorm=0.3059, throughput=21.21K wps, wc=48.27K\n",
      "2018-08-09 00:38:03,774 - root - [Epoch 0 Batch 420/1043] loss=4.8439, ppl=126.9574, gnorm=0.2990, throughput=21.74K wps, wc=56.14K\n",
      "2018-08-09 00:38:06,730 - root - [Epoch 0 Batch 430/1043] loss=4.8964, ppl=133.8086, gnorm=0.3085, throughput=23.47K wps, wc=69.33K\n",
      "2018-08-09 00:38:09,601 - root - [Epoch 0 Batch 440/1043] loss=4.7864, ppl=119.8713, gnorm=0.2834, throughput=23.38K wps, wc=67.08K\n",
      "2018-08-09 00:38:11,978 - root - [Epoch 0 Batch 450/1043] loss=4.6870, ppl=108.5307, gnorm=0.3170, throughput=22.60K wps, wc=53.68K\n",
      "2018-08-09 00:38:14,176 - root - [Epoch 0 Batch 460/1043] loss=4.4739, ppl=87.6952, gnorm=0.3498, throughput=22.92K wps, wc=50.38K\n",
      "2018-08-09 00:38:16,832 - root - [Epoch 0 Batch 470/1043] loss=4.7517, ppl=115.7855, gnorm=0.3114, throughput=22.87K wps, wc=60.70K\n",
      "2018-08-09 00:38:19,164 - root - [Epoch 0 Batch 480/1043] loss=4.3983, ppl=81.3090, gnorm=0.3567, throughput=23.18K wps, wc=54.04K\n",
      "2018-08-09 00:38:21,306 - root - [Epoch 0 Batch 490/1043] loss=4.5235, ppl=92.1587, gnorm=0.4389, throughput=21.64K wps, wc=46.32K\n",
      "2018-08-09 00:38:23,565 - root - [Epoch 0 Batch 500/1043] loss=4.5669, ppl=96.2442, gnorm=0.3113, throughput=21.50K wps, wc=48.55K\n",
      "2018-08-09 00:38:25,531 - root - [Epoch 0 Batch 510/1043] loss=4.3300, ppl=75.9419, gnorm=0.3313, throughput=21.18K wps, wc=41.62K\n",
      "2018-08-09 00:38:27,423 - root - [Epoch 0 Batch 520/1043] loss=4.2328, ppl=68.9083, gnorm=0.3642, throughput=21.14K wps, wc=39.98K\n",
      "2018-08-09 00:38:29,955 - root - [Epoch 0 Batch 530/1043] loss=4.6495, ppl=104.5277, gnorm=0.2987, throughput=23.15K wps, wc=58.58K\n",
      "2018-08-09 00:38:32,251 - root - [Epoch 0 Batch 540/1043] loss=4.5116, ppl=91.0657, gnorm=0.3089, throughput=22.10K wps, wc=50.72K\n",
      "2018-08-09 00:38:34,947 - root - [Epoch 0 Batch 550/1043] loss=4.5376, ppl=93.4680, gnorm=0.3165, throughput=23.36K wps, wc=62.95K\n",
      "2018-08-09 00:38:37,037 - root - [Epoch 0 Batch 560/1043] loss=4.3213, ppl=75.2833, gnorm=0.3476, throughput=22.09K wps, wc=46.13K\n",
      "2018-08-09 00:38:39,588 - root - [Epoch 0 Batch 570/1043] loss=4.3442, ppl=77.0299, gnorm=0.3036, throughput=23.97K wps, wc=61.12K\n",
      "2018-08-09 00:38:42,008 - root - [Epoch 0 Batch 580/1043] loss=4.3574, ppl=78.0516, gnorm=0.3331, throughput=22.92K wps, wc=55.43K\n",
      "2018-08-09 00:38:44,995 - root - [Epoch 0 Batch 590/1043] loss=4.5979, ppl=99.2774, gnorm=0.2910, throughput=24.76K wps, wc=73.93K\n",
      "2018-08-09 00:38:47,475 - root - [Epoch 0 Batch 600/1043] loss=4.4924, ppl=89.3367, gnorm=0.2728, throughput=22.51K wps, wc=55.80K\n",
      "2018-08-09 00:38:49,783 - root - [Epoch 0 Batch 610/1043] loss=4.3166, ppl=74.9308, gnorm=0.3144, throughput=22.67K wps, wc=52.28K\n",
      "2018-08-09 00:38:52,770 - root - [Epoch 0 Batch 620/1043] loss=4.5577, ppl=95.3677, gnorm=0.2611, throughput=24.25K wps, wc=72.39K\n",
      "2018-08-09 00:38:54,498 - root - [Epoch 0 Batch 630/1043] loss=4.0456, ppl=57.1432, gnorm=0.3427, throughput=19.95K wps, wc=34.44K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 00:38:57,007 - root - [Epoch 0 Batch 640/1043] loss=4.3345, ppl=76.2857, gnorm=0.3303, throughput=22.86K wps, wc=57.32K\n",
      "2018-08-09 00:38:59,831 - root - [Epoch 0 Batch 650/1043] loss=4.4754, ppl=87.8291, gnorm=0.2933, throughput=23.53K wps, wc=66.42K\n",
      "2018-08-09 00:39:01,907 - root - [Epoch 0 Batch 660/1043] loss=4.1941, ppl=66.2911, gnorm=0.3411, throughput=21.40K wps, wc=44.42K\n",
      "2018-08-09 00:39:05,118 - root - [Epoch 0 Batch 670/1043] loss=4.6006, ppl=99.5423, gnorm=0.2661, throughput=24.20K wps, wc=77.68K\n",
      "2018-08-09 00:39:07,705 - root - [Epoch 0 Batch 680/1043] loss=4.4242, ppl=83.4484, gnorm=0.2748, throughput=22.82K wps, wc=59.02K\n",
      "2018-08-09 00:39:09,958 - root - [Epoch 0 Batch 690/1043] loss=4.2089, ppl=67.2851, gnorm=0.3153, throughput=22.45K wps, wc=50.54K\n",
      "2018-08-09 00:39:12,316 - root - [Epoch 0 Batch 700/1043] loss=4.2952, ppl=73.3447, gnorm=0.2985, throughput=22.26K wps, wc=52.45K\n",
      "2018-08-09 00:39:14,300 - root - [Epoch 0 Batch 710/1043] loss=4.1837, ppl=65.6061, gnorm=0.3285, throughput=20.85K wps, wc=41.32K\n",
      "2018-08-09 00:39:16,647 - root - [Epoch 0 Batch 720/1043] loss=4.2596, ppl=70.7805, gnorm=0.2928, throughput=21.35K wps, wc=50.09K\n",
      "2018-08-09 00:39:19,052 - root - [Epoch 0 Batch 730/1043] loss=4.1905, ppl=66.0543, gnorm=0.3112, throughput=21.63K wps, wc=52.02K\n",
      "2018-08-09 00:39:21,703 - root - [Epoch 0 Batch 740/1043] loss=4.3307, ppl=76.0004, gnorm=0.2758, throughput=22.60K wps, wc=59.86K\n",
      "2018-08-09 00:39:24,070 - root - [Epoch 0 Batch 750/1043] loss=4.1886, ppl=65.9285, gnorm=0.2958, throughput=22.54K wps, wc=53.35K\n",
      "2018-08-09 00:39:26,906 - root - [Epoch 0 Batch 760/1043] loss=4.2876, ppl=72.7942, gnorm=0.2908, throughput=24.73K wps, wc=70.09K\n",
      "2018-08-09 00:39:28,954 - root - [Epoch 0 Batch 770/1043] loss=4.1118, ppl=61.0561, gnorm=0.3090, throughput=21.11K wps, wc=43.21K\n",
      "2018-08-09 00:39:32,114 - root - [Epoch 0 Batch 780/1043] loss=4.3016, ppl=73.8184, gnorm=0.2768, throughput=24.04K wps, wc=75.93K\n",
      "2018-08-09 00:39:34,333 - root - [Epoch 0 Batch 790/1043] loss=4.1410, ppl=62.8664, gnorm=0.3095, throughput=21.11K wps, wc=46.81K\n",
      "2018-08-09 00:39:36,978 - root - [Epoch 0 Batch 800/1043] loss=4.2096, ppl=67.3280, gnorm=0.3171, throughput=22.21K wps, wc=58.72K\n",
      "2018-08-09 00:39:39,531 - root - [Epoch 0 Batch 810/1043] loss=4.0709, ppl=58.6077, gnorm=0.2959, throughput=22.48K wps, wc=57.38K\n",
      "2018-08-09 00:39:42,075 - root - [Epoch 0 Batch 820/1043] loss=3.9657, ppl=52.7576, gnorm=0.3276, throughput=23.02K wps, wc=58.52K\n",
      "2018-08-09 00:39:44,576 - root - [Epoch 0 Batch 830/1043] loss=4.1248, ppl=61.8524, gnorm=0.3635, throughput=22.89K wps, wc=57.24K\n",
      "2018-08-09 00:39:46,892 - root - [Epoch 0 Batch 840/1043] loss=4.0908, ppl=59.7879, gnorm=0.3268, throughput=22.72K wps, wc=52.57K\n",
      "2018-08-09 00:39:49,539 - root - [Epoch 0 Batch 850/1043] loss=4.1370, ppl=62.6116, gnorm=0.3030, throughput=24.24K wps, wc=64.14K\n",
      "2018-08-09 00:39:52,008 - root - [Epoch 0 Batch 860/1043] loss=4.1237, ppl=61.7861, gnorm=0.2898, throughput=22.14K wps, wc=54.64K\n",
      "2018-08-09 00:39:54,835 - root - [Epoch 0 Batch 870/1043] loss=4.1518, ppl=63.5498, gnorm=0.3071, throughput=23.29K wps, wc=65.81K\n",
      "2018-08-09 00:39:57,101 - root - [Epoch 0 Batch 880/1043] loss=4.0889, ppl=59.6746, gnorm=0.2949, throughput=22.19K wps, wc=50.27K\n",
      "2018-08-09 00:39:59,588 - root - [Epoch 0 Batch 890/1043] loss=4.1547, ppl=63.7313, gnorm=0.2887, throughput=22.57K wps, wc=56.11K\n",
      "2018-08-09 00:40:02,188 - root - [Epoch 0 Batch 900/1043] loss=4.1783, ppl=65.2557, gnorm=0.2822, throughput=23.44K wps, wc=60.91K\n",
      "2018-08-09 00:40:04,517 - root - [Epoch 0 Batch 910/1043] loss=3.9776, ppl=53.3880, gnorm=0.3025, throughput=22.19K wps, wc=51.65K\n",
      "2018-08-09 00:40:07,117 - root - [Epoch 0 Batch 920/1043] loss=4.1620, ppl=64.1998, gnorm=0.2805, throughput=23.29K wps, wc=60.52K\n",
      "2018-08-09 00:40:09,211 - root - [Epoch 0 Batch 930/1043] loss=3.9639, ppl=52.6635, gnorm=0.2998, throughput=20.79K wps, wc=43.51K\n",
      "2018-08-09 00:40:11,436 - root - [Epoch 0 Batch 940/1043] loss=3.9314, ppl=50.9803, gnorm=0.3226, throughput=22.36K wps, wc=49.71K\n",
      "2018-08-09 00:40:14,315 - root - [Epoch 0 Batch 950/1043] loss=4.1919, ppl=66.1460, gnorm=0.2662, throughput=24.64K wps, wc=70.92K\n",
      "2018-08-09 00:40:17,308 - root - [Epoch 0 Batch 960/1043] loss=4.1412, ppl=62.8797, gnorm=0.2778, throughput=24.75K wps, wc=74.06K\n",
      "2018-08-09 00:40:19,381 - root - [Epoch 0 Batch 970/1043] loss=3.8644, ppl=47.6759, gnorm=0.3481, throughput=21.26K wps, wc=44.03K\n",
      "2018-08-09 00:40:22,324 - root - [Epoch 0 Batch 980/1043] loss=4.1707, ppl=64.7607, gnorm=0.2864, throughput=24.72K wps, wc=72.73K\n",
      "2018-08-09 00:40:25,041 - root - [Epoch 0 Batch 990/1043] loss=4.0863, ppl=59.5207, gnorm=0.3003, throughput=24.07K wps, wc=65.39K\n",
      "2018-08-09 00:40:27,526 - root - [Epoch 0 Batch 1000/1043] loss=3.9803, ppl=53.5317, gnorm=0.3003, throughput=22.41K wps, wc=55.65K\n",
      "2018-08-09 00:40:30,285 - root - [Epoch 0 Batch 1010/1043] loss=4.0691, ppl=58.5016, gnorm=0.2884, throughput=23.79K wps, wc=65.61K\n",
      "2018-08-09 00:40:32,371 - root - [Epoch 0 Batch 1020/1043] loss=3.8092, ppl=45.1129, gnorm=0.3235, throughput=21.93K wps, wc=45.72K\n",
      "2018-08-09 00:40:34,819 - root - [Epoch 0 Batch 1030/1043] loss=3.9673, ppl=52.8427, gnorm=0.2978, throughput=22.87K wps, wc=55.97K\n",
      "2018-08-09 00:40:37,139 - root - [Epoch 0 Batch 1040/1043] loss=3.9235, ppl=50.5770, gnorm=0.3414, throughput=22.98K wps, wc=53.27K\n",
      "2018-08-09 00:40:54,358 - root - [Epoch 0] valid Loss=3.4337, valid ppl=30.9902, valid bleu=3.22\n",
      "2018-08-09 00:41:09,201 - root - [Epoch 0] test Loss=3.6425, test ppl=38.1883, test bleu=2.98\n",
      "2018-08-09 00:41:09,207 - root - Save best parameters to gnmt_en_vi_u512/valid_best.params\n",
      "/usr/local/lib/python3.5/dist-packages/mxnet/gluon/block.py:344: UserWarning: save_params is deprecated. Please use save_parameters. Note that if you want load from SymbolBlock later, please use export instead. For details, see https://mxnet.incubator.apache.org/tutorials/gluon/save_load_params.html\n",
      "  warnings.warn(\"save_params is deprecated. Please use save_parameters. \"\n",
      "2018-08-09 00:41:09,343 - root - Learning rate change to 0.0005\n",
      "2018-08-09 00:41:12,272 - root - [Epoch 1 Batch 10/1043] loss=4.0017, ppl=54.6894, gnorm=0.3097, throughput=22.23K wps, wc=65.08K\n",
      "2018-08-09 00:41:15,250 - root - [Epoch 1 Batch 20/1043] loss=4.0086, ppl=55.0705, gnorm=0.2867, throughput=24.10K wps, wc=71.73K\n",
      "2018-08-09 00:41:17,846 - root - [Epoch 1 Batch 30/1043] loss=3.8578, ppl=47.3622, gnorm=0.3045, throughput=23.68K wps, wc=61.44K\n",
      "2018-08-09 00:41:19,742 - root - [Epoch 1 Batch 40/1043] loss=3.5483, ppl=34.7542, gnorm=0.3234, throughput=19.88K wps, wc=37.67K\n",
      "2018-08-09 00:41:21,626 - root - [Epoch 1 Batch 50/1043] loss=3.4522, ppl=31.5709, gnorm=0.3326, throughput=20.06K wps, wc=37.77K\n",
      "2018-08-09 00:41:24,783 - root - [Epoch 1 Batch 60/1043] loss=4.0111, ppl=55.2050, gnorm=0.2842, throughput=23.81K wps, wc=75.14K\n",
      "2018-08-09 00:41:27,396 - root - [Epoch 1 Batch 70/1043] loss=3.8931, ppl=49.0616, gnorm=0.2804, throughput=23.17K wps, wc=60.52K\n",
      "2018-08-09 00:41:30,049 - root - [Epoch 1 Batch 80/1043] loss=3.7990, ppl=44.6562, gnorm=0.2982, throughput=23.98K wps, wc=63.60K\n",
      "2018-08-09 00:41:32,748 - root - [Epoch 1 Batch 90/1043] loss=3.9364, ppl=51.2342, gnorm=0.3461, throughput=23.37K wps, wc=63.07K\n",
      "2018-08-09 00:41:35,175 - root - [Epoch 1 Batch 100/1043] loss=3.6660, ppl=39.0933, gnorm=0.3052, throughput=23.77K wps, wc=57.64K\n",
      "2018-08-09 00:41:37,763 - root - [Epoch 1 Batch 110/1043] loss=3.8238, ppl=45.7768, gnorm=0.3005, throughput=22.35K wps, wc=57.82K\n",
      "2018-08-09 00:41:40,235 - root - [Epoch 1 Batch 120/1043] loss=3.7594, ppl=42.9215, gnorm=0.3527, throughput=22.69K wps, wc=56.05K\n",
      "2018-08-09 00:41:43,426 - root - [Epoch 1 Batch 130/1043] loss=4.0213, ppl=55.7733, gnorm=0.2896, throughput=24.28K wps, wc=77.45K\n",
      "2018-08-09 00:41:46,438 - root - [Epoch 1 Batch 140/1043] loss=3.8549, ppl=47.2262, gnorm=0.2973, throughput=23.71K wps, wc=71.37K\n",
      "2018-08-09 00:41:49,750 - root - [Epoch 1 Batch 150/1043] loss=4.3002, ppl=73.7113, gnorm=0.2721, throughput=24.97K wps, wc=82.65K\n",
      "2018-08-09 00:41:52,249 - root - [Epoch 1 Batch 160/1043] loss=3.9365, ppl=51.2407, gnorm=0.3163, throughput=21.86K wps, wc=54.62K\n",
      "2018-08-09 00:41:55,110 - root - [Epoch 1 Batch 170/1043] loss=4.0238, ppl=55.9112, gnorm=0.3080, throughput=23.66K wps, wc=67.68K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 00:41:57,559 - root - [Epoch 1 Batch 180/1043] loss=3.8040, ppl=44.8808, gnorm=0.3003, throughput=21.69K wps, wc=53.09K\n",
      "2018-08-09 00:41:59,793 - root - [Epoch 1 Batch 190/1043] loss=3.6796, ppl=39.6292, gnorm=0.3289, throughput=21.63K wps, wc=48.29K\n",
      "2018-08-09 00:42:02,439 - root - [Epoch 1 Batch 200/1043] loss=3.9207, ppl=50.4379, gnorm=0.4044, throughput=23.66K wps, wc=62.59K\n",
      "2018-08-09 00:42:04,980 - root - [Epoch 1 Batch 210/1043] loss=3.7377, ppl=41.9998, gnorm=0.3113, throughput=23.12K wps, wc=58.70K\n",
      "2018-08-09 00:42:07,984 - root - [Epoch 1 Batch 220/1043] loss=3.9903, ppl=54.0730, gnorm=0.3114, throughput=22.33K wps, wc=67.07K\n",
      "2018-08-09 00:42:10,349 - root - [Epoch 1 Batch 230/1043] loss=3.7459, ppl=42.3477, gnorm=0.3176, throughput=22.24K wps, wc=52.58K\n",
      "2018-08-09 00:42:12,649 - root - [Epoch 1 Batch 240/1043] loss=3.6709, ppl=39.2868, gnorm=0.3185, throughput=22.55K wps, wc=51.83K\n",
      "2018-08-09 00:42:15,113 - root - [Epoch 1 Batch 250/1043] loss=3.7696, ppl=43.3626, gnorm=0.3470, throughput=22.91K wps, wc=56.43K\n",
      "2018-08-09 00:42:17,361 - root - [Epoch 1 Batch 260/1043] loss=3.6648, ppl=39.0481, gnorm=0.3155, throughput=21.67K wps, wc=48.67K\n",
      "2018-08-09 00:42:20,046 - root - [Epoch 1 Batch 270/1043] loss=3.8209, ppl=45.6457, gnorm=0.3047, throughput=23.82K wps, wc=63.93K\n",
      "2018-08-09 00:42:22,846 - root - [Epoch 1 Batch 280/1043] loss=3.8154, ppl=45.3959, gnorm=0.3145, throughput=23.96K wps, wc=67.06K\n",
      "2018-08-09 00:42:25,664 - root - [Epoch 1 Batch 290/1043] loss=3.8657, ppl=47.7365, gnorm=0.2939, throughput=23.66K wps, wc=66.64K\n",
      "2018-08-09 00:42:28,442 - root - [Epoch 1 Batch 300/1043] loss=3.7825, ppl=43.9248, gnorm=0.2926, throughput=24.04K wps, wc=66.76K\n",
      "2018-08-09 00:42:30,902 - root - [Epoch 1 Batch 310/1043] loss=3.6978, ppl=40.3572, gnorm=0.3149, throughput=23.51K wps, wc=57.80K\n",
      "2018-08-09 00:42:32,666 - root - [Epoch 1 Batch 320/1043] loss=3.4022, ppl=30.0315, gnorm=0.3471, throughput=20.07K wps, wc=35.38K\n",
      "2018-08-09 00:42:35,196 - root - [Epoch 1 Batch 330/1043] loss=3.6688, ppl=39.2052, gnorm=0.3169, throughput=22.35K wps, wc=56.52K\n",
      "2018-08-09 00:42:38,197 - root - [Epoch 1 Batch 340/1043] loss=3.8721, ppl=48.0442, gnorm=0.2921, throughput=24.00K wps, wc=72.01K\n",
      "2018-08-09 00:42:40,493 - root - [Epoch 1 Batch 350/1043] loss=3.5880, ppl=36.1613, gnorm=0.3356, throughput=22.91K wps, wc=52.60K\n",
      "2018-08-09 00:42:43,754 - root - [Epoch 1 Batch 360/1043] loss=3.9273, ppl=50.7699, gnorm=0.4628, throughput=25.25K wps, wc=82.28K\n",
      "2018-08-09 00:42:46,102 - root - [Epoch 1 Batch 370/1043] loss=3.5420, ppl=34.5344, gnorm=0.3780, throughput=23.22K wps, wc=54.51K\n",
      "2018-08-09 00:42:48,904 - root - [Epoch 1 Batch 380/1043] loss=3.7633, ppl=43.0917, gnorm=0.3291, throughput=23.25K wps, wc=65.12K\n",
      "2018-08-09 00:42:51,780 - root - [Epoch 1 Batch 390/1043] loss=3.7894, ppl=44.2299, gnorm=0.3179, throughput=23.89K wps, wc=68.68K\n",
      "2018-08-09 00:42:53,742 - root - [Epoch 1 Batch 400/1043] loss=3.4771, ppl=32.3642, gnorm=0.3524, throughput=20.82K wps, wc=40.81K\n",
      "2018-08-09 00:42:56,493 - root - [Epoch 1 Batch 410/1043] loss=3.6934, ppl=40.1822, gnorm=0.3025, throughput=22.53K wps, wc=61.97K\n",
      "2018-08-09 00:42:59,589 - root - [Epoch 1 Batch 420/1043] loss=3.8278, ppl=45.9623, gnorm=0.2990, throughput=23.72K wps, wc=73.42K\n",
      "2018-08-09 00:43:02,046 - root - [Epoch 1 Batch 430/1043] loss=3.5690, ppl=35.4824, gnorm=0.3273, throughput=21.71K wps, wc=53.33K\n",
      "2018-08-09 00:43:04,565 - root - [Epoch 1 Batch 440/1043] loss=3.6324, ppl=37.8051, gnorm=0.3220, throughput=22.42K wps, wc=56.45K\n",
      "2018-08-09 00:43:06,763 - root - [Epoch 1 Batch 450/1043] loss=3.4938, ppl=32.9116, gnorm=0.3430, throughput=21.41K wps, wc=47.03K\n",
      "2018-08-09 00:43:09,342 - root - [Epoch 1 Batch 460/1043] loss=3.6323, ppl=37.8014, gnorm=0.3204, throughput=23.52K wps, wc=60.63K\n",
      "2018-08-09 00:43:11,461 - root - [Epoch 1 Batch 470/1043] loss=3.5101, ppl=33.4527, gnorm=0.3367, throughput=20.94K wps, wc=44.36K\n",
      "2018-08-09 00:43:13,746 - root - [Epoch 1 Batch 480/1043] loss=3.5364, ppl=34.3438, gnorm=0.3525, throughput=22.27K wps, wc=50.86K\n",
      "2018-08-09 00:43:16,096 - root - [Epoch 1 Batch 490/1043] loss=3.5536, ppl=34.9383, gnorm=0.3382, throughput=22.33K wps, wc=52.44K\n",
      "2018-08-09 00:43:18,889 - root - [Epoch 1 Batch 500/1043] loss=3.7139, ppl=41.0143, gnorm=0.3344, throughput=23.87K wps, wc=66.65K\n",
      "2018-08-09 00:43:21,158 - root - [Epoch 1 Batch 510/1043] loss=3.4637, ppl=31.9335, gnorm=0.3604, throughput=22.91K wps, wc=51.95K\n",
      "2018-08-09 00:43:23,520 - root - [Epoch 1 Batch 520/1043] loss=3.5926, ppl=36.3266, gnorm=0.3423, throughput=22.20K wps, wc=52.40K\n",
      "2018-08-09 00:43:26,177 - root - [Epoch 1 Batch 530/1043] loss=3.6473, ppl=38.3696, gnorm=0.3109, throughput=23.45K wps, wc=62.28K\n",
      "2018-08-09 00:43:28,019 - root - [Epoch 1 Batch 540/1043] loss=3.3302, ppl=27.9441, gnorm=0.3600, throughput=20.03K wps, wc=36.85K\n",
      "2018-08-09 00:43:30,563 - root - [Epoch 1 Batch 550/1043] loss=3.4800, ppl=32.4592, gnorm=0.3600, throughput=24.45K wps, wc=62.18K\n",
      "2018-08-09 00:43:32,668 - root - [Epoch 1 Batch 560/1043] loss=3.4923, ppl=32.8611, gnorm=0.3500, throughput=21.16K wps, wc=44.51K\n",
      "2018-08-09 00:43:35,291 - root - [Epoch 1 Batch 570/1043] loss=3.5607, ppl=35.1875, gnorm=0.3649, throughput=22.32K wps, wc=58.51K\n",
      "2018-08-09 00:43:37,646 - root - [Epoch 1 Batch 580/1043] loss=3.5059, ppl=33.3110, gnorm=0.3491, throughput=21.89K wps, wc=51.54K\n",
      "2018-08-09 00:43:39,986 - root - [Epoch 1 Batch 590/1043] loss=3.3853, ppl=29.5270, gnorm=0.3507, throughput=21.92K wps, wc=51.26K\n",
      "2018-08-09 00:43:42,355 - root - [Epoch 1 Batch 600/1043] loss=3.4205, ppl=30.5833, gnorm=0.3414, throughput=21.18K wps, wc=50.15K\n",
      "2018-08-09 00:43:45,440 - root - [Epoch 1 Batch 610/1043] loss=3.6633, ppl=38.9901, gnorm=0.3197, throughput=23.84K wps, wc=73.53K\n",
      "2018-08-09 00:43:47,447 - root - [Epoch 1 Batch 620/1043] loss=3.2973, ppl=27.0403, gnorm=0.3633, throughput=20.46K wps, wc=41.04K\n",
      "2018-08-09 00:43:49,924 - root - [Epoch 1 Batch 630/1043] loss=3.5504, ppl=34.8285, gnorm=0.3184, throughput=22.18K wps, wc=54.92K\n",
      "2018-08-09 00:43:52,361 - root - [Epoch 1 Batch 640/1043] loss=3.4704, ppl=32.1501, gnorm=0.3642, throughput=22.29K wps, wc=54.32K\n",
      "2018-08-09 00:43:54,649 - root - [Epoch 1 Batch 650/1043] loss=3.4334, ppl=30.9815, gnorm=0.3390, throughput=21.73K wps, wc=49.69K\n",
      "2018-08-09 00:43:57,150 - root - [Epoch 1 Batch 660/1043] loss=3.5413, ppl=34.5114, gnorm=0.3347, throughput=22.73K wps, wc=56.81K\n",
      "2018-08-09 00:43:59,777 - root - [Epoch 1 Batch 670/1043] loss=3.4524, ppl=31.5762, gnorm=0.3593, throughput=23.13K wps, wc=60.72K\n",
      "2018-08-09 00:44:02,782 - root - [Epoch 1 Batch 680/1043] loss=3.6077, ppl=36.8814, gnorm=0.3226, throughput=24.99K wps, wc=75.07K\n",
      "2018-08-09 00:44:05,504 - root - [Epoch 1 Batch 690/1043] loss=3.6031, ppl=36.7102, gnorm=0.3354, throughput=24.15K wps, wc=65.71K\n",
      "2018-08-09 00:44:08,156 - root - [Epoch 1 Batch 700/1043] loss=3.5310, ppl=34.1568, gnorm=0.3334, throughput=22.68K wps, wc=60.12K\n",
      "2018-08-09 00:44:10,600 - root - [Epoch 1 Batch 710/1043] loss=3.4730, ppl=32.2345, gnorm=0.3511, throughput=22.77K wps, wc=55.62K\n",
      "2018-08-09 00:44:13,411 - root - [Epoch 1 Batch 720/1043] loss=3.5324, ppl=34.2059, gnorm=0.3564, throughput=24.32K wps, wc=68.33K\n",
      "2018-08-09 00:44:15,815 - root - [Epoch 1 Batch 730/1043] loss=3.4321, ppl=30.9426, gnorm=0.3576, throughput=22.79K wps, wc=54.75K\n",
      "2018-08-09 00:44:17,953 - root - [Epoch 1 Batch 740/1043] loss=3.2309, ppl=25.3025, gnorm=0.3945, throughput=22.62K wps, wc=48.33K\n",
      "2018-08-09 00:44:20,641 - root - [Epoch 1 Batch 750/1043] loss=3.5386, ppl=34.4203, gnorm=0.3376, throughput=23.22K wps, wc=62.37K\n",
      "2018-08-09 00:44:22,769 - root - [Epoch 1 Batch 760/1043] loss=3.2576, ppl=25.9870, gnorm=0.3926, throughput=22.24K wps, wc=47.31K\n",
      "2018-08-09 00:44:25,072 - root - [Epoch 1 Batch 770/1043] loss=3.3795, ppl=29.3564, gnorm=0.3635, throughput=22.24K wps, wc=51.20K\n",
      "2018-08-09 00:44:27,529 - root - [Epoch 1 Batch 780/1043] loss=3.4536, ppl=31.6150, gnorm=0.3424, throughput=22.35K wps, wc=54.88K\n",
      "2018-08-09 00:44:30,305 - root - [Epoch 1 Batch 790/1043] loss=3.4530, ppl=31.5956, gnorm=0.3718, throughput=24.46K wps, wc=67.87K\n",
      "2018-08-09 00:44:32,635 - root - [Epoch 1 Batch 800/1043] loss=3.3959, ppl=29.8419, gnorm=0.3642, throughput=22.80K wps, wc=53.09K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-09 00:44:35,002 - root - [Epoch 1 Batch 810/1043] loss=3.3919, ppl=29.7211, gnorm=0.3311, throughput=22.34K wps, wc=52.84K\n",
      "2018-08-09 00:44:37,356 - root - [Epoch 1 Batch 820/1043] loss=3.3596, ppl=28.7771, gnorm=0.3546, throughput=22.68K wps, wc=53.37K\n",
      "2018-08-09 00:44:40,252 - root - [Epoch 1 Batch 830/1043] loss=3.5207, ppl=33.8094, gnorm=0.3184, throughput=23.84K wps, wc=69.01K\n",
      "2018-08-09 00:44:43,358 - root - [Epoch 1 Batch 840/1043] loss=3.5542, ppl=34.9582, gnorm=0.2961, throughput=24.61K wps, wc=76.39K\n",
      "2018-08-09 00:44:45,862 - root - [Epoch 1 Batch 850/1043] loss=3.4535, ppl=31.6093, gnorm=0.3355, throughput=23.07K wps, wc=57.73K\n",
      "2018-08-09 00:44:48,189 - root - [Epoch 1 Batch 860/1043] loss=3.3361, ppl=28.1081, gnorm=0.3342, throughput=21.90K wps, wc=50.93K\n",
      "2018-08-09 00:44:50,189 - root - [Epoch 1 Batch 870/1043] loss=3.2230, ppl=25.1032, gnorm=0.3695, throughput=21.58K wps, wc=43.16K\n",
      "2018-08-09 00:44:52,295 - root - [Epoch 1 Batch 880/1043] loss=3.2658, ppl=26.2001, gnorm=0.3530, throughput=21.57K wps, wc=45.40K\n",
      "2018-08-09 00:44:55,214 - root - [Epoch 1 Batch 890/1043] loss=3.4909, ppl=32.8168, gnorm=0.3252, throughput=24.02K wps, wc=70.10K\n",
      "2018-08-09 00:44:57,696 - root - [Epoch 1 Batch 900/1043] loss=3.3732, ppl=29.1719, gnorm=0.3870, throughput=23.84K wps, wc=59.13K\n",
      "2018-08-09 00:44:59,911 - root - [Epoch 1 Batch 910/1043] loss=3.2597, ppl=26.0405, gnorm=0.3859, throughput=21.22K wps, wc=46.98K\n",
      "2018-08-09 00:45:02,846 - root - [Epoch 1 Batch 920/1043] loss=3.4553, ppl=31.6674, gnorm=0.3341, throughput=22.65K wps, wc=66.44K\n",
      "2018-08-09 00:45:05,149 - root - [Epoch 1 Batch 930/1043] loss=3.3500, ppl=28.5024, gnorm=0.3509, throughput=21.72K wps, wc=50.01K\n",
      "2018-08-09 00:45:07,465 - root - [Epoch 1 Batch 940/1043] loss=3.3088, ppl=27.3510, gnorm=0.3801, throughput=22.96K wps, wc=53.14K\n",
      "2018-08-09 00:45:09,731 - root - [Epoch 1 Batch 950/1043] loss=3.2629, ppl=26.1245, gnorm=0.3688, throughput=22.55K wps, wc=51.06K\n",
      "2018-08-09 00:45:12,308 - root - [Epoch 1 Batch 960/1043] loss=3.4124, ppl=30.3378, gnorm=0.3338, throughput=23.31K wps, wc=60.05K\n",
      "2018-08-09 00:45:14,932 - root - [Epoch 1 Batch 970/1043] loss=3.4442, ppl=31.3191, gnorm=0.3462, throughput=23.24K wps, wc=60.95K\n",
      "2018-08-09 00:45:16,823 - root - [Epoch 1 Batch 980/1043] loss=3.1892, ppl=24.2679, gnorm=0.3731, throughput=20.67K wps, wc=39.06K\n",
      "2018-08-09 00:45:19,209 - root - [Epoch 1 Batch 990/1043] loss=3.3549, ppl=28.6416, gnorm=0.3263, throughput=21.74K wps, wc=51.84K\n",
      "2018-08-09 00:45:21,233 - root - [Epoch 1 Batch 1000/1043] loss=3.1500, ppl=23.3358, gnorm=0.4069, throughput=21.75K wps, wc=43.98K\n",
      "2018-08-09 00:45:23,503 - root - [Epoch 1 Batch 1010/1043] loss=3.2651, ppl=26.1830, gnorm=0.3532, throughput=22.20K wps, wc=50.37K\n",
      "2018-08-09 00:45:26,219 - root - [Epoch 1 Batch 1020/1043] loss=3.3810, ppl=29.4007, gnorm=0.3494, throughput=24.11K wps, wc=65.44K\n",
      "2018-08-09 00:45:28,491 - root - [Epoch 1 Batch 1030/1043] loss=3.2706, ppl=26.3272, gnorm=0.3884, throughput=22.09K wps, wc=50.17K\n",
      "2018-08-09 00:45:30,284 - root - [Epoch 1 Batch 1040/1043] loss=3.0723, ppl=21.5910, gnorm=0.3929, throughput=21.06K wps, wc=37.72K\n",
      "2018-08-09 00:45:46,682 - root - [Epoch 1] valid Loss=2.9051, valid ppl=18.2667, valid bleu=9.04\n",
      "2018-08-09 00:46:01,239 - root - [Epoch 1] test Loss=3.0188, test ppl=20.4666, test bleu=9.15\n",
      "2018-08-09 00:46:01,246 - root - Save best parameters to gnmt_en_vi_u512/valid_best.params\n",
      "2018-08-09 00:46:01,529 - root - Learning rate change to 0.00025\n"
     ]
    }
   ],
   "source": [
    "best_valid_bleu = 0.0\n",
    "for epoch_id in range(epochs):\n",
    "    log_avg_loss = 0\n",
    "    log_avg_gnorm = 0\n",
    "    log_wc = 0\n",
    "    log_start_time = time.time()\n",
    "    for batch_id, (src_seq, tgt_seq, src_valid_length, tgt_valid_length)\\\n",
    "            in enumerate(train_data_loader):\n",
    "        # logging.info(src_seq.context) Context suddenly becomes GPU.\n",
    "        src_seq = src_seq.as_in_context(ctx)\n",
    "        tgt_seq = tgt_seq.as_in_context(ctx)\n",
    "        src_valid_length = src_valid_length.as_in_context(ctx)\n",
    "        tgt_valid_length = tgt_valid_length.as_in_context(ctx)\n",
    "        with mx.autograd.record():\n",
    "            out, _ = model(src_seq, tgt_seq[:, :-1], src_valid_length, tgt_valid_length - 1)\n",
    "            loss = loss_function(out, tgt_seq[:, 1:], tgt_valid_length - 1).mean()\n",
    "            loss = loss * (tgt_seq.shape[1] - 1) / (tgt_valid_length - 1).mean()\n",
    "            loss.backward()\n",
    "        grads = [p.grad(ctx) for p in model.collect_params().values()]\n",
    "        gnorm = gluon.utils.clip_global_norm(grads, clip)\n",
    "        trainer.step(1)\n",
    "        src_wc = src_valid_length.sum().asscalar()\n",
    "        tgt_wc = (tgt_valid_length - 1).sum().asscalar()\n",
    "        step_loss = loss.asscalar()\n",
    "        log_avg_loss += step_loss\n",
    "        log_avg_gnorm += gnorm\n",
    "        log_wc += src_wc + tgt_wc\n",
    "        if (batch_id + 1) % log_interval == 0:\n",
    "            wps = log_wc / (time.time() - log_start_time)\n",
    "            logging.info('[Epoch {} Batch {}/{}] loss={:.4f}, ppl={:.4f}, gnorm={:.4f}, '\n",
    "                         'throughput={:.2f}K wps, wc={:.2f}K'\n",
    "                         .format(epoch_id, batch_id + 1, len(train_data_loader),\n",
    "                                 log_avg_loss / log_interval,\n",
    "                                 np.exp(log_avg_loss / log_interval),\n",
    "                                 log_avg_gnorm / log_interval,\n",
    "                                 wps / 1000, log_wc / 1000))\n",
    "            log_start_time = time.time()\n",
    "            log_avg_loss = 0\n",
    "            log_avg_gnorm = 0\n",
    "            log_wc = 0\n",
    "    valid_loss, valid_translation_out = evaluate(val_data_loader)\n",
    "    valid_bleu_score, _, _, _, _ = compute_bleu([val_tgt_sentences], valid_translation_out)\n",
    "    logging.info('[Epoch {}] valid Loss={:.4f}, valid ppl={:.4f}, valid bleu={:.2f}'\n",
    "                 .format(epoch_id, valid_loss, np.exp(valid_loss), valid_bleu_score * 100))\n",
    "    test_loss, test_translation_out = evaluate(test_data_loader)\n",
    "    test_bleu_score, _, _, _, _ = compute_bleu([test_tgt_sentences], test_translation_out)\n",
    "    logging.info('[Epoch {}] test Loss={:.4f}, test ppl={:.4f}, test bleu={:.2f}'\n",
    "                 .format(epoch_id, test_loss, np.exp(test_loss), test_bleu_score * 100))\n",
    "    write_sentences(valid_translation_out,\n",
    "                    os.path.join(save_dir, 'epoch{:d}_valid_out.txt').format(epoch_id))\n",
    "    write_sentences(test_translation_out,\n",
    "                    os.path.join(save_dir, 'epoch{:d}_test_out.txt').format(epoch_id))\n",
    "    if valid_bleu_score > best_valid_bleu:\n",
    "        best_valid_bleu = valid_bleu_score\n",
    "        save_path = os.path.join(save_dir, 'valid_best.params')\n",
    "        logging.info('Save best parameters to {}'.format(save_path))\n",
    "        model.save_parameters(save_path)\n",
    "    if epoch_id + 1 >= (epochs * 2) // 3:\n",
    "        new_lr = trainer.learning_rate * lr_update_factor\n",
    "        logging.info('Learning rate change to {}'.format(new_lr))\n",
    "        trainer.set_learning_rate(new_lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
