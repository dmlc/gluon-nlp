{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modeling using NLP Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will go through an example of using Gluon NLP Toolkit to \n",
    "\n",
    "1. define language model architecture, and train a standard LSTM language model;\n",
    "2. use your own dataset to train the model; \n",
    "3. and use the pre-trained state-of-the-art languague models in Gluon NLP Toolkit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language model definition - one sentence\n",
    "\n",
    "A Language model tries to predict the next word given the previous ones.\n",
    "\n",
    "<img src=language_model_intro.png width=\"500\">\n",
    "\n",
    "## Train your own language model\n",
    "\n",
    "### Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load gluonnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import glob\n",
    "import time\n",
    "import math\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd\n",
    "from mxnet.gluon.utils import download\n",
    "\n",
    "import gluonnlp as nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = 1\n",
    "context = [mx.gpu(i) for i in range(num_gpus)] if num_gpus else [mx.cpu()]\n",
    "log_interval = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20 * len(context)\n",
    "lr = 20\n",
    "epochs = 3\n",
    "bptt = 35\n",
    "grad_clip = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset, extract vocabulary, numericalize, and batchify for truncated BPTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'wikitext-2'\n",
    "train_dataset, val_dataset, test_dataset = [\n",
    "    nlp.data.WikiText2(\n",
    "        segment=segment, bos=None, eos='<eos>', skip_empty=False)\n",
    "    for segment in ['train', 'val', 'test']\n",
    "]\n",
    "\n",
    "vocab = nlp.Vocab(\n",
    "    nlp.data.Counter(train_dataset), padding_token=None, bos_token=None)\n",
    "\n",
    "\n",
    "bptt_batchify = nlp.data.batchify.CorpusBPTTBatchify(\n",
    "    vocab, bptt, batch_size, last_batch='discard')\n",
    "train_data, val_data, test_data = [\n",
    "    bptt_batchify(x) for x in [train_dataset, val_dataset, test_dataset]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pre-defined language model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardRNN(\n",
      "  (embedding): HybridSequential(\n",
      "    (0): Embedding(33278 -> 200, float32)\n",
      "    (1): Dropout(p = 0.2, axes=())\n",
      "  )\n",
      "  (encoder): LSTM(200 -> 200, TNC, num_layers=2, dropout=0.2)\n",
      "  (decoder): HybridSequential(\n",
      "    (0): Dense(200 -> 33278, linear)\n",
      "  )\n",
      ")\n",
      "Vocab(size=33278, unk=\"<unk>\", reserved=\"['<eos>']\")\n"
     ]
    }
   ],
   "source": [
    "model_name = 'standard_lstm_lm_200'\n",
    "model, vocab = nlp.model.get_model(model_name, vocab=vocab, dataset_name=None)\n",
    "print(model)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.initialize(mx.init.Xavier(), ctx=context)\n",
    "trainer = gluon.Trainer(model.collect_params(), 'sgd', {\n",
    "    'learning_rate': lr,\n",
    "    'momentum': 0,\n",
    "    'wd': 0\n",
    "})\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is ready, we can start training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detach gradients on states for truncated BPTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detach(hidden):\n",
    "    if isinstance(hidden, (tuple, list)):\n",
    "        hidden = [detach(i) for i in hidden]\n",
    "    else:\n",
    "        hidden = hidden.detach()\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_source, batch_size, ctx):\n",
    "    total_L = 0.0\n",
    "    ntotal = 0\n",
    "    hidden = model.begin_state(\n",
    "        batch_size=batch_size, func=mx.nd.zeros, ctx=ctx)\n",
    "    for i, (data, target) in enumerate(data_source):\n",
    "        data = data.as_in_context(ctx)\n",
    "        target = target.as_in_context(ctx)\n",
    "        output, hidden = model(data, hidden)\n",
    "        hidden = detach(hidden)\n",
    "        L = loss(output.reshape(-3, -1), target.reshape(-1))\n",
    "        total_L += mx.nd.sum(L).asscalar()\n",
    "        ntotal += L.size\n",
    "    return total_L / ntotal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop\n",
    "\n",
    "We train the model using truncated [back-propagation-through-time (BPTT)](https://en.wikipedia.org/wiki/Backpropagation_through_time) . We backpropagate\n",
    "for 35 time steps using stochastic gradient descent with the learning rate.\n",
    "\n",
    "<img src=https://upload.wikimedia.org/wikipedia/commons/e/ee/Unfold_through_time.png width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, val_data, test_data, epochs, lr):\n",
    "    best_val = float(\"Inf\")\n",
    "    start_train_time = time.time()\n",
    "    parameters = model.collect_params().values()\n",
    "    for epoch in range(epochs):\n",
    "        total_L = 0.0\n",
    "        start_epoch_time = time.time()\n",
    "        start_log_interval_time = time.time()\n",
    "        hiddens = [model.begin_state(batch_size//len(context), func=mx.nd.zeros, ctx=ctx) \n",
    "                   for ctx in context]\n",
    "        for i, (data, target) in enumerate(train_data):\n",
    "            data_list = gluon.utils.split_and_load(data, context, \n",
    "                                                   batch_axis=1, even_split=True)\n",
    "            target_list = gluon.utils.split_and_load(target, context, \n",
    "                                                     batch_axis=1, even_split=True)\n",
    "            hiddens = detach(hiddens)\n",
    "            L = 0\n",
    "            Ls = []\n",
    "            with autograd.record():\n",
    "                for j, (X, y, h) in enumerate(zip(data_list, target_list, hiddens)):\n",
    "                    output, h = model(X, h)\n",
    "                    batch_L = loss(output.reshape(-3, -1), y.reshape(-1,))\n",
    "                    L = L + batch_L.as_in_context(context[0]) / X.size\n",
    "                    Ls.append(batch_L / X.size)\n",
    "                    hiddens[j] = h\n",
    "            L.backward()\n",
    "            grads = [p.grad(x.context) for p in parameters for x in data_list]\n",
    "            gluon.utils.clip_global_norm(grads, grad_clip)\n",
    "\n",
    "            trainer.step(1)\n",
    "\n",
    "            total_L += sum([mx.nd.sum(l).asscalar() for l in Ls])\n",
    "\n",
    "            if i % log_interval == 0 and i > 0:\n",
    "                cur_L = total_L / log_interval\n",
    "                print('[Epoch %d Batch %d/%d] loss %.2f, ppl %.2f, '\n",
    "                      'throughput %.2f samples/s'%(\n",
    "                    epoch, i, len(train_data), cur_L, math.exp(cur_L), \n",
    "                    batch_size * log_interval / (time.time() - start_log_interval_time)))\n",
    "                total_L = 0.0\n",
    "                start_log_interval_time = time.time()\n",
    "\n",
    "        mx.nd.waitall()\n",
    "\n",
    "        print('[Epoch %d] throughput %.2f samples/s'%(\n",
    "                    epoch, len(train_data)*batch_size / (time.time() - start_epoch_time)))\n",
    "        val_L = evaluate(model, val_data, batch_size, context[0])\n",
    "        print('[Epoch %d] time cost %.2fs, valid loss %.2f, valid ppl %.2f'%(\n",
    "            epoch, time.time()-start_epoch_time, val_L, math.exp(val_L)))\n",
    "\n",
    "        if val_L < best_val:\n",
    "            best_val = val_L\n",
    "            test_L = evaluate(model, test_data, batch_size, context[0])\n",
    "            model.save_parameters('{}_{}-{}.params'.format(model_name, dataset_name, epoch))\n",
    "            print('test loss %.2f, test ppl %.2f'%(test_L, math.exp(test_L)))\n",
    "        else:\n",
    "            lr = lr*0.25\n",
    "            print('Learning rate now %f'%(lr))\n",
    "            trainer.set_learning_rate(lr)\n",
    "\n",
    "    print('Total training throughput %.2f samples/s'%(\n",
    "                            (batch_size * len(train_data) * epochs) / \n",
    "                            (time.time() - start_train_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 200/2983] loss 7.67, ppl 2143.41, throughput 489.16 samples/s\n",
      "[Epoch 0 Batch 400/2983] loss 6.78, ppl 884.47, throughput 468.09 samples/s\n",
      "[Epoch 0 Batch 600/2983] loss 6.37, ppl 586.74, throughput 470.85 samples/s\n",
      "[Epoch 0 Batch 800/2983] loss 6.20, ppl 493.10, throughput 484.43 samples/s\n",
      "[Epoch 0 Batch 1000/2983] loss 6.06, ppl 427.79, throughput 473.08 samples/s\n",
      "[Epoch 0 Batch 1200/2983] loss 5.97, ppl 391.13, throughput 486.57 samples/s\n",
      "[Epoch 0 Batch 1400/2983] loss 5.86, ppl 352.27, throughput 467.20 samples/s\n",
      "[Epoch 0 Batch 1600/2983] loss 5.86, ppl 350.84, throughput 471.05 samples/s\n",
      "[Epoch 0 Batch 1800/2983] loss 5.70, ppl 299.50, throughput 491.18 samples/s\n",
      "[Epoch 0 Batch 2000/2983] loss 5.67, ppl 289.28, throughput 470.97 samples/s\n",
      "[Epoch 0 Batch 2200/2983] loss 5.56, ppl 259.36, throughput 471.44 samples/s\n",
      "[Epoch 0 Batch 2400/2983] loss 5.57, ppl 263.67, throughput 467.94 samples/s\n",
      "[Epoch 0 Batch 2600/2983] loss 5.57, ppl 261.23, throughput 488.71 samples/s\n",
      "[Epoch 0 Batch 2800/2983] loss 5.46, ppl 234.55, throughput 469.93 samples/s\n",
      "[Epoch 0] throughput 477.20 samples/s\n",
      "[Epoch 0] time cost 138.33s, valid loss 5.49, valid ppl 241.17\n",
      "test loss 5.40, test ppl 221.33\n",
      "[Epoch 1 Batch 200/2983] loss 5.46, ppl 235.81, throughput 437.38 samples/s\n",
      "[Epoch 1 Batch 400/2983] loss 5.45, ppl 233.30, throughput 425.85 samples/s\n",
      "[Epoch 1 Batch 600/2983] loss 5.29, ppl 197.45, throughput 468.58 samples/s\n",
      "[Epoch 1 Batch 800/2983] loss 5.30, ppl 200.52, throughput 487.41 samples/s\n",
      "[Epoch 1 Batch 1000/2983] loss 5.27, ppl 194.34, throughput 461.15 samples/s\n",
      "[Epoch 1 Batch 1200/2983] loss 5.26, ppl 192.98, throughput 472.51 samples/s\n",
      "[Epoch 1 Batch 1400/2983] loss 5.26, ppl 192.77, throughput 467.80 samples/s\n",
      "[Epoch 1 Batch 1600/2983] loss 5.32, ppl 203.97, throughput 432.83 samples/s\n",
      "[Epoch 1 Batch 1800/2983] loss 5.19, ppl 179.69, throughput 405.37 samples/s\n",
      "[Epoch 1 Batch 2000/2983] loss 5.21, ppl 182.35, throughput 390.66 samples/s\n",
      "[Epoch 1 Batch 2200/2983] loss 5.12, ppl 166.77, throughput 393.49 samples/s\n",
      "[Epoch 1 Batch 2400/2983] loss 5.15, ppl 172.80, throughput 461.57 samples/s\n",
      "[Epoch 1 Batch 2600/2983] loss 5.16, ppl 174.77, throughput 478.38 samples/s\n",
      "[Epoch 1 Batch 2800/2983] loss 5.09, ppl 161.63, throughput 454.80 samples/s\n",
      "[Epoch 1] throughput 445.46 samples/s\n",
      "[Epoch 1] time cost 147.19s, valid loss 5.16, valid ppl 174.52\n",
      "test loss 5.09, test ppl 162.07\n",
      "[Epoch 2 Batch 200/2983] loss 5.14, ppl 171.10, throughput 486.27 samples/s\n",
      "[Epoch 2 Batch 400/2983] loss 5.16, ppl 174.27, throughput 468.24 samples/s\n",
      "[Epoch 2 Batch 600/2983] loss 4.99, ppl 146.77, throughput 488.07 samples/s\n",
      "[Epoch 2 Batch 800/2983] loss 5.03, ppl 152.77, throughput 472.90 samples/s\n",
      "[Epoch 2 Batch 1000/2983] loss 5.02, ppl 151.09, throughput 472.45 samples/s\n",
      "[Epoch 2 Batch 1200/2983] loss 5.02, ppl 151.38, throughput 475.97 samples/s\n",
      "[Epoch 2 Batch 1400/2983] loss 5.05, ppl 155.42, throughput 474.23 samples/s\n",
      "[Epoch 2 Batch 1600/2983] loss 5.11, ppl 165.86, throughput 469.61 samples/s\n",
      "[Epoch 2 Batch 1800/2983] loss 4.98, ppl 146.19, throughput 478.28 samples/s\n",
      "[Epoch 2 Batch 2000/2983] loss 5.02, ppl 150.76, throughput 478.30 samples/s\n",
      "[Epoch 2 Batch 2200/2983] loss 4.93, ppl 138.39, throughput 478.30 samples/s\n",
      "[Epoch 2 Batch 2400/2983] loss 4.97, ppl 143.85, throughput 473.46 samples/s\n",
      "[Epoch 2 Batch 2600/2983] loss 4.99, ppl 146.78, throughput 483.09 samples/s\n",
      "[Epoch 2 Batch 2800/2983] loss 4.91, ppl 136.24, throughput 474.51 samples/s\n",
      "[Epoch 2] throughput 476.82 samples/s\n",
      "[Epoch 2] time cost 138.33s, valid loss 5.08, valid ppl 160.59\n",
      "test loss 5.00, test ppl 148.30\n",
      "Total training throughput 381.75 samples/s\n"
     ]
    }
   ],
   "source": [
    "train(model, train_data, val_data, test_data, epochs, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use your own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ./sherlockholmes.train.txt from https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/sherlockholmes/sherlockholmes.train.txt...\n",
      "Downloading ./sherlockholmes.valid.txt from https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/sherlockholmes/sherlockholmes.valid.txt...\n",
      "Downloading ./sherlockholmes.test.txt from https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/sherlockholmes/sherlockholmes.test.txt...\n",
      "['sherlockholmes.test.txt', 'sherlockholmes.train.txt', 'sherlockholmes.valid.txt']\n"
     ]
    }
   ],
   "source": [
    "TRAIN_PATH = \"./sherlockholmes.train.txt\"\n",
    "VALID_PATH = \"./sherlockholmes.valid.txt\"\n",
    "TEST_PATH = \"./sherlockholmes.test.txt\"\n",
    "PREDICT_PATH = \"./tinyshakespeare/input.txt\"\n",
    "download(\n",
    "    \"https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/sherlockholmes/sherlockholmes.train.txt\",\n",
    "    TRAIN_PATH,\n",
    "    sha1_hash=\"d65a52baaf32df613d4942e0254c81cff37da5e8\")\n",
    "download(\n",
    "    \"https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/sherlockholmes/sherlockholmes.valid.txt\",\n",
    "    VALID_PATH,\n",
    "    sha1_hash=\"71133db736a0ff6d5f024bb64b4a0672b31fc6b3\")\n",
    "download(\n",
    "    \"https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/sherlockholmes/sherlockholmes.test.txt\",\n",
    "    TEST_PATH,\n",
    "    sha1_hash=\"b7ccc4778fd3296c515a3c21ed79e9c2ee249f70\")\n",
    "download(\n",
    "    \"https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/tinyshakespeare/input.txt\",\n",
    "    PREDICT_PATH,\n",
    "    sha1_hash=\"04486597058d11dcc2c556b1d0433891eb639d2e\")\n",
    "sherlockholmes_dataset = glob.glob(\"sherlockholmes.*.txt\")\n",
    "print(sherlockholmes_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "moses_tokenizer = nlp.data.SacreMosesTokenizer()\n",
    "\n",
    "sherlockholmes_val = nlp.data.CorpusDataset(\n",
    "    'sherlockholmes.valid.txt',\n",
    "    sample_splitter=nltk.tokenize.sent_tokenize,\n",
    "    tokenizer=moses_tokenizer,\n",
    "    flatten=True,\n",
    "    eos='<eos>')\n",
    "\n",
    "sherlockholmes_val_data = bptt_batchify(sherlockholmes_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation loss 4.69, test ppl 108.81\n"
     ]
    }
   ],
   "source": [
    "sherlockholmes_L = evaluate(model, sherlockholmes_val_data, batch_size,\n",
    "                            context[0])\n",
    "print('Best validation loss %.2f, test ppl %.2f' %\n",
    "      (sherlockholmes_L, math.exp(sherlockholmes_L)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] throughput 497.20 samples/s\n",
      "[Epoch 0] time cost 3.22s, valid loss 3.54, valid ppl 34.44\n",
      "test loss 3.54, test ppl 34.44\n",
      "[Epoch 1] throughput 477.74 samples/s\n",
      "[Epoch 1] time cost 3.34s, valid loss 3.28, valid ppl 26.47\n",
      "test loss 3.28, test ppl 26.47\n",
      "[Epoch 2] throughput 478.67 samples/s\n",
      "[Epoch 2] time cost 3.31s, valid loss 3.11, valid ppl 22.33\n",
      "test loss 3.11, test ppl 22.33\n",
      "Total training throughput 159.23 samples/s\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model,\n",
    "    sherlockholmes_val_data,\n",
    "    sherlockholmes_val_data,\n",
    "    sherlockholmes_val_data,\n",
    "    epochs=3,\n",
    "    lr=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use pretrained state-of-the-art language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use AWD LSTM language model\n",
    "\n",
    "AWD LSTM language model is the state-of-the-art RNN language model [1]. The main technique is to add weight-dropout on the recurrent hidden to hidden matrices to prevent the overfitting from occurring on the recurrent connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load vocabulary and pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWDRNN(\n",
      "  (embedding): HybridSequential(\n",
      "    (0): Embedding(33278 -> 400, float32)\n",
      "    (1): Dropout(p = 0.65, axes=(0,))\n",
      "  )\n",
      "  (encoder): Sequential(\n",
      "    (0): LSTM(400 -> 1150, TNC)\n",
      "    (1): LSTM(1150 -> 1150, TNC)\n",
      "    (2): LSTM(1150 -> 400, TNC)\n",
      "  )\n",
      "  (decoder): HybridSequential(\n",
      "    (0): Dense(400 -> 33278, linear)\n",
      "  )\n",
      ")\n",
      "Vocab(size=33278, unk=\"<unk>\", reserved=\"['<eos>']\")\n"
     ]
    }
   ],
   "source": [
    "awd_model_name = 'awd_lstm_lm_1150'\n",
    "awd_model, vocab = nlp.model.get_model(\n",
    "    awd_model_name,\n",
    "    vocab=vocab,\n",
    "    dataset_name=dataset_name,\n",
    "    pretrained=True,\n",
    "    ctx=context[0])\n",
    "print(awd_model)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the pretrained model on val and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation loss 4.30, val ppl 73.44\n",
      "Best test loss 4.25, test ppl 69.81\n"
     ]
    }
   ],
   "source": [
    "val_L = evaluate(awd_model, val_data, batch_size, context[0])\n",
    "test_L = evaluate(awd_model, test_data, batch_size, context[0])\n",
    "print('Best validation loss %.2f, val ppl %.2f' % (val_L, math.exp(val_L)))\n",
    "print('Best test loss %.2f, test ppl %.2f' % (test_L, math.exp(test_L)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Cache LSTM language model\n",
    "\n",
    "Cache LSTM language model [2] adds a cache-like memory to neural network language models, e.g., AWD LSTM language model. It exploits the hidden outputs to define a probability distribution over the words in the cache. It generates the state-of-the-art results in inference time.\n",
    "\n",
    "<img src=cache_model.png width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pretrained model and define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CacheCell(\n",
      "  (lm_model): AWDRNN(\n",
      "    (embedding): HybridSequential(\n",
      "      (0): Embedding(33278 -> 400, float32)\n",
      "      (1): Dropout(p = 0.65, axes=(0,))\n",
      "    )\n",
      "    (encoder): Sequential(\n",
      "      (0): LSTM(400 -> 1150, TNC)\n",
      "      (1): LSTM(1150 -> 1150, TNC)\n",
      "      (2): LSTM(1150 -> 400, TNC)\n",
      "    )\n",
      "    (decoder): HybridSequential(\n",
      "      (0): Dense(400 -> 33278, linear)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "window = 2\n",
    "theta = 0.662\n",
    "lambdas = 0.1279\n",
    "bptt = 2000\n",
    "cache_model = nlp.model.train.get_cache_model(name=awd_model_name,\n",
    "                                             dataset_name=dataset_name,\n",
    "                                             window=window,\n",
    "                                             theta=theta,\n",
    "                                             lambdas=lambdas,\n",
    "                                             ctx=context[0])\n",
    "print(cache_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define specific get_batch and evaluation for cache model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_test_batch_size = 1\n",
    "val_test_batchify = nlp.data.batchify.CorpusBatchify(vocab, val_test_batch_size)\n",
    "val_data = val_test_batchify(val_dataset)\n",
    "test_data = val_test_batchify(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data_source, i, seq_len=None):\n",
    "    seq_len = min(seq_len if seq_len else bptt, len(data_source) - 1 - i)\n",
    "    data = data_source[i:i + seq_len]\n",
    "    target = data_source[i + 1:i + 1 + seq_len]\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cache(model, data_source, batch_size, ctx):\n",
    "    total_L = 0.0\n",
    "    hidden = model.begin_state(\n",
    "        batch_size=batch_size, func=mx.nd.zeros, ctx=ctx)\n",
    "    next_word_history = None\n",
    "    cache_history = None\n",
    "    for i in range(0, len(data_source) - 1, bptt):\n",
    "        if i > 0:\n",
    "            print('Batch %d, ppl %f' % (i, math.exp(total_L / i)))\n",
    "        if i == bptt:\n",
    "            return total_L / i\n",
    "        data, target = get_batch(data_source, i)\n",
    "        data = data.as_in_context(ctx)\n",
    "        target = target.as_in_context(ctx)\n",
    "        L = 0\n",
    "        outs, next_word_history, cache_history, hidden = model(\n",
    "            data, target, next_word_history, cache_history, hidden)\n",
    "        for out in outs:\n",
    "            L += (-mx.nd.log(out)).asscalar()\n",
    "        total_L += L / data.shape[1]\n",
    "        hidden = detach(hidden)\n",
    "    return total_L / len(data_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the pretrained model on val and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2000, ppl 65.772428\n",
      "Batch 2000, ppl 71.184107\n",
      "Best validation loss 4.19, val ppl 65.77\n",
      "Best test loss 4.27, test ppl 71.18\n"
     ]
    }
   ],
   "source": [
    "val_L = evaluate_cache(cache_model, val_data, val_test_batch_size, context[0])\n",
    "test_L = evaluate_cache(cache_model, test_data, val_test_batch_size, context[0])\n",
    "print('Best validation loss %.2f, val ppl %.2f'%(val_L, math.exp(val_L)))\n",
    "print('Best test loss %.2f, test ppl %.2f'%(test_L, math.exp(test_L)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2000, ppl 65.772428\n",
      "Batch 2000, ppl 71.184107\n",
      "Best validation loss 4.19, val ppl 65.77\n",
      "Best test loss 4.27, test ppl 71.18\n"
     ]
    }
   ],
   "source": [
    "val_L = evaluate_cache(cache_model, val_data, val_test_batch_size, context[0])\n",
    "test_L = evaluate_cache(cache_model, test_data, val_test_batch_size, context[0])\n",
    "print('Best validation loss %.2f, val ppl %.2f' % (val_L, math.exp(val_L)))\n",
    "print('Best test loss %.2f, test ppl %.2f' % (test_L, math.exp(test_L)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Showcase with language model, we already support embedding training, sentiment analysis, beam search, and NMT in a similar easy-to-use fashion, and more is on the way.\n",
    "- Gluon NLP Toolkit provides high-level APIs that could drastically simplify the development process of modeling for NLP tasks.\n",
    "- Low-level APIs in NLP Toolkit enables easy customization.\n",
    "\n",
    "Documentation can be found at http://gluon-nlp.mxnet.io/index.html\n",
    "\n",
    "Code is here https://github.com/dmlc/gluon-nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "[1] Merity, S., et al. “Regularizing and optimizing LSTM language models”. ICLR 2018\n",
    "\n",
    "[2] Grave, E., et al. “Improving neural language models with a continuous cache”. ICLR 2017"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
