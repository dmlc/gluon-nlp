{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modeling using NLP Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will go through an example of using tools in Gluon NLP Toolkit to build a data pipeline for language model, and use pre-defined model architecture to train a standard LSTM language model.\n",
    "\n",
    "We train the model using truncated [back-propagation-through-time (BPTT)](https://en.wikipedia.org/wiki/Backpropagation_through_time)\n",
    "\n",
    "![bptt](https://upload.wikimedia.org/wikipedia/commons/e/ee/Unfold_through_time.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load gluonnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd\n",
    "\n",
    "import gluonnlp as nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = 1\n",
    "context = [mx.gpu(i) for i in range(num_gpus)] if num_gpus else [mx.cpu()]\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 80 * len(context)\n",
    "lr = 20\n",
    "epochs = 3\n",
    "bptt = 35\n",
    "grad_clip = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset, extract vocabulary, numericalize, and batchify for truncated BPTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'wikitext-2'\n",
    "train_dataset, val_dataset, test_dataset = [nlp.data.WikiText2(segment=segment,\n",
    "                                                               bos=None, eos='<eos>',\n",
    "                                                               skip_empty=False)\n",
    "                                            for segment in ['train', 'val', 'test']]\n",
    "\n",
    "vocab = nlp.Vocab(nlp.data.Counter(train_dataset), padding_token=None, bos_token=None)\n",
    "\n",
    "bptt_batchify = nlp.data.batchify.LanguageModelBPTT(vocab, batch_size, bptt, last_batch='discard')\n",
    "train_data, val_data, test_data = [bptt_batchify(x)\n",
    "                                   for x in [train_dataset, val_dataset, test_dataset]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-defined language model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardRNN(\n",
      "  (embedding): HybridSequential(\n",
      "    (0): Embedding(33278 -> 200, float32)\n",
      "    (1): Dropout(p = 0.2, axes=())\n",
      "  )\n",
      "  (encoder): LSTM(200 -> 200, TNC, num_layers=2, dropout=0.2)\n",
      "  (decoder): HybridSequential(\n",
      "    (0): Dense(200 -> 33278, linear)\n",
      "  )\n",
      ")\n",
      "Vocab(size=33278, unk=\"<unk>\", reserved=\"['<eos>']\")\n"
     ]
    }
   ],
   "source": [
    "model_name = 'standard_lstm_lm_200'\n",
    "model, vocab = nlp.model.get_model(model_name, vocab=vocab, dataset_name=None)\n",
    "print(model)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.initialize(mx.init.Xavier(), ctx=context)\n",
    "trainer = gluon.Trainer(model.collect_params(), 'sgd',\n",
    "                        {'learning_rate': lr,\n",
    "                         'momentum': 0,\n",
    "                         'wd': 0})\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is ready, we can start training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detach gradients on states for truncated BPTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detach(hidden):\n",
    "    if isinstance(hidden, (tuple, list)):\n",
    "        hidden = [detach(i) for i in hidden]\n",
    "    else:\n",
    "        hidden = hidden.detach()\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_source, ctx):\n",
    "    total_L = 0.0\n",
    "    ntotal = 0\n",
    "    hidden = model.begin_state(batch_size, func=mx.nd.zeros, ctx=ctx)\n",
    "    for i, (data, target) in enumerate(data_source):\n",
    "        data = data.as_in_context(ctx)\n",
    "        target = target.as_in_context(ctx)\n",
    "        output, hidden = model(data, hidden)\n",
    "        L = loss(output.reshape(-3, -1),\n",
    "                 target.reshape(-1))\n",
    "        total_L += mx.nd.sum(L).asscalar()\n",
    "        ntotal += L.size\n",
    "    return total_L / ntotal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, val_data, test_data, epochs, lr):\n",
    "    best_val = float(\"Inf\")\n",
    "    start_train_time = time.time()\n",
    "    parameters = model.collect_params().values()\n",
    "    for epoch in range(epochs):\n",
    "        total_L, n_total = 0.0, 0\n",
    "        start_epoch_time = time.time()\n",
    "        start_log_interval_time = time.time()\n",
    "        hiddens = [model.begin_state(batch_size//len(context), func=mx.nd.zeros, ctx=ctx) \n",
    "                   for ctx in context]\n",
    "        for i, (data, target) in enumerate(train_data):\n",
    "            data_list = gluon.utils.split_and_load(data, context, \n",
    "                                                   batch_axis=1, even_split=True)\n",
    "            target_list = gluon.utils.split_and_load(target, context, \n",
    "                                                     batch_axis=1, even_split=True)\n",
    "            hiddens = detach(hiddens)\n",
    "            L = 0\n",
    "            Ls = []\n",
    "            with autograd.record():\n",
    "                for j, (X, y, h) in enumerate(zip(data_list, target_list, hiddens)):\n",
    "                    output, h = model(X, h)\n",
    "                    batch_L = loss(output.reshape(-3, -1), y.reshape(-1))\n",
    "                    L = L + batch_L.as_in_context(context[0]) / X.size\n",
    "                    Ls.append(batch_L)\n",
    "                    hiddens[j] = h\n",
    "            L.backward()\n",
    "            grads = [p.grad(x.context) for p in parameters for x in data_list]\n",
    "            gluon.utils.clip_global_norm(grads, grad_clip)\n",
    "\n",
    "            trainer.step(1)\n",
    "\n",
    "            total_L += sum([mx.nd.sum(l).asscalar() for l in Ls])\n",
    "            n_total += data.size\n",
    "\n",
    "            if i % log_interval == 0 and i > 0:\n",
    "                cur_L = total_L / n_total\n",
    "                print('[Epoch %d Batch %d/%d] loss %.2f, ppl %.2f, '\n",
    "                      'throughput %.2f samples/s'%(\n",
    "                    epoch, i, len(train_data), cur_L, math.exp(cur_L), \n",
    "                    batch_size * log_interval / (time.time() - start_log_interval_time)))\n",
    "                total_L, n_total = 0.0, 0\n",
    "                start_log_interval_time = time.time()\n",
    "\n",
    "        mx.nd.waitall()\n",
    "\n",
    "        print('[Epoch %d] throughput %.2f samples/s'%(\n",
    "                    epoch, len(train_data)*batch_size / (time.time() - start_epoch_time)))\n",
    "        val_L = evaluate(model, val_data, context[0])\n",
    "        print('[Epoch %d] time cost %.2fs, valid loss %.2f, valid ppl %.2f'%(\n",
    "            epoch, time.time()-start_epoch_time, val_L, math.exp(val_L)))\n",
    "\n",
    "        if val_L < best_val:\n",
    "            best_val = val_L\n",
    "            test_L = evaluate(model, test_data, context[0])\n",
    "            model.save_params('{}_{}-{}.params'.format(model_name, dataset_name, epoch))\n",
    "            print('test loss %.2f, test ppl %.2f'%(test_L, math.exp(test_L)))\n",
    "        else:\n",
    "            lr = lr*0.25\n",
    "            print('Learning rate now %f'%(lr))\n",
    "            trainer.set_learning_rate(lr)\n",
    "\n",
    "    print('Total training throughput %.2f samples/s'%(\n",
    "                            (batch_size * len(train_data) * epochs) / \n",
    "                            (time.time() - start_train_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 100/745] loss 8.03, ppl 3065.94, throughput 1715.26 samples/s\n",
      "[Epoch 0 Batch 200/745] loss 7.25, ppl 1403.39, throughput 1772.06 samples/s\n",
      "[Epoch 0 Batch 300/745] loss 6.94, ppl 1033.97, throughput 1785.40 samples/s\n",
      "[Epoch 0 Batch 400/745] loss 6.67, ppl 787.56, throughput 1786.35 samples/s\n",
      "[Epoch 0 Batch 500/745] loss 6.48, ppl 649.69, throughput 1786.77 samples/s\n",
      "[Epoch 0 Batch 600/745] loss 6.31, ppl 550.47, throughput 1779.51 samples/s\n",
      "[Epoch 0 Batch 700/745] loss 6.20, ppl 492.56, throughput 1776.04 samples/s\n",
      "[Epoch 0] throughput 1774.25 samples/s\n",
      "[Epoch 0] time cost 36.75s, valid loss 5.94, valid ppl 378.13\n",
      "test loss 5.86, test ppl 352.40\n",
      "[Epoch 1 Batch 100/745] loss 6.08, ppl 439.13, throughput 1771.13 samples/s\n",
      "[Epoch 1 Batch 200/745] loss 5.99, ppl 399.40, throughput 1791.90 samples/s\n",
      "[Epoch 1 Batch 300/745] loss 5.93, ppl 377.05, throughput 1790.47 samples/s\n",
      "[Epoch 1 Batch 400/745] loss 5.88, ppl 356.80, throughput 1791.31 samples/s\n",
      "[Epoch 1 Batch 500/745] loss 5.77, ppl 320.72, throughput 1778.47 samples/s\n",
      "[Epoch 1 Batch 600/745] loss 5.69, ppl 296.44, throughput 1789.30 samples/s\n",
      "[Epoch 1 Batch 700/745] loss 5.62, ppl 276.54, throughput 1781.07 samples/s\n",
      "[Epoch 1] throughput 1785.87 samples/s\n",
      "[Epoch 1] time cost 36.54s, valid loss 5.55, valid ppl 255.98\n",
      "test loss 5.46, test ppl 236.26\n",
      "[Epoch 2 Batch 100/745] loss 5.62, ppl 274.64, throughput 1774.99 samples/s\n",
      "[Epoch 2 Batch 200/745] loss 5.55, ppl 257.08, throughput 1783.87 samples/s\n",
      "[Epoch 2 Batch 300/745] loss 5.52, ppl 248.41, throughput 1783.25 samples/s\n",
      "[Epoch 2 Batch 400/745] loss 5.50, ppl 245.33, throughput 1787.02 samples/s\n",
      "[Epoch 2 Batch 500/745] loss 5.41, ppl 223.72, throughput 1798.73 samples/s\n",
      "[Epoch 2 Batch 600/745] loss 5.35, ppl 210.89, throughput 1785.37 samples/s\n",
      "[Epoch 2 Batch 700/745] loss 5.32, ppl 204.93, throughput 1793.54 samples/s\n",
      "[Epoch 2] throughput 1788.93 samples/s\n",
      "[Epoch 2] time cost 36.48s, valid loss 5.27, valid ppl 195.28\n",
      "test loss 5.19, test ppl 179.94\n",
      "Total training throughput 1457.25 samples/s\n"
     ]
    }
   ],
   "source": [
    "train(model, train_data, val_data, test_data, epochs, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use your own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 4982k  100 4982k    0     0  22.4M      0 --:--:-- --:--:-- --:--:-- 22.4M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  390k  100  390k    0     0  4337k      0 --:--:-- --:--:-- --:--:-- 4386k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  439k  100  439k    0     0  4529k      0 --:--:-- --:--:-- --:--:-- 4529k\n",
      "['ptb.test.txt', 'ptb.train.txt', 'ptb.valid.txt']\n"
     ]
    }
   ],
   "source": [
    "!./get_ptb_data.sh\n",
    "ptb_dataset = !ls ptb.*.txt\n",
    "print(ptb_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(['perluniprops', 'nonbreaking_prefixes', 'punkt'])\n",
    "moses_tokenizer = nlp.data.NLTKMosesTokenizer()\n",
    "\n",
    "ptb_val = nlp.data.CorpusDataset('ptb.valid.txt',\n",
    "                                    flatten=True,\n",
    "                                    sample_splitter=nltk.tokenize.sent_tokenize,\n",
    "                                    tokenizer=moses_tokenizer, eos='<eos>')\n",
    "\n",
    "ptb_val_data = nlp.data.batchify.LanguageModelBPTT(vocab, batch_size, bptt, last_batch='discard')(ptb_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation loss 6.48, test ppl 653.59\n"
     ]
    }
   ],
   "source": [
    "ptb_L = evaluate(model, ptb_val_data, context[0])\n",
    "print('Best validation loss %.2f, test ppl %.2f'%(ptb_L, math.exp(ptb_L)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] throughput 1409.67 samples/s\n",
      "[Epoch 0] time cost 2.49s, valid loss 5.80, valid ppl 329.53\n",
      "test loss 5.80, test ppl 329.53\n",
      "[Epoch 1] throughput 1764.51 samples/s\n",
      "[Epoch 1] time cost 2.13s, valid loss 5.75, valid ppl 313.96\n",
      "test loss 5.75, test ppl 313.96\n",
      "[Epoch 2] throughput 1763.15 samples/s\n",
      "[Epoch 2] time cost 2.13s, valid loss 4.91, valid ppl 135.13\n",
      "test loss 4.91, test ppl 135.13\n",
      "Total training throughput 608.55 samples/s\n"
     ]
    }
   ],
   "source": [
    "train(model, ptb_val_data, ptb_val_data, ptb_val_data, epochs=3, lr=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gluon NLP Toolkit provides high-level APIs that could drastically simplify the development process of modeling for NLP tasks.\n",
    "- Low-level APIs in NLP Toolkit enables easy customization.\n",
    "\n",
    "Documentation can be found at http://gluon-nlp.mxnet.io/index.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
