{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modeling using GluonNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A statistical model is simple a probability distribution over sequences of words or characters [1].\n",
    "In this tutorial, we'll restrict our attention to word-based language models.\n",
    "Given a reliable language model we can answer questions like \n",
    "*which among the following strings are we more likely to encounter?*\n",
    "\n",
    "1. 'On Monday, Mr. Lamar’s “DAMN.” took home an even more elusive honor, one that may never have even seemed within reach: the Pulitzer Prize\" \n",
    "1. \"Frog zealot flagged xylophone the bean wallaby anaphylaxis extraneous porpoise into deleterious carrot banana apricot.\"\n",
    "\n",
    "Even if we've never seen either of these sentences in our entire lives, and even though no rapper has previously been awarded a Pulitzer Prize, we wouldn't be shocked to see the first sentence in the New York Times. By comparison, we can all agree that the second sentence, consisting of incoherent babble, is comparatively unlikely. \n",
    "A statistical language model can assign precise probabilities to each string of words.\n",
    "\n",
    "Given a large corpus of text, we can estimate (i.e., train) a language model $\\hat{p}(x_1, ..., x_n)$. And given such a model, we can sample strings $\\mathbf{x} \\sim \\hat{p}(x_1, ..., x_n)$, generating new strings according to their estimated probability. Among other useful applications, we can use language models to score candidate transcriptions from speech recognition models, given preference to sentences that seem more probable (at the expense of those deemed anomalous).\n",
    "\n",
    "These days recurrent neural networks (RNNs) are the preferred method for LM. In this notebook, we will go through an example of using GluonNLP to (i) implement a typical LSTM language model architecture, (ii) train the language model on a corpus of real data; and (iii) bring in your own dataset for training; and (iv) grab off-the-shelf pre-trained state-of-the-art languague models (i.e., AWD lanaguge model) using GluonNLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language model definition - one sentence\n",
    "\n",
    "The standard approach to language modeling consists of training a model that given a trailing window of text, predicts  the next word in the sequence. When we train the model we feed in the inputs $x1, x_2, ...$ and try at each time step to predict the corresponding next word $x_2, ..., x_{n+1}$. To generate text from a language model, we can iteratively predict the next word, and then feed this word as the input to the model at the subsequent time step.\n",
    "\n",
    "<img src=\"https://gluon.mxnet.io/_images/recurrent-lm.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "## Train your own language model\n",
    "Now let's step through how to train your own language model using GluonNLP.\n",
    "\n",
    "\n",
    "### Preparation\n",
    "We'll start by taking care of our basic dependencies and setting up our environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load gluonnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import glob\n",
    "import time\n",
    "import math\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd\n",
    "from mxnet.gluon.utils import download\n",
    "\n",
    "import gluonnlp as nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = 1\n",
    "context = [mx.gpu(i) for i in range(num_gpus)] if num_gpus else [mx.cpu()]\n",
    "log_interval = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20 * len(context)\n",
    "lr = 20\n",
    "epochs = 3\n",
    "bptt = 35\n",
    "grad_clip = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset, extract vocabulary, numericalize, and batchify for truncated BPTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'wikitext-2'\n",
    "train_dataset, val_dataset, test_dataset = [\n",
    "    nlp.data.WikiText2(\n",
    "        segment=segment, bos=None, eos='<eos>', skip_empty=False)\n",
    "    for segment in ['train', 'val', 'test']\n",
    "]\n",
    "\n",
    "vocab = nlp.Vocab(\n",
    "    nlp.data.Counter(train_dataset), padding_token=None, bos_token=None)\n",
    "\n",
    "\n",
    "bptt_batchify = nlp.data.batchify.CorpusBPTTBatchify(\n",
    "    vocab, bptt, batch_size, last_batch='discard')\n",
    "train_data, val_data, test_data = [\n",
    "    bptt_batchify(x) for x in [train_dataset, val_dataset, test_dataset]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pre-defined language model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardRNN(\n",
      "  (embedding): HybridSequential(\n",
      "    (0): Embedding(33278 -> 200, float32)\n",
      "    (1): Dropout(p = 0.2, axes=())\n",
      "  )\n",
      "  (encoder): LSTM(200 -> 200, TNC, num_layers=2, dropout=0.2)\n",
      "  (decoder): HybridSequential(\n",
      "    (0): Dense(200 -> 33278, linear)\n",
      "  )\n",
      ")\n",
      "Vocab(size=33278, unk=\"<unk>\", reserved=\"['<eos>']\")\n"
     ]
    }
   ],
   "source": [
    "model_name = 'standard_lstm_lm_200'\n",
    "model, vocab = nlp.model.get_model(model_name, vocab=vocab, dataset_name=None)\n",
    "print(model)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.initialize(mx.init.Xavier(), ctx=context)\n",
    "trainer = gluon.Trainer(model.collect_params(), 'sgd', {\n",
    "    'learning_rate': lr,\n",
    "    'momentum': 0,\n",
    "    'wd': 0\n",
    "})\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is ready, we can start training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detach gradients on states for truncated BPTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detach(hidden):\n",
    "    if isinstance(hidden, (tuple, list)):\n",
    "        hidden = [detach(i) for i in hidden]\n",
    "    else:\n",
    "        hidden = hidden.detach()\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_source, batch_size, ctx):\n",
    "    total_L = 0.0\n",
    "    ntotal = 0\n",
    "    hidden = model.begin_state(\n",
    "        batch_size=batch_size, func=mx.nd.zeros, ctx=ctx)\n",
    "    for i, (data, target) in enumerate(data_source):\n",
    "        data = data.as_in_context(ctx)\n",
    "        target = target.as_in_context(ctx)\n",
    "        output, hidden = model(data, hidden)\n",
    "        hidden = detach(hidden)\n",
    "        L = loss(output.reshape(-3, -1), target.reshape(-1))\n",
    "        total_L += mx.nd.sum(L).asscalar()\n",
    "        ntotal += L.size\n",
    "    return total_L / ntotal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop\n",
    "\n",
    "Our loss function will be the standard cross-entropy loss function used for multiclass classification,\n",
    "applied at each time step to compare our predictions to the true next word in the sequence.\n",
    "We can calculate gradients with respect to our parameters using truncated [back-propagation-through-time (BPTT)](https://en.wikipedia.org/wiki/Backpropagation_through_time). \n",
    "In this case, we'll backpropagate for $35$ time steps, updating our weights with stochastic gradient descent with the learning rate of $20$, hyperparameters that we chose earlier in the notebook.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/e/ee/Unfold_through_time.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, val_data, test_data, epochs, lr):\n",
    "    best_val = float(\"Inf\")\n",
    "    start_train_time = time.time()\n",
    "    parameters = model.collect_params().values()\n",
    "    for epoch in range(epochs):\n",
    "        total_L = 0.0\n",
    "        start_epoch_time = time.time()\n",
    "        start_log_interval_time = time.time()\n",
    "        hiddens = [model.begin_state(batch_size//len(context), func=mx.nd.zeros, ctx=ctx) \n",
    "                   for ctx in context]\n",
    "        for i, (data, target) in enumerate(train_data):\n",
    "            data_list = gluon.utils.split_and_load(data, context, \n",
    "                                                   batch_axis=1, even_split=True)\n",
    "            target_list = gluon.utils.split_and_load(target, context, \n",
    "                                                     batch_axis=1, even_split=True)\n",
    "            hiddens = detach(hiddens)\n",
    "            L = 0\n",
    "            Ls = []\n",
    "            with autograd.record():\n",
    "                for j, (X, y, h) in enumerate(zip(data_list, target_list, hiddens)):\n",
    "                    output, h = model(X, h)\n",
    "                    batch_L = loss(output.reshape(-3, -1), y.reshape(-1,))\n",
    "                    L = L + batch_L.as_in_context(context[0]) / X.size\n",
    "                    Ls.append(batch_L / X.size)\n",
    "                    hiddens[j] = h\n",
    "            L.backward()\n",
    "            grads = [p.grad(x.context) for p in parameters for x in data_list]\n",
    "            gluon.utils.clip_global_norm(grads, grad_clip)\n",
    "\n",
    "            trainer.step(1)\n",
    "\n",
    "            total_L += sum([mx.nd.sum(l).asscalar() for l in Ls])\n",
    "\n",
    "            if i % log_interval == 0 and i > 0:\n",
    "                cur_L = total_L / log_interval\n",
    "                print('[Epoch %d Batch %d/%d] loss %.2f, ppl %.2f, '\n",
    "                      'throughput %.2f samples/s'%(\n",
    "                    epoch, i, len(train_data), cur_L, math.exp(cur_L), \n",
    "                    batch_size * log_interval / (time.time() - start_log_interval_time)))\n",
    "                total_L = 0.0\n",
    "                start_log_interval_time = time.time()\n",
    "\n",
    "        mx.nd.waitall()\n",
    "\n",
    "        print('[Epoch %d] throughput %.2f samples/s'%(\n",
    "                    epoch, len(train_data)*batch_size / (time.time() - start_epoch_time)))\n",
    "        val_L = evaluate(model, val_data, batch_size, context[0])\n",
    "        print('[Epoch %d] time cost %.2fs, valid loss %.2f, valid ppl %.2f'%(\n",
    "            epoch, time.time()-start_epoch_time, val_L, math.exp(val_L)))\n",
    "\n",
    "        if val_L < best_val:\n",
    "            best_val = val_L\n",
    "            test_L = evaluate(model, test_data, batch_size, context[0])\n",
    "            model.save_parameters('{}_{}-{}.params'.format(model_name, dataset_name, epoch))\n",
    "            print('test loss %.2f, test ppl %.2f'%(test_L, math.exp(test_L)))\n",
    "        else:\n",
    "            lr = lr*0.25\n",
    "            print('Learning rate now %f'%(lr))\n",
    "            trainer.set_learning_rate(lr)\n",
    "\n",
    "    print('Total training throughput %.2f samples/s'%(\n",
    "                            (batch_size * len(train_data) * epochs) / \n",
    "                            (time.time() - start_train_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 200/2983] loss 7.67, ppl 2143.41, throughput 489.16 samples/s\n",
      "[Epoch 0 Batch 400/2983] loss 6.78, ppl 884.47, throughput 468.09 samples/s\n",
      "[Epoch 0 Batch 600/2983] loss 6.37, ppl 586.74, throughput 470.85 samples/s\n",
      "[Epoch 0 Batch 800/2983] loss 6.20, ppl 493.10, throughput 484.43 samples/s\n",
      "[Epoch 0 Batch 1000/2983] loss 6.06, ppl 427.79, throughput 473.08 samples/s\n",
      "[Epoch 0 Batch 1200/2983] loss 5.97, ppl 391.13, throughput 486.57 samples/s\n",
      "[Epoch 0 Batch 1400/2983] loss 5.86, ppl 352.27, throughput 467.20 samples/s\n",
      "[Epoch 0 Batch 1600/2983] loss 5.86, ppl 350.84, throughput 471.05 samples/s\n",
      "[Epoch 0 Batch 1800/2983] loss 5.70, ppl 299.50, throughput 491.18 samples/s\n",
      "[Epoch 0 Batch 2000/2983] loss 5.67, ppl 289.28, throughput 470.97 samples/s\n",
      "[Epoch 0 Batch 2200/2983] loss 5.56, ppl 259.36, throughput 471.44 samples/s\n",
      "[Epoch 0 Batch 2400/2983] loss 5.57, ppl 263.67, throughput 467.94 samples/s\n",
      "[Epoch 0 Batch 2600/2983] loss 5.57, ppl 261.23, throughput 488.71 samples/s\n",
      "[Epoch 0 Batch 2800/2983] loss 5.46, ppl 234.55, throughput 469.93 samples/s\n",
      "[Epoch 0] throughput 477.20 samples/s\n",
      "[Epoch 0] time cost 138.33s, valid loss 5.49, valid ppl 241.17\n",
      "test loss 5.40, test ppl 221.33\n",
      "[Epoch 1 Batch 200/2983] loss 5.46, ppl 235.81, throughput 437.38 samples/s\n",
      "[Epoch 1 Batch 400/2983] loss 5.45, ppl 233.30, throughput 425.85 samples/s\n",
      "[Epoch 1 Batch 600/2983] loss 5.29, ppl 197.45, throughput 468.58 samples/s\n",
      "[Epoch 1 Batch 800/2983] loss 5.30, ppl 200.52, throughput 487.41 samples/s\n",
      "[Epoch 1 Batch 1000/2983] loss 5.27, ppl 194.34, throughput 461.15 samples/s\n",
      "[Epoch 1 Batch 1200/2983] loss 5.26, ppl 192.98, throughput 472.51 samples/s\n",
      "[Epoch 1 Batch 1400/2983] loss 5.26, ppl 192.77, throughput 467.80 samples/s\n",
      "[Epoch 1 Batch 1600/2983] loss 5.32, ppl 203.97, throughput 432.83 samples/s\n",
      "[Epoch 1 Batch 1800/2983] loss 5.19, ppl 179.69, throughput 405.37 samples/s\n",
      "[Epoch 1 Batch 2000/2983] loss 5.21, ppl 182.35, throughput 390.66 samples/s\n",
      "[Epoch 1 Batch 2200/2983] loss 5.12, ppl 166.77, throughput 393.49 samples/s\n",
      "[Epoch 1 Batch 2400/2983] loss 5.15, ppl 172.80, throughput 461.57 samples/s\n",
      "[Epoch 1 Batch 2600/2983] loss 5.16, ppl 174.77, throughput 478.38 samples/s\n",
      "[Epoch 1 Batch 2800/2983] loss 5.09, ppl 161.63, throughput 454.80 samples/s\n",
      "[Epoch 1] throughput 445.46 samples/s\n",
      "[Epoch 1] time cost 147.19s, valid loss 5.16, valid ppl 174.52\n",
      "test loss 5.09, test ppl 162.07\n",
      "[Epoch 2 Batch 200/2983] loss 5.14, ppl 171.10, throughput 486.27 samples/s\n",
      "[Epoch 2 Batch 400/2983] loss 5.16, ppl 174.27, throughput 468.24 samples/s\n",
      "[Epoch 2 Batch 600/2983] loss 4.99, ppl 146.77, throughput 488.07 samples/s\n",
      "[Epoch 2 Batch 800/2983] loss 5.03, ppl 152.77, throughput 472.90 samples/s\n",
      "[Epoch 2 Batch 1000/2983] loss 5.02, ppl 151.09, throughput 472.45 samples/s\n",
      "[Epoch 2 Batch 1200/2983] loss 5.02, ppl 151.38, throughput 475.97 samples/s\n",
      "[Epoch 2 Batch 1400/2983] loss 5.05, ppl 155.42, throughput 474.23 samples/s\n",
      "[Epoch 2 Batch 1600/2983] loss 5.11, ppl 165.86, throughput 469.61 samples/s\n",
      "[Epoch 2 Batch 1800/2983] loss 4.98, ppl 146.19, throughput 478.28 samples/s\n",
      "[Epoch 2 Batch 2000/2983] loss 5.02, ppl 150.76, throughput 478.30 samples/s\n",
      "[Epoch 2 Batch 2200/2983] loss 4.93, ppl 138.39, throughput 478.30 samples/s\n",
      "[Epoch 2 Batch 2400/2983] loss 4.97, ppl 143.85, throughput 473.46 samples/s\n",
      "[Epoch 2 Batch 2600/2983] loss 4.99, ppl 146.78, throughput 483.09 samples/s\n",
      "[Epoch 2 Batch 2800/2983] loss 4.91, ppl 136.24, throughput 474.51 samples/s\n",
      "[Epoch 2] throughput 476.82 samples/s\n",
      "[Epoch 2] time cost 138.33s, valid loss 5.08, valid ppl 160.59\n",
      "test loss 5.00, test ppl 148.30\n",
      "Total training throughput 381.75 samples/s\n"
     ]
    }
   ],
   "source": [
    "train(model, train_data, val_data, test_data, epochs, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use your own dataset\n",
    "\n",
    "When we train a language model, we fit to the statistics of a given dataset. \n",
    "While many papers focus on a few standard datasets, such as WikiText or the Penn Tree Bank,\n",
    "that's just to provide a standard benchmark for the purpose of comparing models against each other.\n",
    "In general, for any given use case, you'll want to train your own language model using a dataset of your choosing.\n",
    "Here, for demonstration, we'll grab some `.txt` files corresponding to Sherlock Holmes novels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ./sherlockholmes.train.txt from https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/sherlockholmes/sherlockholmes.train.txt...\n",
      "Downloading ./sherlockholmes.valid.txt from https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/sherlockholmes/sherlockholmes.valid.txt...\n",
      "Downloading ./sherlockholmes.test.txt from https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/sherlockholmes/sherlockholmes.test.txt...\n",
      "['sherlockholmes.test.txt', 'sherlockholmes.train.txt', 'sherlockholmes.valid.txt']\n"
     ]
    }
   ],
   "source": [
    "TRAIN_PATH = \"./sherlockholmes.train.txt\"\n",
    "VALID_PATH = \"./sherlockholmes.valid.txt\"\n",
    "TEST_PATH = \"./sherlockholmes.test.txt\"\n",
    "PREDICT_PATH = \"./tinyshakespeare/input.txt\"\n",
    "download(\n",
    "    \"https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/sherlockholmes/sherlockholmes.train.txt\",\n",
    "    TRAIN_PATH,\n",
    "    sha1_hash=\"d65a52baaf32df613d4942e0254c81cff37da5e8\")\n",
    "download(\n",
    "    \"https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/sherlockholmes/sherlockholmes.valid.txt\",\n",
    "    VALID_PATH,\n",
    "    sha1_hash=\"71133db736a0ff6d5f024bb64b4a0672b31fc6b3\")\n",
    "download(\n",
    "    \"https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/sherlockholmes/sherlockholmes.test.txt\",\n",
    "    TEST_PATH,\n",
    "    sha1_hash=\"b7ccc4778fd3296c515a3c21ed79e9c2ee249f70\")\n",
    "download(\n",
    "    \"https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/tinyshakespeare/input.txt\",\n",
    "    PREDICT_PATH,\n",
    "    sha1_hash=\"04486597058d11dcc2c556b1d0433891eb639d2e\")\n",
    "sherlockholmes_dataset = glob.glob(\"sherlockholmes.*.txt\")\n",
    "print(sherlockholmes_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "moses_tokenizer = nlp.data.SacreMosesTokenizer()\n",
    "\n",
    "sherlockholmes_val = nlp.data.CorpusDataset(\n",
    "    'sherlockholmes.valid.txt',\n",
    "    sample_splitter=nltk.tokenize.sent_tokenize,\n",
    "    tokenizer=moses_tokenizer,\n",
    "    flatten=True,\n",
    "    eos='<eos>')\n",
    "\n",
    "sherlockholmes_val_data = bptt_batchify(sherlockholmes_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation loss 4.69, test ppl 108.81\n"
     ]
    }
   ],
   "source": [
    "sherlockholmes_L = evaluate(model, sherlockholmes_val_data, batch_size,\n",
    "                            context[0])\n",
    "print('Best validation loss %.2f, test ppl %.2f' %\n",
    "      (sherlockholmes_L, math.exp(sherlockholmes_L)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] throughput 497.20 samples/s\n",
      "[Epoch 0] time cost 3.22s, valid loss 3.54, valid ppl 34.44\n",
      "test loss 3.54, test ppl 34.44\n",
      "[Epoch 1] throughput 477.74 samples/s\n",
      "[Epoch 1] time cost 3.34s, valid loss 3.28, valid ppl 26.47\n",
      "test loss 3.28, test ppl 26.47\n",
      "[Epoch 2] throughput 478.67 samples/s\n",
      "[Epoch 2] time cost 3.31s, valid loss 3.11, valid ppl 22.33\n",
      "test loss 3.11, test ppl 22.33\n",
      "Total training throughput 159.23 samples/s\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model,\n",
    "    sherlockholmes_val_data,\n",
    "    sherlockholmes_val_data,\n",
    "    sherlockholmes_val_data,\n",
    "    epochs=3,\n",
    "    lr=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use pre-trained AWD LSTM language model\n",
    "\n",
    "AWD LSTM language model is the state-of-the-art RNN language model [1]. The main technique is to add weight-dropout on the recurrent hidden to hidden matrices to prevent the overfitting from occurring on the recurrent connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load vocabulary and pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWDRNN(\n",
      "  (embedding): HybridSequential(\n",
      "    (0): Embedding(33278 -> 400, float32)\n",
      "    (1): Dropout(p = 0.65, axes=(0,))\n",
      "  )\n",
      "  (encoder): Sequential(\n",
      "    (0): LSTM(400 -> 1150, TNC)\n",
      "    (1): LSTM(1150 -> 1150, TNC)\n",
      "    (2): LSTM(1150 -> 400, TNC)\n",
      "  )\n",
      "  (decoder): HybridSequential(\n",
      "    (0): Dense(400 -> 33278, linear)\n",
      "  )\n",
      ")\n",
      "Vocab(size=33278, unk=\"<unk>\", reserved=\"['<eos>']\")\n"
     ]
    }
   ],
   "source": [
    "awd_model_name = 'awd_lstm_lm_1150'\n",
    "awd_model, vocab = nlp.model.get_model(\n",
    "    awd_model_name,\n",
    "    vocab=vocab,\n",
    "    dataset_name=dataset_name,\n",
    "    pretrained=True,\n",
    "    ctx=context[0])\n",
    "print(awd_model)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the pre-trained model on val and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation loss 4.30, val ppl 73.44\n",
      "Best test loss 4.25, test ppl 69.81\n"
     ]
    }
   ],
   "source": [
    "val_L = evaluate(awd_model, val_data, batch_size, context[0])\n",
    "test_L = evaluate(awd_model, test_data, batch_size, context[0])\n",
    "print('Best validation loss %.2f, val ppl %.2f' % (val_L, math.exp(val_L)))\n",
    "print('Best test loss %.2f, test ppl %.2f' % (test_L, math.exp(test_L)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Cache LSTM language model\n",
    "\n",
    "Cache LSTM language model [2] adds a cache-like memory to neural network language models, e.g., AWD LSTM language model. It exploits the hidden outputs to define a probability distribution over the words in the cache. It generates the state-of-the-art results in inference time.\n",
    "\n",
    "<img src=cache_model.png width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained model and define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CacheCell(\n",
      "  (lm_model): AWDRNN(\n",
      "    (embedding): HybridSequential(\n",
      "      (0): Embedding(33278 -> 400, float32)\n",
      "      (1): Dropout(p = 0.65, axes=(0,))\n",
      "    )\n",
      "    (encoder): Sequential(\n",
      "      (0): LSTM(400 -> 1150, TNC)\n",
      "      (1): LSTM(1150 -> 1150, TNC)\n",
      "      (2): LSTM(1150 -> 400, TNC)\n",
      "    )\n",
      "    (decoder): HybridSequential(\n",
      "      (0): Dense(400 -> 33278, linear)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "window = 2\n",
    "theta = 0.662\n",
    "lambdas = 0.1279\n",
    "bptt = 2000\n",
    "cache_model = nlp.model.train.get_cache_model(name=awd_model_name,\n",
    "                                             dataset_name=dataset_name,\n",
    "                                             window=window,\n",
    "                                             theta=theta,\n",
    "                                             lambdas=lambdas,\n",
    "                                             ctx=context[0])\n",
    "print(cache_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define specific get_batch and evaluation for cache model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_test_batch_size = 1\n",
    "val_test_batchify = nlp.data.batchify.CorpusBatchify(vocab, val_test_batch_size)\n",
    "val_data = val_test_batchify(val_dataset)\n",
    "test_data = val_test_batchify(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data_source, i, seq_len=None):\n",
    "    seq_len = min(seq_len if seq_len else bptt, len(data_source) - 1 - i)\n",
    "    data = data_source[i:i + seq_len]\n",
    "    target = data_source[i + 1:i + 1 + seq_len]\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cache(model, data_source, batch_size, ctx):\n",
    "    total_L = 0.0\n",
    "    hidden = model.begin_state(\n",
    "        batch_size=batch_size, func=mx.nd.zeros, ctx=ctx)\n",
    "    next_word_history = None\n",
    "    cache_history = None\n",
    "    for i in range(0, len(data_source) - 1, bptt):\n",
    "        if i > 0:\n",
    "            print('Batch %d, ppl %f' % (i, math.exp(total_L / i)))\n",
    "        if i == bptt:\n",
    "            return total_L / i\n",
    "        data, target = get_batch(data_source, i)\n",
    "        data = data.as_in_context(ctx)\n",
    "        target = target.as_in_context(ctx)\n",
    "        L = 0\n",
    "        outs, next_word_history, cache_history, hidden = model(\n",
    "            data, target, next_word_history, cache_history, hidden)\n",
    "        for out in outs:\n",
    "            L += (-mx.nd.log(out)).asscalar()\n",
    "        total_L += L / data.shape[1]\n",
    "        hidden = detach(hidden)\n",
    "    return total_L / len(data_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the pre-trained model on val and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2000, ppl 65.772428\n",
      "Batch 2000, ppl 71.184107\n",
      "Best validation loss 4.19, val ppl 65.77\n",
      "Best test loss 4.27, test ppl 71.18\n"
     ]
    }
   ],
   "source": [
    "val_L = evaluate_cache(cache_model, val_data, val_test_batch_size, context[0])\n",
    "test_L = evaluate_cache(cache_model, test_data, val_test_batch_size, context[0])\n",
    "print('Best validation loss %.2f, val ppl %.2f'%(val_L, math.exp(val_L)))\n",
    "print('Best test loss %.2f, test ppl %.2f'%(test_L, math.exp(test_L)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2000, ppl 65.772428\n",
      "Batch 2000, ppl 71.184107\n",
      "Best validation loss 4.19, val ppl 65.77\n",
      "Best test loss 4.27, test ppl 71.18\n"
     ]
    }
   ],
   "source": [
    "val_L = evaluate_cache(cache_model, val_data, val_test_batch_size, context[0])\n",
    "test_L = evaluate_cache(cache_model, test_data, val_test_batch_size, context[0])\n",
    "print('Best validation loss %.2f, val ppl %.2f' % (val_L, math.exp(val_L)))\n",
    "print('Best test loss %.2f, test ppl %.2f' % (test_L, math.exp(test_L)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we see that:\n",
    "- GluonNLP Toolkit provides high-level APIs that could drastically simplify the development process of modeling for NLP tasks.\n",
    "- Low-level APIs in NLP Toolkit enables easy customization.\n",
    "\n",
    "Documentation can be found at http://gluon-nlp.mxnet.io/index.html\n",
    "\n",
    "Code is here https://github.com/dmlc/gluon-nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "[1] Merity, S., et al. “Regularizing and optimizing LSTM language models”. ICLR 2018\n",
    "\n",
    "[2] Grave, E., et al. “Improving neural language models with a continuous cache”. ICLR 2017"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
