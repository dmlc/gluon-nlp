{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Structured Self-attentive Sentence Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After word embedding is applied to the representation of words, natural language processing(NLP) has been effectively improved in many ways. Along with the widespread use of word embedding, many techniques have been developed to express the semantics of sentences by words, such as:\n",
    "1. The vector representation of multiple words in a sentence is concated or weighted to obtain a vector to represent the sentence.\n",
    "2. Convolution(CNN) and maximum pooling(MaxPooling) on the matrix of all the word vectors of the sentence, using the final result to represent the semantics of the sentence.\n",
    "3. Unroll the sentence according to the time step of the word, input the vector representation of each word into a recurrent neural network(RNN), and use the output of the last time step of the RNN as the semantic representation of the sentence.\n",
    "\n",
    "\n",
    "The above method solves the problem of sentence meaning in a certain extent in many aspects. When concating is used in method one, if the word of the sentence is too long and the vector dimension of the word is slightly larger, then the vector dimension of the sentence will be particularly large, and the internal interaction between the words of the sentence is not taken into account. The use of weighted averaging is not accurate and does not adequately express the impact of each word on sentence semantics. Many useful word meanings may be lost in Method2. The third method selects the output of the last step. If the sentence is too long, the output of the last step does not accurately express the semantics of the sentence.\n",
    "\n",
    "\n",
    "Based on the above mentioned method, Zhouhan Lin, Minwei Feng et al. published a paper [A Structured Self-attentive Sentence Embedding](https://arxiv.org/pdf/1703.03130.pdf)[1] in 2017, proposed a method based on self-attention structure for sentence embedding and applied to user's reviews classification, textual entailment and other tasks. In the end, good results were obtained.\n",
    "\n",
    "\n",
    "In this note, we will use [GluonNLP](https://gluon-nlp.mxnet.io/index.html) to reproduce the model structure in A Structured Self-attentive Sentence Embedding and apply it to [Yelp Data's review star rating data set](https://www.yelp.com/dataset/challenge) for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T10:38:11.616938Z",
     "start_time": "2018-09-19T10:37:58.856455Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "import numpy as np \n",
    "import mxnet as mx\n",
    "import multiprocessing as mp\n",
    "import gluonnlp as nlp\n",
    "\n",
    "from mxnet import gluon, nd, init\n",
    "from mxnet.gluon import nn, rnn\n",
    "from mxnet import autograd, gluon, nd\n",
    "\n",
    "# iUse sklearn's metric function to evaluate the results of the experiment\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# fixed random number seed\n",
    "np.random.seed(2018)\n",
    "mx.random.seed(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T10:38:15.611882Z",
     "start_time": "2018-09-19T10:38:15.597921Z"
    }
   },
   "outputs": [],
   "source": [
    "def try_gpu():\n",
    "    \"\"\"If GPU is available, return mx.gpu(0); else return mx.cpu().\"\"\"\n",
    "    try:\n",
    "        ctx = mx.gpu()\n",
    "        _ = nd.array([0], ctx=ctx)\n",
    "    except:\n",
    "        ctx = mx.cpu()\n",
    "    return ctx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load The Reviews of Yelp Data\n",
    "\n",
    "The [user's review data released](https://www.kaggle.com/yelp-dataset/yelp-dataset) by yelp data is json format. The original paper selected 500K data as the training set, 2000 as the validation set, and 2000 as the test set. Due to the limitation of computing power of personal computers and the quick experiment, 200K pieces of data were selected as the training set in this notebook, and 2000 pieces were randomly divided into validation set.\n",
    "Each sample in the data is a user's comment, the language is English, and each comment category is marked 1-5, representing 5 different emotional colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T10:38:22.772798Z",
     "start_time": "2018-09-19T10:38:21.453532Z"
    }
   },
   "outputs": [],
   "source": [
    "## load json data.\n",
    "with open('../data/sub_review_labels.json', 'r', encoding='utf-8') as fr:\n",
    "    data = json.load(fr)\n",
    "\n",
    "# create a list of review a label paris.\n",
    "dataset = [[text, int(label)] for text, label in zip(data['texts'], data['labels'])]\n",
    "\n",
    "# randomly divide one percent from the training set as a verification set.\n",
    "train_dataset, valid_dataset = nlp.data.train_valid_split(dataset, 0.01)\n",
    "len(train_dataset), len(valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing\n",
    "\n",
    "The purpose of the following code is to process the raw data so that the processed data can be used for model training and prediction. We will use the `SpacyTokenizer` to split the comments into strings, `ClipSequence` to crop the comments to the specified length, and build the dictionary based on the word frequency of the training data. Then we attach the [Glove](https://nlp.stanford.edu/pubs/glove.pdf)[2]  pre-trained word vector to the vocab and convert the characters of each comment data into the corresponding word index in the vocab.\n",
    "Finally get the standardized training data set and verification data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-09-19T10:38:24.370Z"
    }
   },
   "outputs": [],
   "source": [
    "# tokenizer takes as input a string and outputs a list of tokens.\n",
    "tokenizer = nlp.data.SpacyTokenizer('en')\n",
    "\n",
    "# length_clip takes as input a list and outputs a list with maximum length 100.\n",
    "length_clip = nlp.data.ClipSequence(100)\n",
    "\n",
    "def preprocess(x):\n",
    "    \n",
    "    # Limit the range of labels to 0-4\n",
    "    data, label = x[0], x[1]-1\n",
    "    \n",
    "    # clip the length of review words\n",
    "    data = length_clip(tokenizer(data))\n",
    "    return data, label\n",
    "\n",
    "def get_length(x):\n",
    "    return float(len(x[0]))\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    start = time.time()\n",
    "    \n",
    "    with mp.Pool() as pool:\n",
    "        # Each sample is processed in an asynchronous manner.\n",
    "        dataset = gluon.data.SimpleDataset(pool.map(preprocess, dataset))\n",
    "        lengths = gluon.data.SimpleDataset(pool.map(get_length, dataset))\n",
    "    end = time.time()\n",
    "    \n",
    "    print('Done! Tokenizing Time={:.2f}s, #Sentences={}'.format(end - start, len(dataset)))\n",
    "    return dataset, lengths\n",
    "\n",
    "# Preprocess the dataset\n",
    "train_dataset, train_data_lengths = preprocess_dataset(train_dataset)\n",
    "valid_dataset, valid_data_lengths = preprocess_dataset(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-09-19T10:38:26.213Z"
    }
   },
   "outputs": [],
   "source": [
    "# create vocab\n",
    "train_seqs = [sample[0] for sample in train_dataset]\n",
    "counter = nlp.data.count_tokens(list(itertools.chain.from_iterable(train_seqs)))\n",
    "\n",
    "vocab = nlp.Vocab(counter, max_size=10000, padding_token=None, bos_token=None, eos_token=None)\n",
    " \n",
    "# load pre-trained embedding,Glove\n",
    "embedding_weights = nlp.embedding.GloVe(source='glove.6B.300d', embedding_root='../data')\n",
    "vocab.set_embedding(embedding_weights)\n",
    "print(vocab)\n",
    "\n",
    "def token_to_idx(x):\n",
    "    return vocab[x[0]], x[1]\n",
    "\n",
    "# A token index or a list of token indices is returned according to the vocabulary.\n",
    "with mp.Pool() as pool:\n",
    "    train_dataset = pool.map(token_to_idx, train_dataset)\n",
    "    valid_dataset = pool.map(token_to_idx, valid_dataset)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucketing and DataLoader\n",
    "Since each sentence may have a different length, we need to use `Pad` to fill the sentences in a minibatch to equal lengths so that the data can be quickly tensored on the GPU. At the same time, we need to use `Stack` to stack the category tags of a batch of data. For convenience, we use `Tuple` to combine `Pad` and `Stack`.\n",
    "\n",
    "In order to make the length of the sentence pad in each minibatch as small as possible, we should make the sentences with similar lengths in a batch as much as possible. In light of this, we consider constructing a sampler using `FixedBucketSampler`, which defines how the samples in a dataset will be iterated in a more economic way.\n",
    "\n",
    "Finally, we use `DataLoader` to build a data loader for the training. dataset and validation dataset. The training dataset requires FixedBucketSampler, but the validation dataset dosen't require the sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T09:42:32.272491Z",
     "start_time": "2018-09-19T09:42:32.101316Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "bucket_num = 10\n",
    "bucket_ratio = 0.5\n",
    "\n",
    "\n",
    "def get_dataloader():\n",
    "    # Construct the DataLoader Pad data, stack label and lengths\n",
    "    batchify_fn = nlp.data.batchify.Tuple(\n",
    "        nlp.data.batchify.Pad(axis=0),\n",
    "        nlp.data.batchify.Stack())\n",
    "    \n",
    "    # n this example, we use a FixedBucketSampler, \n",
    "    # which assigns each data sample to a fixed bucket based on its length. \n",
    "    batch_sampler = nlp.data.sampler.FixedBucketSampler(\n",
    "        train_data_lengths,\n",
    "        batch_size=batch_size,\n",
    "        num_buckets=bucket_num,\n",
    "        ratio=bucket_ratio,\n",
    "        shuffle=True)\n",
    "    print(batch_sampler.stats())\n",
    "    \n",
    "    # train_dataloader\n",
    "    train_dataloader = gluon.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        batchify_fn=batchify_fn)\n",
    "    # valid_dataloader\n",
    "    valid_dataloader = gluon.data.DataLoader(\n",
    "        dataset=valid_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        batchify_fn=batchify_fn)\n",
    "    return train_dataloader, valid_dataloader\n",
    "\n",
    "train_dataloader, valid_dataloader = get_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T03:20:59.998570Z",
     "start_time": "2018-09-17T03:20:59.993852Z"
    }
   },
   "source": [
    "## Model Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original paper, the representation of the sentence is: firstly, the sentence is disassembled into a list corresponding to the word, then the words are unrolled in order, and the word vector of each word is calculated as the input of each step of the [bidirectional LSTM neural network layer](https://www.bioinf.jku.at/publications/older/2604.pdf)[3]. Taking the output of each step of the bidirectional LSTM network layer, a matrix H is obtained. Suppose the hidden_dim of the bidirectional LSTM is `u`, the word length of the sentence is `n`, then the dimension of the last H is `n-by-2u`. For example, the sentence :this movie is amazing.\n",
    "![](../images/Bi-LSTM-Rep.png)\n",
    "\n",
    "Attention is similar to when humans look at things, we always give different importance to what we see in the perspective of the eye. For example, when we are communicating with people, our eyes will always pay more attention to the face of the communicator, not to the edge of other perspectives. So when we want to express the sentence, we can pay different attention to the output H of the bidirectional LSTM layer.\n",
    "![](../images/attention-nlp.png)\n",
    "\n",
    "$$\n",
    "A = Softmax(W_{s2}tanh(W_{s1}H^T)) \n",
    "$$\n",
    "\n",
    "Here, W<sub>s1</sub> is a weight matrix with the shape: d<sub>a</sub>-by-2u, where d<sub>a</sub> is a hyperparameter.\n",
    "W<sub>s2</sub> is a weight matrix with the shape: r-by-d<sub>a</sub>, where r is the number of different attentions you want to use.\n",
    "\n",
    "When the attention matrix A and the output H of the LSTM are obtained, the final representation is obtained by M=AH.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T07:58:13.253294Z",
     "start_time": "2018-09-19T07:58:13.085740Z"
    }
   },
   "source": [
    "We can first customize a layer of attention, specify the number of hidden nodes att_unit and the number of attention channels att_hops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T09:42:32.278797Z",
     "start_time": "2018-09-19T09:42:32.274348Z"
    }
   },
   "outputs": [],
   "source": [
    "# custom attention layer\n",
    "class SelfAttention(nn.HybridBlock):\n",
    "    def __init__(self, att_unit, att_hops, **kwargs):\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.ut_dense = nn.Dense(att_unit, activation='tanh', flatten=False)\n",
    "            self.et_dense = nn.Dense(att_hops, activation=None, flatten=False)\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        # x shape: [batch_size, seq_len, embedding_width]\n",
    "        # ut shape: [batch_size, seq_len, att_unit]\n",
    "        ut = self.ut_dense(x)\n",
    "        # et shape: [batch_size, seq_len, att_hops]\n",
    "        et = self.et_dense(ut)\n",
    "\n",
    "        # att shape: [batch_size,  att_hops, seq_len]\n",
    "        att = F.softmax(F.transpose(et, axes=(0, 2, 1)), axis=-1)\n",
    "        # output shape [batch_size, att_hops, embedding_width]\n",
    "        output = F.batch_dot(att, x)\n",
    "\n",
    "        return output, att"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the number of samples in different categories of data is very unbalanced, which can be balanced by applying different weights to the losses of different categories of samples. Here we customize a SoftmaxCrossEntroy layer that accepts the category weight parameters and resolves the effects of data imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T09:42:32.287026Z",
     "start_time": "2018-09-19T09:42:32.280727Z"
    }
   },
   "outputs": [],
   "source": [
    "   \n",
    "class WeightedSoftmaxCE(nn.HybridBlock):\n",
    "    def __init__(self, sparse_label=True, from_logits=False,  **kwargs):\n",
    "        super(WeightedSoftmaxCE, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.sparse_label = sparse_label\n",
    "            self.from_logits = from_logits\n",
    "\n",
    "    def hybrid_forward(self, F, pred, label, class_weight, depth=None):\n",
    "        if self.sparse_label:\n",
    "            label = F.reshape(label, shape=(-1, ))\n",
    "            label = F.one_hot(label, depth)\n",
    "        if not self.from_logits:\n",
    "            pred = F.log_softmax(pred, -1)\n",
    "\n",
    "        weight_label = F.broadcast_mul(label, class_weight)\n",
    "        loss = -F.sum(pred * weight_label, axis=-1)\n",
    "\n",
    "        # return F.mean(loss, axis=0, exclude=True)\n",
    "        return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T09:42:32.297203Z",
     "start_time": "2018-09-19T09:42:32.288882Z"
    }
   },
   "outputs": [],
   "source": [
    "# model\n",
    "class SelfAttentiveBiLSTM(nn.HybridBlock):\n",
    "    def __init__(self, vocab_len, emsize, nhide, nlayers, att_unit, att_hops, nfc, nclass,\n",
    "                 drop_prob, pool_way, prune_p=None, prune_q=None, **kwargs):\n",
    "        super(SelfAttentiveBiLSTM, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.embedding_layer = nn.Embedding(vocab_len, emsize)\n",
    "            self.bilstm = rnn.LSTM(nhide, num_layers=nlayers, dropout=drop_prob, bidirectional=True)\n",
    "            self.att_encoder = SelfAttention(att_unit, att_hops)\n",
    "            self.dense = nn.Dense(nfc, activation='tanh')\n",
    "            self.output_layer = nn.Dense(nclass)\n",
    "\n",
    "            self.dense_p, self.dense_q = None, None\n",
    "            if all([prune_p, prune_q]):\n",
    "                self.dense_p = nn.Dense(prune_p, activation='tanh', flatten=False)\n",
    "                self.dense_q = nn.Dense(prune_q, activation='tanh', flatten=False)\n",
    "\n",
    "            self.drop_prob = drop_prob\n",
    "            self.pool_way = pool_way\n",
    "\n",
    "    def hybrid_forward(self, F, inp):\n",
    "        # input_embed: [batch, len, emsize]\n",
    "        inp_embed = self.embedding_layer(inp)\n",
    "        h_output = self.bilstm(F.transpose(inp_embed, axes=(1, 0, 2)))\n",
    "        # att_output: [batch, att_hops, emsize]\n",
    "        att_output, att = self.att_encoder(F.transpose(h_output, axes=(1, 0, 2)))\n",
    "\n",
    "        dense_input = None\n",
    "        if self.pool_way == 'flatten':\n",
    "            dense_input = F.Dropout(F.flatten(att_output), self.drop_prob)\n",
    "        elif self.pool_way == 'mean':\n",
    "            dense_input = F.Dropout(F.mean(att_output, axis=1), self.drop_prob)\n",
    "        elif self.pool_way == 'prune' and all([self.dense_p, self.dense_q]):\n",
    "            # p_section: [batch, att_hops, prune_p]\n",
    "            p_section = self.dense_p(att_output)\n",
    "            # q_section: [batch, emsize, prune_q]\n",
    "            q_section = self.dense_q(F.transpose(att_output, axes=(0, 2, 1)))\n",
    "            dense_input = F.Dropout(F.concat(F.flatten(p_section), F.flatten(q_section), dim=-1), self.drop_prob)\n",
    "\n",
    "        dense_out = self.dense(dense_input)\n",
    "        output = self.output_layer(F.Dropout(dense_out, self.drop_prob))\n",
    "\n",
    "        return output, att"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure parameters and build models\n",
    "\n",
    "The resulting `M` is a matrix, and the way to classify this matrix is `flatten`, `mean` or `prune`. Prune is a way of trimming parameters proposed in the original paper and has been implemented here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T09:42:34.720905Z",
     "start_time": "2018-09-19T09:42:32.299074Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_len = len(vocab)\n",
    "emsize = 300    # word embedding size\n",
    "nhide = 300    # lstm hidden_dim\n",
    "nlayers = 2     # lstm layers\n",
    "att_unit = 350     # the hidden_units of attenion layer\n",
    "att_hops = 2    # the channels of attention\n",
    "nfc = 512\n",
    "nclass = 5\n",
    "\n",
    "drop_prob = 0.5\n",
    "pool_way = 'flatten'    # # The way to handle M\n",
    "prune_p = None\n",
    "prune_q = None\n",
    "\n",
    "ctx = try_gpu()\n",
    "\n",
    "model = SelfAttentiveBiLSTM(vocab_len, emsize, nhide, nlayers, \n",
    "                            att_unit, att_hops, nfc, nclass,\n",
    "                            drop_prob, pool_way, prune_p, prune_q)\n",
    "\n",
    "model.initialize(init=init.Xavier(), ctx=ctx)\n",
    "model.hybridize()\n",
    "\n",
    "# Attach a pre-trained glove word vector to the embedding layer\n",
    "model.embedding_layer.weight.set_data(vocab.embedding.idx_to_vec)\n",
    "# fixed the layer\n",
    "model.embedding_layer.collect_params().setattr('grad_req', 'null')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Using r attention can improve the representation of sentences with different semantics, but if the value of each line in the attention matrix A (r-byn) is very close, that is, there is no difference between several attentions. In the subsequent M = AH, the resulting M will contain a lot of redundant information.\n",
    "So in order to solve this problem, we should try to force A to ensure that the value of each line has obvious differences, that is, try to satisfy the diversity of attention. Therefore, a penalty can be used to achieve this goal.\n",
    "$$ P = ||(AA^T-I)||_F^2 $$\n",
    "\n",
    "\n",
    "It can be seen from the above formula that if the value of each row of A is more similar, the result of P will be larger, and the value of A is less similar for each row, and P is smaller. This means that when the r-focused diversity of A is larger, the smaller P is. So by including this penalty item with the Loss of the model, you can try to ensure the diversity of A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T09:42:34.730600Z",
     "start_time": "2018-09-19T09:42:34.723305Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(data_iter_valid, model, loss, ctx, penal_coeff=0.0, class_weight=None, loss_name='wsce'):\n",
    "    valid_loss = 0.\n",
    "    total_pred = []\n",
    "    total_true = []\n",
    "    n_batch = 0\n",
    "    for batch_x, batch_y in data_iter_valid:\n",
    "        batch_x = batch_x.as_in_context(ctx)\n",
    "        batch_y = batch_y.as_in_context(ctx)\n",
    "        batch_pred, att = model(batch_x)\n",
    "        if loss_name == 'sce':\n",
    "            l = loss(batch_pred, batch_y)\n",
    "        elif loss_name == 'wsce':\n",
    "            l = loss(batch_pred, batch_y, class_weight, class_weight.shape[0])\n",
    "        \n",
    "        # penalty\n",
    "        temp = nd.batch_dot(att, nd.transpose(att, axes=(0, 2, 1))\n",
    "                            ) - nd.eye(att.shape[1], ctx=att.context)\n",
    "        l = l + penal_coeff * temp.norm(axis=(1, 2))\n",
    "        total_pred.extend(np.argmax(nd.softmax(batch_pred, axis=1).asnumpy(), axis=1).tolist())\n",
    "        total_true.extend(np.reshape(batch_y.asnumpy(), (-1,)).tolist())\n",
    "        n_batch += 1\n",
    "        valid_loss += l.mean().asscalar()\n",
    "\n",
    "    F1_valid = f1_score(np.array(total_true), np.array(total_pred), average='weighted')\n",
    "    acc_valid = accuracy_score(np.array(total_true), np.array(total_pred))\n",
    "    valid_loss /= n_batch\n",
    "\n",
    "    return F1_valid, acc_valid, valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T09:42:34.745447Z",
     "start_time": "2018-09-19T09:42:34.732613Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(data_iter_train, data_iter_valid, model, loss, trainer, \n",
    "          ctx, num_epochs, penal_coeff=0.0, clip=None,\n",
    "          class_weight=None, loss_name='wsce'):\n",
    "    \n",
    "    print('Train on ', ctx)\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        start = time.time()\n",
    "        train_loss = 0.\n",
    "        total_pred = []\n",
    "        total_true = []\n",
    "        n_batch = 0\n",
    "\n",
    "        for batch_x, batch_y in data_iter_train:\n",
    "            batch_x = batch_x.as_in_context(ctx)\n",
    "            batch_y = batch_y.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                batch_pred, att = model(batch_x)\n",
    "                if loss_name == 'sce':\n",
    "                    l = loss(batch_pred, batch_y)\n",
    "                elif loss_name == 'wsce':\n",
    "                    l = loss(batch_pred, batch_y, class_weight, class_weight.shape[0])\n",
    "\n",
    "                # penalty\n",
    "                temp = nd.batch_dot(att, nd.transpose(att, axes=(0, 2, 1))\n",
    "                                   ) - nd.eye(att.shape[1], ctx=ctx)\n",
    "                l = l + penal_coeff * temp.norm(axis=(1, 2))\n",
    "            \n",
    "            # backward calculate\n",
    "            l.backward()\n",
    "\n",
    "            # clip gradient\n",
    "            clip_params = [p.data() for p in model.collect_params().values()]\n",
    "            if clip is not None:\n",
    "                norm = nd.array([0.0], ctx)\n",
    "                for param in clip_params:\n",
    "                    if param.grad is not None:\n",
    "                        norm += (param.grad ** 2).sum()\n",
    "                norm = norm.sqrt().asscalar()\n",
    "                if norm > clip:\n",
    "                    for param in clip_params:\n",
    "                        if param.grad is not None:\n",
    "                            param.grad[:] *= clip / norm\n",
    "\n",
    "            # update parmas\n",
    "            trainer.step(batch_x.shape[0])\n",
    "            \n",
    "            # keep result for metric\n",
    "            batch_pred = np.argmax(nd.softmax(batch_pred, axis=1).asnumpy(), axis=1)\n",
    "            batch_true = np.reshape(batch_y.asnumpy(), (-1, ))\n",
    "            total_pred.extend(batch_pred.tolist())\n",
    "            total_true.extend(batch_true.tolist())\n",
    "            batch_train_loss = l.mean().asscalar()\n",
    "\n",
    "            n_batch += 1\n",
    "            train_loss += batch_train_loss\n",
    "\n",
    "            if n_batch % 400 == 0:\n",
    "                print('epoch %d, batch %d, batch_train_loss %.4f, batch_train_acc %.3f' %\n",
    "                      (epoch, n_batch, batch_train_loss, accuracy_score(batch_true, batch_pred)))\n",
    "        \n",
    "        # metric\n",
    "        F1_train = f1_score(np.array(total_true), np.array(total_pred), average='weighted')\n",
    "        acc_train = accuracy_score(np.array(total_true), np.array(total_pred))\n",
    "        train_loss /= n_batch\n",
    "        \n",
    "        # evaluate on valid_data\n",
    "        F1_valid, acc_valid, valid_loss = evaluate(data_iter_valid, model, loss, ctx,\n",
    "                                                   penal_coeff, class_weight, loss_name)\n",
    "\n",
    "        print('epoch %d, learning_rate %.5f \\n\\t train_loss %.4f, acc_train %.3f, F1_train %.3f, ' %\n",
    "              (epoch, trainer.learning_rate, train_loss, acc_train, F1_train))\n",
    "        print('\\t valid_loss %.4f, acc_valid %.3f, F1_valid %.3f, '\n",
    "              '\\ntime %.1f sec' % (valid_loss, acc_valid, F1_valid, time.time() - start))\n",
    "        print('='*50)\n",
    "\n",
    "        # declay lr\n",
    "        if epoch % 2 == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Now that we are training the model, we use WeightedSoftmaxCE to alleviate the problem of data category imbalance. We performed statistical analysis on the data in advance to get a set of class_weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T09:42:34.754325Z",
     "start_time": "2018-09-19T09:42:34.747297Z"
    }
   },
   "outputs": [],
   "source": [
    "class_weight = None\n",
    "loss_name = 'wsce'\n",
    "optim = 'adam'\n",
    "lr = 0.001\n",
    "penal_coeff = 0.1\n",
    "clip = 0.5\n",
    "nepochs = 5\n",
    "\n",
    "trainer = gluon.Trainer(model.collect_params(), optim, {'learning_rate': lr})\n",
    "\n",
    "if loss_name == 'sce':\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "elif loss_name == 'wsce':\n",
    "    loss = WeightedSoftmaxCE()\n",
    "    class_weight = nd.array([3.0, 5.3, 4.0, 2.0, 1.0], ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T09:54:17.827700Z",
     "start_time": "2018-09-19T09:42:34.756241Z"
    }
   },
   "outputs": [],
   "source": [
    "train(train_dataloader, valid_dataloader, model, loss, \n",
    "      trainer, ctx, nepochs, penal_coeff=penal_coeff, \n",
    "      clip=clip, class_weight=class_weight, loss_name=loss_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "Now we will randomly input a sentence into the model and predict its emotional value tag. The range of emotional markers is 1-5, which corresponds to the degree of negative to positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T09:54:17.847951Z",
     "start_time": "2018-09-19T09:54:17.830729Z"
    }
   },
   "outputs": [],
   "source": [
    "input_ar = nd.array(vocab[['This', 'movie', 'is', 'amazing']], ctx=ctx).reshape((1, -1))\n",
    "pred, att = model(input_ar)\n",
    "\n",
    "label = np.argmax(nd.softmax(pred, axis=1).asnumpy(), axis=1) + 1\n",
    "print(label)\n",
    "print(att)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to intuitively feel the role of the attention mechanism, we visualize the output of the model's attention on the predicted samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T09:54:18.471582Z",
     "start_time": "2018-09-19T09:54:17.850801Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "np.squeeze(att.asnumpy(), 0).shape\n",
    "plt.figure(figsize=(8,1))\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "sns.heatmap(np.squeeze(att.asnumpy(), 0), cmap=cmap, annot=True,\n",
    "            xticklabels=['This', 'movie', 'is', 'amazing'], yticklabels=['att0', 'att1'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T04:57:27.632225Z",
     "start_time": "2018-09-17T04:53:33.714Z"
    }
   },
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T09:54:18.514567Z",
     "start_time": "2018-09-19T09:54:18.473604Z"
    }
   },
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "model_dir = '../models'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "model_path = os.path.join(model_dir, 'self_att_bilstm_model')\n",
    "model.export(model_path)\n",
    "print('the structure and params of model have saved in：', model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T06:48:54.898116Z",
     "start_time": "2018-09-19T06:48:54.891514Z"
    }
   },
   "source": [
    "## Conclusion\n",
    "Word embedding can effectively represent the semantic similarity between words, which brings many breakthroughs for complex natural language processing tasks. The attention mechanism can intuitively grasp the important semantic features in the sentence. The LSTM captures the word order relationship between words in a sentence. Through word embedding, LSTM and attention mechanisms work together to effectively represent the semantics of a sentence and apply it to many practical tasks.\n",
    "\n",
    "GluonNLP provides us with many efficient and convenient tools to help us experiment quickly. This greatly simplifies the tedious work of natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "1. [A Structured Self-Attentive Sentence Embedding](https://arxiv.org/pdf/1703.03130.pdf)\n",
    "2. [Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing](https://nlp.stanford.edu/pubs/glove.pdf)\n",
    "3. [Long short-term memory](https://www.bioinf.jku.at/publications/older/2604.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
