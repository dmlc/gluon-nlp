{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Pre-trained Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we introduce how to use pre-trained word embeddings, where each word is represened by a vector. Two popular word embeddings are GloVe and fastText. The used GloVe and fastText pre-trained word embeddings here are from the following sources:\n",
    "\n",
    "* GloVe project website：https://nlp.stanford.edu/projects/glove/\n",
    "* fastText project website：https://fasttext.cc/\n",
    "\n",
    "Let us first import the following packages used in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T17:37:49.890192Z",
     "start_time": "2018-04-23T17:37:48.488280Z"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "from mxnet import nd\n",
    "import gluonnlp as nlp\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Vocabulary with Word Embeddings\n",
    "\n",
    "As a common use case, let us index words, attach pre-trained word embeddings for them, and use such embeddings in Gluon. We will assign a unique ID and word vector to each word in the vocabulary in just a few lines of code.\n",
    "\n",
    "### Creating Vocabulary from Data Sets\n",
    "\n",
    "To begin with, suppose that we have a simple text data set in the string format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T17:37:49.898025Z",
     "start_time": "2018-04-23T17:37:49.892761Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \" hello world \\n hello nice world \\n hi world \\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use our defined tokenizer to count word frequency in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T17:37:49.912629Z",
     "start_time": "2018-04-23T17:37:49.901865Z"
    }
   },
   "outputs": [],
   "source": [
    "def simple_tokenize(source_str, token_delim=' ', seq_delim='\\n'):\n",
    "    return filter(None, re.split(token_delim + '|' + seq_delim, source_str))\n",
    "counter = nlp.data.count_tokens(simple_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The obtained `counter` has key-value pairs whose keys are words and values are word frequencies. This allows us to filter out infrequent words via `Vocab` arguments such as `max_size` and `min_freq`. Suppose that we want to build indices for all the keys in counter. We need a `Vocab` instance with counter as its argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T17:37:49.922658Z",
     "start_time": "2018-04-23T17:37:49.916209Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = nlp.Vocab(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To attach word embedding to indexed words in `vocab`, let us go on to create a fastText word embedding instance by specifying the embedding name `fasttext` and the source name `wiki.simple.vec`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T17:37:52.299935Z",
     "start_time": "2018-04-23T17:37:49.931174Z"
    }
   },
   "outputs": [],
   "source": [
    "fasttext_simple = nlp.embedding.create('fasttext', source='wiki.simple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can attach word embedding `fasttext_simple` to indexed words in `vocab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T17:37:52.313965Z",
     "start_time": "2018-04-23T17:37:52.303336Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab.set_embedding(fasttext_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see other source names under the fastText word embedding, we can use `text.embedding.list_sources`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T17:37:52.341667Z",
     "start_time": "2018-04-23T17:37:52.320124Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['crawl-300d-2M', 'wiki.aa', 'wiki.ab', 'wiki.ace', 'wiki.ady']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.embedding.list_sources('fasttext')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The created vocabulary `vocab` includes four different words and a special unknown token. Let us check the size of `vocab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T17:37:52.350759Z",
     "start_time": "2018-04-23T17:37:52.344763Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the vector of any token that is unknown to `vocab` is a zero vector. Its length is equal to the vector dimension of the fastText word embeddings: 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T17:37:52.365047Z",
     "start_time": "2018-04-23T17:37:52.355680Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.embedding['beautiful'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first five elements of the vector of any unknown token are zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T17:37:52.376383Z",
     "start_time": "2018-04-23T17:37:52.368708Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[ 0.  0.  0.  0.  0.]\n",
       "<NDArray 5 @cpu(0)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.embedding['beautiful'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the shape of the embedding of words 'hello' and 'world' from `vocab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T17:37:52.392627Z",
     "start_time": "2018-04-23T17:37:52.379861Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 300)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.embedding['hello', 'world'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-26T23:29:07.340108Z",
     "start_time": "2018-03-26T23:29:07.334790Z"
    }
   },
   "source": [
    "We can access the first five elements of the embedding of 'hello' and 'world'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T17:37:52.403302Z",
     "start_time": "2018-04-23T17:37:52.395882Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.39567     0.21454    -0.035389   -0.24299    -0.095645  ]\n",
       " [ 0.10444    -0.10858     0.27212     0.13299    -0.33164999]]\n",
       "<NDArray 2x5 @cpu(0)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.embedding['hello', 'world'][:, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pre-trained Word Embeddings in Gluon\n",
    "\n",
    "To demonstrate how to use pre-trained word embeddings in Gluon, let us first obtain indices of the words 'hello' and 'world'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T17:37:52.413473Z",
     "start_time": "2018-04-23T17:37:52.406622Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 4]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['hello', 'world']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain the vectors for the words 'hello' and 'world' by specifying their indices (2 and 1) and the weight matrix `vocab.embedding.idx_to_vec` in `gluon.nn.Embedding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T17:37:52.435511Z",
     "start_time": "2018-04-23T17:37:52.417061Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.39567     0.21454    -0.035389   -0.24299    -0.095645  ]\n",
       " [ 0.10444    -0.10858     0.27212     0.13299    -0.33164999]]\n",
       "<NDArray 2x5 @cpu(0)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim, output_dim = vocab.embedding.idx_to_vec.shape\n",
    "layer = gluon.nn.Embedding(input_dim, output_dim)\n",
    "layer.initialize()\n",
    "layer.weight.set_data(vocab.embedding.idx_to_vec)\n",
    "layer(nd.array([5, 4]))[:, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Vocabulary from Pre-trained Word Embeddings\n",
    "\n",
    "We can also create vocabulary by using vocabulary of pre-trained word embeddings, such as GloVe. Below are a few pre-trained file names under the GloVe word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T17:37:52.445620Z",
     "start_time": "2018-04-23T17:37:52.439905Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['glove.42B.300d',\n",
       " 'glove.6B.100d',\n",
       " 'glove.6B.200d',\n",
       " 'glove.6B.300d',\n",
       " 'glove.6B.50d']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.embedding.list_sources('glove')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity of demonstration, we use a smaller word embedding file, such as the 50-dimensional one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T17:37:54.629083Z",
     "start_time": "2018-04-23T17:37:52.448999Z"
    }
   },
   "outputs": [],
   "source": [
    "glove_6b50d = nlp.embedding.create('glove', source='glove.6B.50d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create vocabulary by using all the tokens from `glove_6b50d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T17:37:56.697847Z",
     "start_time": "2018-04-23T17:37:54.631521Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = nlp.Vocab(nlp.data.Counter(glove_6b50d.idx_to_token))\n",
    "vocab.set_embedding(glove_6b50d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below shows the size of `vocab` including a special unknown token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T17:37:56.707396Z",
     "start_time": "2018-04-23T17:37:56.700531Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400004"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.idx_to_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access attributes of `vocab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T17:37:56.719943Z",
     "start_time": "2018-04-23T17:37:56.710953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71424\n",
      "beautiful\n"
     ]
    }
   ],
   "source": [
    "print(vocab['beautiful'])\n",
    "print(vocab.idx_to_token[71424])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of Word Embeddings\n",
    "\n",
    "To apply word embeddings, we need to define cosine similarity. It can compare similarity of two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T17:37:56.731258Z",
     "start_time": "2018-04-23T17:37:56.725865Z"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import nd\n",
    "def cos_sim(x, y):\n",
    "    return nd.dot(x, y) / (nd.norm(x) * nd.norm(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The range of cosine similarity between two vectors is between -1 and 1. The larger the value, the similarity between two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T17:37:56.947824Z",
     "start_time": "2018-04-23T17:37:56.734509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ 1.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[-1.]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "x = nd.array([1, 2])\n",
    "y = nd.array([10, 20])\n",
    "z = nd.array([-1, -2])\n",
    "\n",
    "print(cos_sim(x, y))\n",
    "print(cos_sim(x, z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Similarity\n",
    "\n",
    "Given an input word, we can find the nearest $k$ words from the vocabulary (400,000 words excluding the unknown token) by similarity. The similarity between any pair of words can be represented by the cosine similarity of their vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T17:37:56.966910Z",
     "start_time": "2018-04-23T17:37:56.950352Z"
    }
   },
   "outputs": [],
   "source": [
    "def norm_vecs_by_row(x):\n",
    "    return x / nd.sqrt(nd.sum(x * x, axis=1)).reshape((-1,1))\n",
    "\n",
    "def get_knn(vocab, k, word):\n",
    "    word_vec = vocab.embedding[word].reshape((-1, 1))\n",
    "    vocab_vecs = norm_vecs_by_row(vocab.embedding.idx_to_vec)\n",
    "    dot_prod = nd.dot(vocab_vecs, word_vec)\n",
    "    indices = nd.topk(dot_prod.reshape((len(vocab), )), k=k+4, ret_typ='indices')\n",
    "    indices = [int(i.asscalar()) for i in indices]\n",
    "    # Remove unknown and input tokens.\n",
    "    return vocab.to_tokens(indices[4:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us find the 5 most similar words of 'baby' from the vocabulary (size: 400,000 words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T17:37:57.370401Z",
     "start_time": "2018-04-23T17:37:56.970599Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['baby', 'babies', 'boy', 'girl', 'newborn']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_knn(vocab, 5, 'baby')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify the cosine similarity of vectors of 'baby' and 'babies'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T17:37:57.384739Z",
     "start_time": "2018-04-23T17:37:57.376476Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[ 0.83871299]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(vocab.embedding['baby'], vocab.embedding['babies'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us find the 5 most similar words of 'computers' from the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T17:37:57.905416Z",
     "start_time": "2018-04-23T17:37:57.388649Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['computers', 'computer', 'phones', 'pcs', 'machines']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_knn(vocab, 5, 'computers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us find the 5 most similar words of 'run' from the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T17:37:58.308799Z",
     "start_time": "2018-04-23T17:37:57.911040Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['run', 'running', 'runs', 'went', 'start']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_knn(vocab, 5, 'run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us find the 5 most similar words of 'beautiful' from the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T17:37:58.702005Z",
     "start_time": "2018-04-23T17:37:58.311135Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beautiful', 'lovely', 'gorgeous', 'wonderful', 'charming']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_knn(vocab, 5, 'beautiful')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Analogy\n",
    "\n",
    "We can also apply pre-trained word embeddings to the word analogy problem. For instance, \"man : woman :: son : daughter\" is an analogy. The word analogy completion problem is defined as: for analogy 'a : b :: c : d', given teh first three words 'a', 'b', 'c', find 'd'. The idea is to find the most similar word vector for vec('c') + (vec('b')-vec('a')).\n",
    "\n",
    "In this example, we will find words by analogy from the 400,000 indexed words in `vocab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T17:37:58.734661Z",
     "start_time": "2018-04-23T17:37:58.706280Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_top_k_by_analogy(vocab, k, word1, word2, word3):\n",
    "    word_vecs = vocab.embedding[word1, word2, word3]\n",
    "    word_diff = (word_vecs[1] - word_vecs[0] + word_vecs[2]).reshape((-1, 1))\n",
    "    vocab_vecs = norm_vecs_by_row(vocab.embedding.idx_to_vec)\n",
    "    dot_prod = nd.dot(vocab_vecs, word_diff)\n",
    "    indices = nd.topk(dot_prod.reshape((len(vocab), )), k=k+4, ret_typ='indices')\n",
    "    indices = [int(i.asscalar()) for i in indices]\n",
    "\n",
    "    # Filter out unknown tokens.\n",
    "    if vocab.to_tokens(indices[0]) == vocab.unknown_token:\n",
    "        return vocab.to_tokens(indices[4:])\n",
    "    else:\n",
    "        return vocab.to_tokens(indices[:-4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete word analogy 'man : woman :: son :'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T17:37:59.198197Z",
     "start_time": "2018-04-23T17:37:58.738997Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['daughter']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_k_by_analogy(vocab, 1, 'man', 'woman', 'son')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us verify the cosine similarity between vec('son')+vec('woman')-vec('man') and vec('daughter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T17:37:59.223207Z",
     "start_time": "2018-04-23T17:37:59.202860Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[ 0.96583432]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cos_sim_word_analogy(vocab, word1, word2, word3, word4):\n",
    "    words = [word1, word2, word3, word4]\n",
    "    vecs = vocab.embedding[words]\n",
    "    return cos_sim(vecs[1] - vecs[0] + vecs[2], vecs[3])\n",
    "\n",
    "cos_sim_word_analogy(vocab, 'man', 'woman', 'son', 'daughter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete word analogy 'beijing : china :: tokyo : '."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T17:37:59.695579Z",
     "start_time": "2018-04-23T17:37:59.230320Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['japan']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_k_by_analogy(vocab, 1, 'beijing', 'china', 'tokyo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete word analogy 'bad : worst :: big : '."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T17:38:00.155040Z",
     "start_time": "2018-04-23T17:37:59.698283Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['biggest']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_k_by_analogy(vocab, 1, 'bad', 'worst', 'big')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete word analogy 'do : did :: go :'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T17:38:00.594591Z",
     "start_time": "2018-04-23T17:38:00.160326Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['went']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_k_by_analogy(vocab, 1, 'do', 'did', 'go')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
