{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings Toolkit\n",
    "\n",
    "## Evaluating Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous example has introduced how to load  pre-trained word embeddings from a set of sources included in the Gluon NLP toolkit. It was shown how make use of the word vectors to find the top most similar words of a given words or to solve the analogy task.\n",
    "\n",
    "Besides manually investigating similar words or the predicted analogous words, we can facilitate word embedding evaluation datasets to quantify the evaluation.\n",
    "\n",
    "Datasets for the *similarity* task come with a list of word pairs together with a human similarity judgement. The task is to recover the order of most-similar to least-similar pairs.\n",
    "\n",
    "Datasets for the *analogy* tasks supply a set of analogy quadruples of the form  ‘a : b :: c : d’ and the task is to recover find the correct ‘d’ in as many cases as possible given just ‘a’, ‘b’, ‘c’. For instance, “man : woman :: son : daughter” is an analogy.\n",
    "\n",
    "The Gluon NLP toolkit includes a set of popular *similarity* and *analogy* task datasets as well as helpers for computing the evaluation scores. Here we show how to make use of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T20:27:24.991031Z",
     "start_time": "2018-03-28T20:27:23.669296Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import mxnet as mx\n",
    "import gluonnlp as nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load some word embeddings. Here we choose the embeddings trained on the Wikipedia Simple English corpus, as they are small and fast to load for demo purposes. For better results you may consider the 'crawl-300d-2M' source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding = nlp.embedding.create('fasttext', source='crawl-300d-2M')\n",
    "embedding = nlp.embedding.create('fasttext', source='wiki.simple')\n",
    "\n",
    "vocab = nlp.Vocab(nlp.data.Counter(embedding.idx_to_token))\n",
    "vocab.set_embedding(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T20:27:25.000472Z",
     "start_time": "2018-03-28T20:27:24.993155Z"
    }
   },
   "source": [
    "### Word Similarity and Relatedness Task\n",
    "\n",
    "Word embeddings should capture the relationsship between words in natural language.\n",
    "In the Word Similarity and Relatedness Task word embeddings are evaluated by comparing word similarity scores computed from a pair of words with human labels for the similarity or relatedness of the pair.\n",
    "\n",
    "`gluonnlp` includes a number of common datasets for the Word Similarity and Relatedness Task. The included datasets are listed in the [API documentation](http://gluon-nlp.mxnet.io/api/data.html#word-embedding-evaluation-datasets). We use several of them in the evaluation example below.\n",
    "\n",
    "We first show a few samples from the WordSim353 dataset, to get an overall feeling of the Dataset structur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer, keyboard, 7.62\n",
      "Jerusalem, Israel, 8.46\n",
      "planet, galaxy, 8.11\n",
      "canyon, landscape, 7.53\n",
      "OPEC, country, 5.63\n",
      "day, summer, 3.94\n",
      "day, dawn, 7.53\n",
      "country, citizen, 7.31\n",
      "planet, people, 5.75\n",
      "environment, ecology, 8.81\n",
      "Maradona, football, 8.62\n",
      "OPEC, oil, 8.59\n",
      "money, bank, 8.5\n",
      "computer, software, 8.5\n",
      "law, lawyer, 8.38\n"
     ]
    }
   ],
   "source": [
    "wordsim353 = nlp.data.WordSim353()\n",
    "for i in range(15):\n",
    "    print(*wordsim353[i], sep=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity evaluator\n",
    "\n",
    "The Gluon NLP toolkit includes a `WordEmbeddingSimilarity`  block, which predicts similarity score between word pairs given an embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context = mx.cpu()  # Replace this with mx.gpu(0) if you got a GPU\n",
    "context = mx.gpu(0)  # Replace this with mx.cpu() if you got no GPU\n",
    "\n",
    "evaluator = nlp.embedding.evaluation.WordEmbeddingSimilarity(\n",
    "    idx_to_vec=vocab.embedding.idx_to_vec,\n",
    "    similarity_function=\"CosineSimilarity\")\n",
    "evaluator.initialize(ctx=context)\n",
    "evaluator.hybridize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation: Running the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words1, words2, scores = zip(*([vocab[d[0]], vocab[d[1]], d[2]] for d in wordsim353))\n",
    "words1 = mx.nd.array(words1, ctx=context)\n",
    "words2 = mx.nd.array(words2, ctx=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarities can be predicted by passing the two arrays of words through the evaluator. Thereby the *ith* word in `words1` will be compared with the *ith* word in `words2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0.42459148 0.         0.426882   0.27679962 0.        ]\n",
      "<NDArray 5 @gpu(0)>\n"
     ]
    }
   ],
   "source": [
    "pred_similarity = evaluator(words1, words2)\n",
    "print(pred_similarity[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the predicted similarities, and thereby the word embeddings, by computing the Spearman Rank Correlation between the predicted similarities and the groundtruth, human, similarity scores from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman rank correlation on WordSim353: 0.619\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "sr = stats.spearmanr(pred_similarity.asnumpy(), np.array(scores))\n",
    "print('Spearman rank correlation on {}: {}'.format(wordsim353.__class__.__name__,\n",
    "                                                   sr.correlation.round(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T20:27:25.000472Z",
     "start_time": "2018-03-28T20:27:24.993155Z"
    }
   },
   "source": [
    "### Word Analogy Task\n",
    "\n",
    "In the Word Analogy Task word embeddings are evaluated by inferring an analogous word `D`, which is related to a given word `C` in the same way as a given pair of words `A, B` are related.\n",
    "\n",
    "`gluonnlp` includes a number of common datasets for the Word Analogy Task. The included datasets are listed in the [API documentation](http://gluon-nlp.mxnet.io/api/data.html#word-embedding-evaluation-datasets). In this notebook we use the GoogleAnalogyTestSet dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_analogy = nlp.data.GoogleAnalogyTestSet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first demonstrate the structure of the dataset by printing a few examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing every 1000st analogy question from the 19544 questionsin the Google Analogy Test Set:\n",
      "\n",
      "athens greece baghdad iraq\n",
      "baku azerbaijan dushanbe tajikistan\n",
      "dublin ireland kathmandu nepal\n",
      "lusaka zambia tehran iran\n",
      "rome italy windhoek namibia\n",
      "zagreb croatia astana kazakhstan\n",
      "philadelphia pennsylvania tampa florida\n",
      "wichita kansas shreveport louisiana\n",
      "shreveport louisiana oxnard california\n",
      "complete completely lucky luckily\n",
      "comfortable uncomfortable clear unclear\n",
      "good better high higher\n",
      "young younger tight tighter\n",
      "weak weakest bright brightest\n",
      "slow slowing describe describing\n",
      "ireland irish greece greek\n",
      "feeding fed sitting sat\n",
      "slowing slowed decreasing decreased\n",
      "finger fingers onion onions\n",
      "play plays sing sings\n"
     ]
    }
   ],
   "source": [
    "sample = []\n",
    "print(('Printing every 1000st analogy question '\n",
    "       'from the {} questions'\n",
    "        'in the Google Analogy Test Set:').format(len(google_analogy)))\n",
    "print('')\n",
    "for i in range(0, 19544, 1000):\n",
    "    print(*google_analogy[i])\n",
    "    sample.append(google_analogy[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "words1, words2, words3, words4 = list(zip(*sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up computation, we restrict ourselves here to the most frequent 300000 words in the vocabulary. As well as the first 3000 analogies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 16614 pairs from 19544.\n"
     ]
    }
   ],
   "source": [
    "counter = nlp.data.utils.Counter(embedding.idx_to_token[:300000])\n",
    "vocab = nlp.vocab.Vocab(counter)\n",
    "vocab.set_embedding(embedding)\n",
    "\n",
    "google_analogy_subset = [\n",
    "    d for i, d in enumerate(google_analogy) if\n",
    "    d[0] in vocab and d[1] in vocab and d[2] in vocab and d[3] in vocab and i < 3000\n",
    "]\n",
    "print('Dropped {} pairs from {}.'.format(\n",
    "    len(google_analogy) - len(google_analogy_subset),\n",
    "    len(google_analogy)))\n",
    "\n",
    "google_analogy_coded = [[vocab[d[0]], vocab[d[1]], vocab[d[2]], vocab[d[3]]]\n",
    "                 for d in google_analogy_subset]\n",
    "google_analogy_coded_batched = mx.gluon.data.DataLoader(\n",
    "    google_analogy_coded, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = nlp.embedding.evaluation.WordEmbeddingAnalogy(\n",
    "    idx_to_vec=vocab.embedding.idx_to_vec,\n",
    "    exclude_question_words=True,\n",
    "    analogy_function=\"ThreeCosMul\")\n",
    "evaluator.initialize(ctx=context)\n",
    "evaluator.hybridize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show a visual progressbar, make sure the `tqdm` package is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install  tqdm\n",
    "import sys\n",
    "# workaround for deep learning AMI on EC2\n",
    "sys.path.append('/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:05<00:00,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on GoogleAnalogyTestSet: 0.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import tqdm\n",
    "except:\n",
    "    tqdm = None\n",
    "\n",
    "acc = mx.metric.Accuracy()\n",
    "\n",
    "if tqdm is not None:\n",
    "    google_analogy_coded_batched = tqdm.tqdm(google_analogy_coded_batched)\n",
    "for batch in google_analogy_coded_batched:\n",
    "    batch = batch.as_in_context(context)\n",
    "    words1, words2, words3, words4 = (batch[:, 0], batch[:, 1],\n",
    "                                      batch[:, 2], batch[:, 3])\n",
    "    pred_idxs = evaluator(words1, words2, words3)\n",
    "    acc.update(pred_idxs[:, 0], words4.astype(np.float32))\n",
    "    \n",
    "print('Accuracy on %s: %s'% (google_analogy.__class__.__name__, acc.get()[1].round(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training word embeddings\n",
    "\n",
    "Besides loading pre-trained word embeddings, the toolkit also facilitates training word embedding models with your own datasets. `gluonnlp` provides trainable Blocks for a simple word-level embedding model and the popular FastText embedding model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the training data\n",
    "\n",
    "We can load a word embedding training dataset from the datasets provided by the `gluonnlp` toolkit.\n",
    "\n",
    "Word embedding training datasets are structured as a nested list. The outer list represents sentences in the corpus. The inner lists represents the words in each sentence.\n",
    "\n",
    "We then build a vocabulary of all the tokens in the dataset that occur more than 5 times and code the dataset, ie. replace the words with their indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "dataset = nlp.data.Text8(segment='train')\n",
    "# Here we only consider the first 1000000 words for speed reasons during presentation\n",
    "dataset = [dataset[0][:1000000]]\n",
    "\n",
    "counter = nlp.data.count_tokens(itertools.chain.from_iterable(dataset))\n",
    "vocab = nlp.Vocab(counter, unknown_token=None, padding_token=None,\n",
    "                  bos_token=None, eos_token=None, min_freq=5)\n",
    "coded_dataset = [[vocab[token] for token in sentence if token in vocab] for sentence in dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainable embedding model\n",
    "\n",
    "A word embedding model associates words with word vectors. Each word is represented by it's vocabulary index and the embedding model associates these indices with vectors.\n",
    "\n",
    "`gluonnlp` provides Blocks for simple embedding models as well as models that take into account subword information (covered later). A variety of loss functions exist to train word embedding models. The Skip-Gram objective is a simple and popular objective which we use in this notebook.\n",
    "It was introduced by \"Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. ICLR Workshop , 2013.\"\n",
    "\n",
    "The Skip-Gram objective trains word vectors such that the word vector of a word at some position in a sentence can best predict the surrounding words. We call these words *center* and *context* words.\n",
    "\n",
    "![Skip-Gram model](http://blog.aylien.com/wp-content/uploads/2016/10/skip-gram.png)\n",
    "\n",
    "Skip-Gram and picture from \"Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. ICLR Workshop , 2013.\"\n",
    "\n",
    "\n",
    "For the Skip-Gram objective, we initialize two embedding models: `embedding` and `embedding_out`. `embedding` is used to look up embeddings for the *center* words. `embedding_out` is used for the *context* words.\n",
    "\n",
    "The weights of `embedding` are the final word embedding weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "emsize = 300\n",
    "embedding = nlp.model.train.SimpleEmbeddingModel(\n",
    "    num_tokens=len(vocab),\n",
    "    embedding_size=emsize,\n",
    "    weight_initializer=mx.init.Uniform(scale=1 / emsize),\n",
    ")\n",
    "embedding_out = nlp.model.train.SimpleEmbeddingModel(\n",
    "    num_tokens=len(vocab),\n",
    "    embedding_size=emsize,\n",
    "    weight_initializer=mx.init.Uniform(scale=1 / emsize),\n",
    ")\n",
    "\n",
    "embedding.initialize(ctx=context)\n",
    "embedding_out.initialize(ctx=context)\n",
    "embedding.hybridize()\n",
    "embedding_out.hybridize()\n",
    "\n",
    "params = list(embedding.collect_params().values()) + \\\n",
    "    list(embedding_out.collect_params().values())\n",
    "trainer = mx.gluon.Trainer(params, 'adagrad', dict(learning_rate=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training objective\n",
    "\n",
    "#### Naive objective\n",
    "\n",
    "To naively maximize the Skip-Gram objective, if we sample a center word we need to compute a prediction for every other word in the vocabulary if it occurs in the context of the center word or not. We can then backpropagate and update the parameters to make the prediction of the correct *context* words more likely and of all other words less likely.\n",
    "\n",
    "\n",
    "However, this naive method is computationally very expensive as it requires computing a Softmax function over all words in the vocabulary. Instead, \"Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. ICLR Workshop , 2013.\" introduced *Negative Sampling*.\n",
    "\n",
    "#### Negative sampling\n",
    "\n",
    "*Negative Sampling* means that instead of using a small number of *correct* (or *positive*) *context* and all other (*negative*) words to compute the loss and update the parameters we may choose a small, constant number of *negative* words at random. Negative words are choosen randomly based on their frequency in the training corpus. It is recommend to smoothen the frequency distribution by the factor `0.75`.\n",
    "\n",
    "`gluonnlp` includes a `ContextSampler` and `NegativeSampler`. Once initialized, we can iterate over them to get batches of *center* and *context* words from the `ContextSampler` as well as batches of *negatives* from the `NegativeSampler`.\n",
    "\n",
    "The `ContextSampler` can be initialized with the word embedding training dataset, a batch size and the window size specifying the number of words before and after the *center* word to consider as part of the context. (It is recommended to shuffle the sentences in the dataset before initializing the ContextSampler.) \n",
    "\n",
    "`NegativeSampler` takes a vocabulary with counts, the batch size, the number of samples to consider as well as a smoothing constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_sampler = nlp.data.ContextSampler(coded=coded_dataset,\n",
    "    batch_size=1024, window=5)\n",
    "negatives_sampler = nlp.data.NegativeSampler(\n",
    "    num_samples=context_sampler.num_samples,\n",
    "    batch_size=context_sampler.batch_size,\n",
    "    vocab=vocab, negative=5, power=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a model with the *center*, *context* and *negative* batches, we use a `SigmoidBinaryCrossEntropyLoss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = mx.gluon.loss.SigmoidBinaryCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 917/917 [00:07<00:00, 122.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# The context sampler exposes the number of batches\n",
    "# in the training dataset as it's length \n",
    "num_batches = len(context_sampler)\n",
    "\n",
    "# For better usability the iterator over batches can be wrapped in\n",
    "# tqdm.tqdm which provides a visual progressbar\n",
    "iterable = enumerate(zip(context_sampler, negatives_sampler))\n",
    "if tqdm is not None:\n",
    "    iterable = tqdm.tqdm(iterable, total=num_batches)\n",
    "\n",
    "# We iterate over all batches in the context_sampler and negative_sampler\n",
    "for i, (batch, negatives) in iterable:\n",
    "    \n",
    "    # Each batch from the context_sampler includes\n",
    "    # a batch of center words, their contexts as well\n",
    "    # as a mask as the contexts can be of varying lengths\n",
    "    (center, word_context, word_context_mask) = batch\n",
    "\n",
    "    # We copy all data to the GPU\n",
    "    center = mx.nd.array(center, ctx=context)\n",
    "    center_mask = mx.nd.ones((center.shape[0], ), ctx=center.context)\n",
    "    word_context = mx.nd.array(word_context, ctx=context)\n",
    "    word_context_mask = mx.nd.array(word_context_mask, ctx=context)\n",
    "    negatives = mx.nd.array(negatives, ctx=context)\n",
    "    \n",
    "    # We concatenate the positive context words and negatives\n",
    "    # to a single ndarray \n",
    "    word_context_negatives = mx.nd.concat(word_context, negatives, dim=1)\n",
    "    word_context_negatives_mask = mx.nd.concat(\n",
    "        word_context_mask, mx.nd.ones_like(negatives), dim=1)\n",
    "\n",
    "    # We record the gradient of one forward pass\n",
    "    with mx.autograd.record():\n",
    "        # 1. Compute the embedding of the center words\n",
    "        # (The center_mask is constant 1, ie. doesn't mask anything)\n",
    "        emb_in = embedding(center, center_mask)\n",
    "        \n",
    "        # 2. Compute the context embedding\n",
    "        emb_out = embedding_out(word_context_negatives,\n",
    "                                word_context_negatives_mask)\n",
    "\n",
    "        # 3. Compute the prediction\n",
    "        # To predict if a context work is likely or not, the dot product\n",
    "        # between the word vector of the center word and the output weights\n",
    "        # of the context / negative words is computed and passed through a\n",
    "        # Sigmoid function\n",
    "        pred = mx.nd.batch_dot(emb_in.expand_dims(1), emb_out.swapaxes(1, 2))\n",
    "        pred = pred.squeeze() * word_context_negatives_mask\n",
    "        label = mx.nd.concat(word_context_mask, mx.nd.zeros_like(negatives), dim=1)\n",
    "\n",
    "        # 4. Compute the Loss function (SigmoidBinaryCrossEntropyLoss)\n",
    "        loss = loss_function(pred, label)\n",
    "\n",
    "    # Compute the gradient\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the parameters\n",
    "    trainer.step(batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of trained embedding\n",
    "\n",
    "As we have only obtained word vectors for words that occured in the training corpus,\n",
    "we filter the evaluation dataset and exclude out of vocabulary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "words1, words2, scores = zip(*([vocab[d[0]], vocab[d[1]], d[2]]\n",
    "    for d in wordsim353  if d[0] in vocab and d[1] in vocab))\n",
    "words1 = mx.nd.array(words1, ctx=context)\n",
    "words2 = mx.nd.array(words2, ctx=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained embedding model exposes a `to_token_embedding` method that allows extract a set of vectors for a given list of tokens. The `TokenEmbedding` is the object that is also used to represent the pretrained embeddings. We use it to compute a an evaluation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embedding = embedding.to_token_embedding(vocab.idx_to_token,\n",
    "                                               vocab.token_to_idx, ctx=context)\n",
    "evaluator = nlp.embedding.evaluation.WordEmbeddingSimilarity(\n",
    "    idx_to_vec=token_embedding.idx_to_vec,\n",
    "    similarity_function=\"CosineSimilarity\")\n",
    "evaluator.initialize(ctx=context)\n",
    "evaluator.hybridize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman rank correlation on 349 pairs of WordSim353 (total 455): 0.328\n"
     ]
    }
   ],
   "source": [
    "pred_similarity = evaluator(words1, words2)\n",
    "sr = stats.spearmanr(pred_similarity.asnumpy(), np.array(scores))\n",
    "print('Spearman rank correlation on {} pairs of {} (total {}): {}'.format(\n",
    "    len(words1), wordsim353.__class__.__name__, len(wordsim353), sr.correlation.round(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unknown token handling and subword information\n",
    "\n",
    "Sometimes we may run into a word for which the embedding does not include a word vector. While the `vocab` object is happy to replace it with a special index for unknown tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is \"hello\" known?  True\n",
      "Is \"likelyunknown\" known?  False\n"
     ]
    }
   ],
   "source": [
    "print('Is \"hello\" known? ', 'hello' in vocab)\n",
    "print('Is \"likelyunknown\" known? ', 'likelyunknown' in vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some embedding models such as the FastText model support computing word vectors for unknown words by taking into account their subword units.\n",
    "\n",
    "\n",
    "\n",
    "- Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. ICLR Workshop , 2013."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training word embeddings with subword information\n",
    "\n",
    "`gluonnlp` provides the concept of a SubwordFunction which maps words to a list of indices representing their subword.\n",
    "Possible SubwordFunctions include mapping a word to the sequence of it's characters/bytes or hashes of all its ngrams.\n",
    "\n",
    "FastText models use a hash function to map each ngram of a word to a number in range `[0, num_subwords)`. We include the same hash function.\n",
    "\n",
    "### Concept of a SubwordFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<the>\t[1151 1980 6280 9726  934]\n",
      "<of>\t[7102 8930 4528]\n",
      "<and>\t[8080 5046 5443 5020 9624]\n"
     ]
    }
   ],
   "source": [
    "subword_function = nlp.vocab.create_subword_function(\n",
    "    'NGramHashes', ngrams=[3, 4], num_subwords=10000)\n",
    "\n",
    "idx_to_subwordidxs = subword_function(vocab.idx_to_token)\n",
    "for word, subwords in zip(vocab.idx_to_token[:3], idx_to_subwordidxs[:3]):\n",
    "    print('<'+word+'>', subwords, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As words are of varying length, we have to pad the lists of subwords to obtain a batch. To distinguish padded values from valid subword indices we use a mask.\n",
    "We first pad the subword arrays with `-1`, compute the mask and change the `-1` entries to some valid subword index (here `0`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[1151 1980 6280 9726  934]\n",
      " [7102 8930 4528    0    0]\n",
      " [8080 5046 5443 5020 9624]]\n",
      "<NDArray 3x5 @cpu_shared(0)>\n",
      "\n",
      "[[1 1 1 1 1]\n",
      " [1 1 1 0 0]\n",
      " [1 1 1 1 1]]\n",
      "<NDArray 3x5 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "subword_padding = nlp.data.batchify.Pad(pad_val=-1)\n",
    "\n",
    "subwords = subword_padding(idx_to_subwordidxs[:3])\n",
    "subwords_mask = subwords != -1\n",
    "subwords += subwords == -1  # -1 is invalid. Change to 0\n",
    "print(subwords)\n",
    "print(subwords_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable fast training, we precompute the mapping from the  words in  our training corpus to  the  subword indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute a idx to subwordidxs mapping to support fast lookup\n",
    "idx_to_subwordidxs = list(subword_function(vocab.idx_to_token))\n",
    "max_subwordidxs_len = max(len(s) for s in idx_to_subwordidxs)\n",
    "\n",
    "# Padded max_subwordidxs_len + 1 so each row contains at least one -1\n",
    "# element which can be found by np.argmax below.\n",
    "idx_to_subwordidxs = np.stack(\n",
    "    np.pad(b, (0, max_subwordidxs_len - len(b) + 1), \\\n",
    "           constant_values=-1, mode='constant')\n",
    "    for b in idx_to_subwordidxs).astype(np.float32)\n",
    "\n",
    "def indices_to_subwordindices_mask(indices, idx_to_subwordidxs):\n",
    "    \"\"\"Return array of subwordindices for indices.\n",
    "\n",
    "    A padded numpy array and a mask is returned. The mask is used as\n",
    "    indices map to varying length subwords.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    indices : list of int, numpy array or mxnet NDArray\n",
    "        Token indices that should be mapped to subword indices.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Array of subword indices.\n",
    "\n",
    "    \"\"\"\n",
    "    if isinstance(indices, mx.nd.NDArray):\n",
    "        indices = indices.asnumpy().astype(np.int)\n",
    "    else:\n",
    "        indices = np.array(indices, dtype=np.int)\n",
    "    subwords = idx_to_subwordidxs[indices]\n",
    "    mask = np.zeros_like(subwords)\n",
    "    mask += subwords != -1\n",
    "    subwords += subwords == -1\n",
    "    lengths = np.argmax(subwords == -1, axis=1)\n",
    "\n",
    "    new_length = max(np.max(lengths), 1)\n",
    "    subwords = subwords[:, :new_length]\n",
    "    mask = mask[:, :new_length]\n",
    "\n",
    "    return subwords, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model\n",
    "\n",
    "Instead of the `SimpleEmbeddingModel` we now train a `FasttextEmbeddingModel` Block which can combine the word and subword information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "emsize = 300\n",
    "embedding = nlp.model.train.FasttextEmbeddingModel(\n",
    "    num_tokens=len(vocab),\n",
    "    num_subwords=len(subword_function),\n",
    "    embedding_size=emsize,\n",
    "    weight_initializer=mx.init.Uniform(scale=1 / emsize),\n",
    ")\n",
    "embedding_out = nlp.model.train.SimpleEmbeddingModel(\n",
    "    num_tokens=len(vocab),\n",
    "    embedding_size=emsize,\n",
    "    weight_initializer=mx.init.Uniform(scale=1 / emsize),\n",
    ")\n",
    "loss_function = mx.gluon.loss.SigmoidBinaryCrossEntropyLoss()\n",
    "\n",
    "embedding.initialize(ctx=context)\n",
    "embedding_out.initialize(ctx=context)\n",
    "embedding.hybridize()\n",
    "embedding_out.hybridize()\n",
    "\n",
    "params = list(embedding.collect_params().values()) + \\\n",
    "    list(embedding_out.collect_params().values())\n",
    "trainer = mx.gluon.Trainer(params, 'adagrad', dict(learning_rate=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Compared to training the `SimpleEmbeddingModel`, we now also look up the subwords of each center word in the batch and pass the subword infor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 917/917 [00:05<00:00, 154.08it/s]\n"
     ]
    }
   ],
   "source": [
    "num_batches = len(context_sampler)\n",
    "iterable = enumerate(zip(context_sampler, negatives_sampler))\n",
    "if tqdm is not None:\n",
    "    iterable = tqdm.tqdm(iterable, total=num_batches)\n",
    "\n",
    "for i, (batch, negatives) in iterable:\n",
    "    (center, word_context, word_context_mask) = batch\n",
    "    \n",
    "    # Get subwords\n",
    "    subwords, subwords_mask = indices_to_subwordindices_mask(center, idx_to_subwordidxs)\n",
    "\n",
    "    # To GPU\n",
    "    center = mx.nd.array(center, ctx=context)\n",
    "    center_mask = mx.nd.ones((center.shape[0], ), ctx=center.context)\n",
    "    word_context = mx.nd.array(word_context, ctx=context)\n",
    "    word_context_mask = mx.nd.array(word_context_mask, ctx=context)\n",
    "    negatives = mx.nd.array(negatives, ctx=context)\n",
    "    subwords = mx.nd.array(subwords, ctx=context)\n",
    "    subwords_mask = mx.nd.array(subwords_mask, dtype=np.float32).as_in_context(context)\n",
    "\n",
    "    with mx.autograd.record():\n",
    "        emb_in = embedding(center, center_mask, subwords, subwords_mask)\n",
    "\n",
    "        word_context_negatives = mx.nd.concat(\n",
    "            word_context, negatives, dim=1)\n",
    "        word_context_negatives_mask = mx.nd.concat(\n",
    "            word_context_mask, mx.nd.ones_like(negatives), dim=1)\n",
    "\n",
    "        emb_out = embedding_out(word_context_negatives,\n",
    "                                word_context_negatives_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        pred = mx.nd.batch_dot(\n",
    "            emb_in.expand_dims(1), emb_out.swapaxes(1, 2))\n",
    "        pred = pred.squeeze() * word_context_negatives_mask\n",
    "        label = mx.nd.concat(word_context_mask,\n",
    "                             mx.nd.zeros_like(negatives), dim=1)\n",
    "\n",
    "        loss = loss_function(pred, label)\n",
    "\n",
    "    loss.backward()\n",
    "    trainer.step(batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Thanks to the subword support of the `FasttextEmbeddingModel` we can now evaluate on all words in the evaluation dataset, not only the ones that we observed during training (the `SimpleEmbeddingModel` only provides vectors for words observed at training).\n",
    "\n",
    "We first find the all tokens in the evaluation dataset and then convert the `FasttextEmbeddingModel` to a `TokenEmbedding` with exactly those tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 437 unique tokens in WordSim353\n",
      "The imputed TokenEmbedding has shape (437, 300)\n"
     ]
    }
   ],
   "source": [
    "wordsim353_tokens  = list(set(itertools.chain.from_iterable((d[0], d[1]) for d in wordsim353)))\n",
    "token_embedding = embedding.to_token_embedding(wordsim353_tokens, vocab.token_to_idx, subword_function, ctx=context)\n",
    "\n",
    "print('There are', len(wordsim353_tokens), 'unique tokens in WordSim353')\n",
    "print('The imputed TokenEmbedding has shape', token_embedding.idx_to_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = nlp.embedding.evaluation.WordEmbeddingSimilarity(\n",
    "    idx_to_vec=token_embedding.idx_to_vec,\n",
    "    similarity_function=\"CosineSimilarity\")\n",
    "evaluator.initialize(ctx=context)\n",
    "evaluator.hybridize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "words1, words2, scores = zip(*([token_embedding.token_to_idx[d[0]],\n",
    "                                token_embedding.token_to_idx[d[1]],\n",
    "                                d[2]] for d in wordsim353))\n",
    "words1 = mx.nd.array(words1, ctx=context)\n",
    "words2 = mx.nd.array(words2, ctx=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman rank correlation on 455 pairs of WordSim353: 0.221\n"
     ]
    }
   ],
   "source": [
    "pred_similarity = evaluator(words1, words2)\n",
    "sr = stats.spearmanr(pred_similarity.asnumpy(), np.array(scores))\n",
    "print('Spearman rank correlation on {} pairs of {}: {}'.format(\n",
    "    len(words1), wordsim353.__class__.__name__, sr.correlation.round(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pretrained FastText models with subword information\n",
    "\n",
    "As the `FasttextEmbeddingModel` in `gluonnlp` uses the same structure as the models provided by `facebookresearch/fasttext` it is possible to load models trained by `facebookresearch/fasttext` into the `FasttextEmbeddingModel`. However, be careful to specify the same ngram sizes for the `NGramHashes` subword function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(path, context):\n",
    "    import gensim\n",
    "    import struct\n",
    "    \n",
    "    assert '.bin' in path  # Assume binary fasttext format\n",
    "\n",
    "    gensim_fasttext = gensim.models.FastText()\n",
    "    gensim_fasttext.file_name = path\n",
    "    with open(path, 'rb') as f:\n",
    "        gensim_fasttext._load_model_params(f)\n",
    "        gensim_fasttext._load_dict(f)\n",
    "\n",
    "        if gensim_fasttext.new_format:\n",
    "            # quant input\n",
    "            gensim_fasttext.struct_unpack(f, '@?')\n",
    "        num_vectors, dim = gensim_fasttext.struct_unpack(f, '@2q')\n",
    "        assert gensim_fasttext.wv.vector_size == dim\n",
    "        dtype = np.float32 if struct.calcsize('@f') == 4 else np.float64\n",
    "        matrix = np.fromfile(f, dtype=dtype, count=num_vectors * dim)\n",
    "        matrix = matrix.reshape((-1, dim))\n",
    "\n",
    "        num_words = len(gensim_fasttext.wv.vocab)\n",
    "        num_subwords = gensim_fasttext.bucket\n",
    "        assert num_words + num_subwords == num_vectors\n",
    "\n",
    "    idx_to_token = list(gensim_fasttext.wv.vocab.keys())\n",
    "    idx_to_vec = mx.nd.array(matrix[:num_words])\n",
    "    token_to_idx = {(token, idx) for idx, token in enumerate(idx_to_token)}\n",
    "\n",
    "    assert num_subwords\n",
    "    subword_function = nlp.vocab.create_subword_function(\n",
    "        'NGramHashes', num_subwords=num_subwords)\n",
    "\n",
    "    embedding = nlp.model.train.FasttextEmbeddingModel(\n",
    "        num_tokens=len(token_to_idx),\n",
    "        num_subwords=len(subword_function),\n",
    "        embedding_size=dim,\n",
    "    )\n",
    "\n",
    "    embedding.initialize(ctx=context)\n",
    "    embedding.embedding.weight.set_data(idx_to_vec)\n",
    "    embedding.subword_embedding.embedding.weight.set_data(\n",
    "        mx.nd.array(matrix[num_words:]))\n",
    "\n",
    "    return embedding, idx_to_token, token_to_idx, subword_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/scipy/sparse/sparsetools.py:20: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/gensim/models/fasttext.py:273: DeprecationWarning: Call to deprecated `bucket` (Attribute will be removed in 4.0.0, use trainables.bucket instead).\n",
      "  self.wv.bucket = self.bucket\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:23: DeprecationWarning: Call to deprecated `bucket` (Attribute will be removed in 4.0.0, use trainables.bucket instead).\n"
     ]
    }
   ],
   "source": [
    "embedding, idx_to_token, token_to_idx, subword_function = get_model(\n",
    "    '/home/ubuntu/software/fastText/skipgramtext8-e5-ngrams.bin', context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman rank correlation on 455 pairs of WordSim353: 0.58\n"
     ]
    }
   ],
   "source": [
    "token_embedding = embedding.to_token_embedding(wordsim353_tokens, vocab.token_to_idx, subword_function, ctx=context)\n",
    "\n",
    "evaluator = nlp.embedding.evaluation.WordEmbeddingSimilarity(\n",
    "    idx_to_vec=token_embedding.idx_to_vec,\n",
    "    similarity_function=\"CosineSimilarity\")\n",
    "evaluator.initialize(ctx=context)\n",
    "evaluator.hybridize()\n",
    "\n",
    "pred_similarity = evaluator(words1, words2)\n",
    "sr = stats.spearmanr(pred_similarity.asnumpy(), np.array(scores))\n",
    "print('Spearman rank correlation on {} pairs of {}: {}'.format(\n",
    "    len(words1), wordsim353.__class__.__name__, sr.correlation.round(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
