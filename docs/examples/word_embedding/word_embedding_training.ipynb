{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings Training and Evaluation\n",
    "\n",
    "## Evaluating Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous example has introduced how to load  pre-trained word embeddings from a set of sources included in the Gluon NLP toolkit. It was shown how make use of the word vectors to find the top most similar words of a given words or to solve the analogy task.\n",
    "\n",
    "Besides manually investigating similar words or the predicted analogous words, we can facilitate word embedding evaluation datasets to quantify the evaluation.\n",
    "\n",
    "Datasets for the *similarity* task come with a list of word pairs together with a human similarity judgement. The task is to recover the order of most-similar to least-similar pairs.\n",
    "\n",
    "Datasets for the *analogy* tasks supply a set of analogy quadruples of the form  ‘a : b :: c : d’ and the task is to recover find the correct ‘d’ in as many cases as possible given just ‘a’, ‘b’, ‘c’. For instance, “man : woman :: son : daughter” is an analogy.\n",
    "\n",
    "The Gluon NLP toolkit includes a set of popular *similarity* and *analogy* task datasets as well as helpers for computing the evaluation scores. Here we show how to make use of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MXNET_FORCE_ADDTAKEGRAD=1\n"
     ]
    }
   ],
   "source": [
    "# Workaround for https://github.com/apache/incubator-mxnet/issues/11314\n",
    "%env MXNET_FORCE_ADDTAKEGRAD = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T20:27:24.991031Z",
     "start_time": "2018-03-28T20:27:23.669296Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "import logging\n",
    "import random\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import mxnet as mx\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# context = mx.cpu()  # Enable this to run on CPU\n",
    "context = mx.gpu(0)  # Enable this to run on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load pretrained FastText word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nlp.embedding.create('fasttext', source='crawl-300d-2M')\n",
    "\n",
    "vocab = nlp.Vocab(nlp.data.Counter(embedding.idx_to_token))\n",
    "vocab.set_embedding(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T20:27:25.000472Z",
     "start_time": "2018-03-28T20:27:24.993155Z"
    }
   },
   "source": [
    "### Word Similarity and Relatedness Task\n",
    "\n",
    "Word embeddings should capture the relationsship between words in natural language.\n",
    "In the Word Similarity and Relatedness Task word embeddings are evaluated by comparing word similarity scores computed from a pair of words with human labels for the similarity or relatedness of the pair.\n",
    "\n",
    "`gluonnlp` includes a number of common datasets for the Word Similarity and Relatedness Task. The included datasets are listed in the [API documentation](http://gluon-nlp.mxnet.io/api/data.html#word-embedding-evaluation-datasets). We use several of them in the evaluation example below.\n",
    "\n",
    "We first show a few samples from the WordSim353 dataset, to get an overall feeling of the Dataset structur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer, keyboard, 7.62\n",
      "Jerusalem, Israel, 8.46\n",
      "planet, galaxy, 8.11\n",
      "canyon, landscape, 7.53\n",
      "OPEC, country, 5.63\n",
      "day, summer, 3.94\n",
      "day, dawn, 7.53\n",
      "country, citizen, 7.31\n",
      "planet, people, 5.75\n",
      "environment, ecology, 8.81\n",
      "Maradona, football, 8.62\n",
      "OPEC, oil, 8.59\n",
      "money, bank, 8.5\n",
      "computer, software, 8.5\n",
      "law, lawyer, 8.38\n"
     ]
    }
   ],
   "source": [
    "wordsim353 = nlp.data.WordSim353()\n",
    "for i in range(15):\n",
    "    print(*wordsim353[i], sep=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity evaluator\n",
    "\n",
    "The Gluon NLP toolkit includes a `WordEmbeddingSimilarity`  block, which predicts similarity score between word pairs given an embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = nlp.embedding.evaluation.WordEmbeddingSimilarity(\n",
    "    idx_to_vec=vocab.embedding.idx_to_vec,\n",
    "    similarity_function=\"CosineSimilarity\")\n",
    "evaluator.initialize(ctx=context)\n",
    "evaluator.hybridize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation: Running the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words1, words2, scores = zip(*([vocab[d[0]], vocab[d[1]], d[2]] for d in wordsim353))\n",
    "words1 = mx.nd.array(words1, ctx=context)\n",
    "words2 = mx.nd.array(words2, ctx=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarities can be predicted by passing the two arrays of words through the evaluator. Thereby the *ith* word in `words1` will be compared with the *ith* word in `words2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0.4934404  0.69630307 0.5902223  0.31201977 0.16985895]\n",
      "<NDArray 5 @gpu(0)>\n"
     ]
    }
   ],
   "source": [
    "pred_similarity = evaluator(words1, words2)\n",
    "print(pred_similarity[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the predicted similarities, and thereby the word embeddings, by computing the Spearman Rank Correlation between the predicted similarities and the groundtruth, human, similarity scores from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman rank correlation on WordSim353: 0.792\n"
     ]
    }
   ],
   "source": [
    "sr = stats.spearmanr(pred_similarity.asnumpy(), np.array(scores))\n",
    "print('Spearman rank correlation on {}: {}'.format(wordsim353.__class__.__name__,\n",
    "                                                   sr.correlation.round(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T20:27:25.000472Z",
     "start_time": "2018-03-28T20:27:24.993155Z"
    }
   },
   "source": [
    "### Word Analogy Task\n",
    "\n",
    "In the Word Analogy Task word embeddings are evaluated by inferring an analogous word `D`, which is related to a given word `C` in the same way as a given pair of words `A, B` are related.\n",
    "\n",
    "`gluonnlp` includes a number of common datasets for the Word Analogy Task. The included datasets are listed in the [API documentation](http://gluon-nlp.mxnet.io/api/data.html#word-embedding-evaluation-datasets). In this notebook we use the GoogleAnalogyTestSet dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_analogy = nlp.data.GoogleAnalogyTestSet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first demonstrate the structure of the dataset by printing a few examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing every 1000st analogy question from the 19544 questionsin the Google Analogy Test Set:\n",
      "\n",
      "athens greece baghdad iraq\n",
      "baku azerbaijan dushanbe tajikistan\n",
      "dublin ireland kathmandu nepal\n",
      "lusaka zambia tehran iran\n",
      "rome italy windhoek namibia\n",
      "zagreb croatia astana kazakhstan\n",
      "philadelphia pennsylvania tampa florida\n",
      "wichita kansas shreveport louisiana\n",
      "shreveport louisiana oxnard california\n",
      "complete completely lucky luckily\n",
      "comfortable uncomfortable clear unclear\n",
      "good better high higher\n",
      "young younger tight tighter\n",
      "weak weakest bright brightest\n",
      "slow slowing describe describing\n",
      "ireland irish greece greek\n",
      "feeding fed sitting sat\n",
      "slowing slowed decreasing decreased\n",
      "finger fingers onion onions\n",
      "play plays sing sings\n"
     ]
    }
   ],
   "source": [
    "sample = []\n",
    "print(('Printing every 1000st analogy question '\n",
    "       'from the {} questions'\n",
    "        'in the Google Analogy Test Set:').format(len(google_analogy)))\n",
    "print('')\n",
    "for i in range(0, 19544, 1000):\n",
    "    print(*google_analogy[i])\n",
    "    sample.append(google_analogy[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "words1, words2, words3, words4 = list(zip(*sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We restrict ourselves here to the first (most frequent) 300000 words of the pretrained embedding as well as all tokens that occur in the evaluation datasets as possible answers to the analogy questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using most frequent 300000 + 96 extra words\n",
      "Dropped 1781 pairs from 19544 as they were OOV.\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "most_freq = 300000\n",
    "counter = nlp.data.utils.Counter(embedding.idx_to_token[:most_freq])\n",
    "google_analogy_tokens = set(itertools.chain.from_iterable((d[0], d[1], d[2], d[3]) for d in google_analogy))\n",
    "counter.update(t for t in google_analogy_tokens if t in embedding)\n",
    "\n",
    "vocab = nlp.vocab.Vocab(counter)\n",
    "vocab.set_embedding(embedding)\n",
    "\n",
    "print(\"Using most frequent {} + {} extra words\".format(most_freq, len(vocab) - most_freq))\n",
    "\n",
    "\n",
    "google_analogy_subset = [\n",
    "    d for i, d in enumerate(google_analogy) if\n",
    "    d[0] in vocab and d[1] in vocab and d[2] in vocab and d[3] in vocab\n",
    "]\n",
    "print('Dropped {} pairs from {} as they were OOV.'.format(\n",
    "    len(google_analogy) - len(google_analogy_subset),\n",
    "    len(google_analogy)))\n",
    "\n",
    "google_analogy_coded = [[vocab[d[0]], vocab[d[1]], vocab[d[2]], vocab[d[3]]]\n",
    "                 for d in google_analogy_subset]\n",
    "google_analogy_coded_batched = mx.gluon.data.DataLoader(\n",
    "    google_analogy_coded, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = nlp.embedding.evaluation.WordEmbeddingAnalogy(\n",
    "    idx_to_vec=vocab.embedding.idx_to_vec,\n",
    "    exclude_question_words=True,\n",
    "    analogy_function=\"ThreeCosMul\")\n",
    "evaluator.initialize(ctx=context)\n",
    "evaluator.hybridize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show a visual progressbar, make sure the `tqdm` package is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install  tqdm\n",
    "import sys\n",
    "# workaround for deep learning AMI on EC2\n",
    "sys.path.append('/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:34<00:00,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on GoogleAnalogyTestSet: 0.772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import tqdm\n",
    "except:\n",
    "    tqdm = None\n",
    "\n",
    "acc = mx.metric.Accuracy()\n",
    "\n",
    "if tqdm is not None:\n",
    "    google_analogy_coded_batched = tqdm.tqdm(google_analogy_coded_batched)\n",
    "for batch in google_analogy_coded_batched:\n",
    "    batch = batch.as_in_context(context)\n",
    "    words1, words2, words3, words4 = (batch[:, 0], batch[:, 1],\n",
    "                                      batch[:, 2], batch[:, 3])\n",
    "    pred_idxs = evaluator(words1, words2, words3)\n",
    "    acc.update(pred_idxs[:, 0], words4.astype(np.float32))\n",
    "    \n",
    "print('Accuracy on %s: %s'% (google_analogy.__class__.__name__, acc.get()[1].round(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training word embeddings\n",
    "\n",
    "Besides loading pre-trained word embeddings, the toolkit also facilitates training word embedding models with your own datasets. `gluonnlp` provides trainable Blocks for a simple word-level embedding model and the popular FastText embedding model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the training data\n",
    "\n",
    "We can load a word embedding training dataset from the datasets provided by the `gluonnlp` toolkit.\n",
    "\n",
    "Word embedding training datasets are structured as a nested list. The outer list represents sentences in the corpus. The inner lists represents the words in each sentence.\n",
    "\n",
    "We then build a vocabulary of all the tokens in the dataset that occur more than 5 times and code the dataset, ie. replace the words with their indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_token_subsampling = 1E-4\n",
    "\n",
    "import itertools\n",
    "dataset = nlp.data.Text8(segment='train')\n",
    "\n",
    "counter = nlp.data.count_tokens(itertools.chain.from_iterable(dataset))\n",
    "vocab = nlp.Vocab(counter, unknown_token=None, padding_token=None,\n",
    "                  bos_token=None, eos_token=None, min_freq=5)\n",
    "idx_to_counts = np.array([counter[w] for w in vocab.idx_to_token])\n",
    "f = idx_to_counts / np.sum(idx_to_counts)\n",
    "idx_to_pdiscard = 1 - np.sqrt(frequent_token_subsampling / f)\n",
    "\n",
    "coded_dataset = [[vocab[token] for token in sentence\n",
    "                  if token in vocab\n",
    "                  and random.uniform(0, 1) > idx_to_pdiscard[vocab[token]]] for sentence in dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainable embedding model\n",
    "\n",
    "A word embedding model associates words with word vectors. Each word is represented by it's vocabulary index and the embedding model associates these indices with vectors.\n",
    "\n",
    "`gluonnlp` provides Blocks for simple embedding models as well as models that take into account subword information (covered later). A variety of loss functions exist to train word embedding models. The Skip-Gram objective is a simple and popular objective which we use in this notebook.\n",
    "It was introduced by \"Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. ICLR Workshop , 2013.\"\n",
    "\n",
    "The Skip-Gram objective trains word vectors such that the word vector of a word at some position in a sentence can best predict the surrounding words. We call these words *center* and *context* words.\n",
    "\n",
    "![Skip-Gram model](http://blog.aylien.com/wp-content/uploads/2016/10/skip-gram.png)\n",
    "\n",
    "Skip-Gram and picture from \"Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. ICLR Workshop , 2013.\"\n",
    "\n",
    "\n",
    "For the Skip-Gram objective, we initialize two embedding models: `embedding` and `embedding_out`. `embedding` is used to look up embeddings for the *center* words. `embedding_out` is used for the *context* words.\n",
    "\n",
    "The weights of `embedding` are the final word embedding weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "emsize = 300\n",
    "embedding = nlp.model.train.SimpleEmbeddingModel(\n",
    "    token_to_idx=vocab.token_to_idx,\n",
    "    embedding_size=emsize,\n",
    "    weight_initializer=mx.init.Uniform(scale=1 / emsize))\n",
    "embedding_out = nlp.model.train.SimpleEmbeddingModel(\n",
    "    token_to_idx=vocab.token_to_idx,\n",
    "    embedding_size=emsize,\n",
    "    weight_initializer=mx.init.Uniform(scale=1 / emsize))\n",
    "\n",
    "embedding.initialize(ctx=context)\n",
    "embedding_out.initialize(ctx=context)\n",
    "embedding.hybridize(static_alloc=True)\n",
    "embedding_out.hybridize(static_alloc=True)\n",
    "\n",
    "params = list(embedding.collect_params().values()) + \\\n",
    "    list(embedding_out.collect_params().values())\n",
    "trainer = mx.gluon.Trainer(params, 'adagrad', dict(learning_rate=0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training objective\n",
    "\n",
    "#### Naive objective\n",
    "\n",
    "To naively maximize the Skip-Gram objective, if we sample a center word we need to compute a prediction for every other word in the vocabulary if it occurs in the context of the center word or not. We can then backpropagate and update the parameters to make the prediction of the correct *context* words more likely and of all other words less likely.\n",
    "\n",
    "\n",
    "However, this naive method is computationally very expensive as it requires computing a Softmax function over all words in the vocabulary. Instead, \"Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. ICLR Workshop , 2013.\" introduced *Negative Sampling*.\n",
    "\n",
    "#### Negative sampling\n",
    "\n",
    "*Negative Sampling* means that instead of using a small number of *correct* (or *positive*) *context* and all other (*negative*) words to compute the loss and update the parameters we may choose a small, constant number of *negative* words at random. Negative words are choosen randomly based on their frequency in the training corpus. It is recommend to smoothen the frequency distribution by the factor `0.75`.\n",
    "\n",
    "`gluonnlp` includes a `ContextSampler` and `NegativeSampler`. Once initialized, we can iterate over them to get batches of *center* and *context* words from the `ContextSampler` as well as batches of *negatives* from the `NegativeSampler`.\n",
    "\n",
    "The `ContextSampler` can be initialized with the word embedding training dataset, a batch size and the window size specifying the number of words before and after the *center* word to consider as part of the context. (It is recommended to shuffle the sentences in the dataset before initializing the ContextSampler.) \n",
    "\n",
    "`NegativeSampler` takes a vocabulary with counts, the batch size, the number of samples to consider as well as a smoothing constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_sampler = nlp.data.ContextSampler(coded=coded_dataset, batch_size=2048, window=5)\n",
    "\n",
    "negatives_weights = mx.nd.array([counter[w] for w in vocab.idx_to_token])**0.75\n",
    "negatives_sampler = nlp.data.UnigramCandidateSampler(negatives_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a model with the *center*, *context* and *negative* batches, we use a `SigmoidBinaryCrossEntropyLoss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = mx.gluon.loss.SigmoidBinaryCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 500/4118] loss=0.3939, throughput=125.90K wps, wc=1024.00K\n",
      "[Batch 1000/4118] loss=0.3641, throughput=181.37K wps, wc=1024.00K\n",
      "[Batch 1500/4118] loss=0.3553, throughput=189.75K wps, wc=1024.00K\n",
      "[Batch 2000/4118] loss=0.3509, throughput=186.19K wps, wc=1024.00K\n",
      "[Batch 2500/4118] loss=0.3464, throughput=180.96K wps, wc=1024.00K\n",
      "[Batch 3000/4118] loss=0.3440, throughput=187.66K wps, wc=1024.00K\n",
      "[Batch 3500/4118] loss=0.3426, throughput=181.33K wps, wc=1024.00K\n",
      "[Batch 4000/4118] loss=0.3393, throughput=176.82K wps, wc=1024.00K\n"
     ]
    }
   ],
   "source": [
    "# The context sampler exposes the number of batches\n",
    "# in the training dataset as it's length \n",
    "num_batches = len(context_sampler)\n",
    "num_negatives = 5\n",
    "\n",
    "# Logging variables\n",
    "log_interval = 500\n",
    "log_wc = 0\n",
    "log_start_time = time.time()\n",
    "log_avg_loss = 0\n",
    "\n",
    "# We iterate over all batches in the context_sampler\n",
    "for i, batch in enumerate(context_sampler):\n",
    "    # Each batch from the context_sampler includes\n",
    "    # a batch of center words, their contexts as well\n",
    "    # as a mask as the contexts can be of varying lengths\n",
    "    (center, word_context, word_context_mask) = batch\n",
    "    \n",
    "    negatives_shape = (word_context.shape[0],\n",
    "                       word_context.shape[1] * num_negatives)\n",
    "    negatives, negatives_mask = negatives_sampler(\n",
    "        negatives_shape, word_context, word_context_mask)\n",
    "\n",
    "    # We copy all data to the GPU\n",
    "    center = center.as_in_context(context)\n",
    "    word_context = word_context.as_in_context(context)\n",
    "    word_context_mask = word_context_mask.as_in_context(context)\n",
    "    negatives = negatives.as_in_context(context)\n",
    "    negatives_mask = negatives_mask.as_in_context(context)\n",
    "\n",
    "    \n",
    "    # We concatenate the positive context words and negatives\n",
    "    # to a single ndarray \n",
    "    word_context_negatives = mx.nd.concat(word_context, negatives, dim=1)\n",
    "    word_context_negatives_mask = mx.nd.concat(word_context_mask, negatives_mask, dim=1)\n",
    "\n",
    "    # We record the gradient of one forward pass\n",
    "    with mx.autograd.record():\n",
    "        # 1. Compute the embedding of the center words\n",
    "        emb_in = embedding(center)\n",
    "        \n",
    "        # 2. Compute the context embedding\n",
    "        emb_out = embedding_out(word_context_negatives,\n",
    "                                word_context_negatives_mask)\n",
    "\n",
    "        # 3. Compute the prediction\n",
    "        # To predict if a context work is likely or not, the dot product\n",
    "        # between the word vector of the center word and the output weights\n",
    "        # of the context / negative words is computed and passed through a\n",
    "        # Sigmoid function\n",
    "        pred = mx.nd.batch_dot(emb_in, emb_out.swapaxes(1, 2))\n",
    "        pred = pred.squeeze() * word_context_negatives_mask\n",
    "        label = mx.nd.concat(word_context_mask, mx.nd.zeros_like(negatives), dim=1)\n",
    "\n",
    "        # 4. Compute the Loss function (SigmoidBinaryCrossEntropyLoss)\n",
    "        loss = loss_function(pred, label)\n",
    "\n",
    "    # Compute the gradient\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the parameters\n",
    "    trainer.step(batch_size=1)\n",
    "    \n",
    "    # Logging\n",
    "    log_wc += loss.shape[0]\n",
    "    log_avg_loss += loss.mean()\n",
    "    if (i + 1) % log_interval == 0:\n",
    "        wps = log_wc / (time.time() - log_start_time)\n",
    "        # Forces waiting for computation by computing loss value\n",
    "        log_avg_loss = log_avg_loss.asscalar() / log_interval\n",
    "        print('[Batch {}/{}] loss={:.4f}, '\n",
    "                     'throughput={:.2f}K wps, wc={:.2f}K'.format(\n",
    "                         i + 1, num_batches, log_avg_loss,\n",
    "                         wps / 1000, log_wc / 1000))\n",
    "        log_start_time = time.time()\n",
    "        log_avg_loss = 0\n",
    "        log_wc = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of trained embedding\n",
    "\n",
    "As we have only obtained word vectors for words that occured in the training corpus,\n",
    "we filter the evaluation dataset and exclude out of vocabulary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "words1, words2, scores = zip(*([vocab[d[0]], vocab[d[1]], d[2]]\n",
    "    for d in wordsim353  if d[0] in vocab and d[1] in vocab))\n",
    "words1 = mx.nd.array(words1, ctx=context)\n",
    "words2 = mx.nd.array(words2, ctx=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new `TokenEmbedding` object and set the embedding vectors for the words we care about for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embedding = nlp.embedding.TokenEmbedding(unknown_token=None)\n",
    "token_embedding[vocab.idx_to_token] = embedding[vocab.idx_to_token]\n",
    "\n",
    "evaluator = nlp.embedding.evaluation.WordEmbeddingSimilarity(\n",
    "    idx_to_vec=token_embedding.idx_to_vec,\n",
    "    similarity_function=\"CosineSimilarity\")\n",
    "evaluator.initialize(ctx=context)\n",
    "evaluator.hybridize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman rank correlation on 432 pairs of WordSim353 (total 455): 0.546\n"
     ]
    }
   ],
   "source": [
    "pred_similarity = evaluator(words1, words2)\n",
    "sr = stats.spearmanr(pred_similarity.asnumpy(), np.array(scores))\n",
    "print('Spearman rank correlation on {} pairs of {} (total {}): {}'.format(\n",
    "    len(words1), wordsim353.__class__.__name__, len(wordsim353), sr.correlation.round(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unknown token handling and subword information\n",
    "\n",
    "Sometimes we may run into a word for which the embedding does not include a word vector. While the `vocab` object is happy to replace it with a special index for unknown tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is \"hello\" known?  True\n",
      "Is \"likelyunknown\" known?  False\n"
     ]
    }
   ],
   "source": [
    "print('Is \"hello\" known? ', 'hello' in vocab)\n",
    "print('Is \"likelyunknown\" known? ', 'likelyunknown' in vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some embedding models such as the FastText model support computing word vectors for unknown words by taking into account their subword units.\n",
    "\n",
    "\n",
    "\n",
    "- Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. ICLR Workshop , 2013."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training word embeddings with subword information\n",
    "\n",
    "`gluonnlp` provides the concept of a SubwordFunction which maps words to a list of indices representing their subword.\n",
    "Possible SubwordFunctions include mapping a word to the sequence of it's characters/bytes or hashes of all its ngrams.\n",
    "\n",
    "FastText models use a hash function to map each ngram of a word to a number in range `[0, num_subwords)`. We include the same hash function.\n",
    "\n",
    "### Concept of a SubwordFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<the>\t\n",
      "[151151. 361980. 316280. 409726.  60934. 148960.]\n",
      "<NDArray 6 @cpu(0)>\n",
      "<of>\t\n",
      "[497102. 228930. 164528.]\n",
      "<NDArray 3 @cpu(0)>\n",
      "<and>\t\n",
      "[378080. 395046. 125443. 235020. 119624.  30390.]\n",
      "<NDArray 6 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "subword_function = nlp.vocab.create_subword_function(\n",
    "    'NGramHashes', ngrams=[3, 4, 5, 6], num_subwords=500000)\n",
    "\n",
    "idx_to_subwordidxs = subword_function(vocab.idx_to_token)\n",
    "for word, subwords in zip(vocab.idx_to_token[:3], idx_to_subwordidxs[:3]):\n",
    "    print('<'+word+'>', subwords, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As words are of varying length, we have to pad the lists of subwords to obtain a batch. To distinguish padded values from valid subword indices we use a mask.\n",
    "We first pad the subword arrays with `-1`, compute the mask and change the `-1` entries to some valid subword index (here `0`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[151151. 361980. 316280. 409726.  60934. 148960.]\n",
      " [497102. 228930. 164528.      0.      0.      0.]\n",
      " [378080. 395046. 125443. 235020. 119624.  30390.]]\n",
      "<NDArray 3x6 @cpu_shared(0)>\n",
      "\n",
      "[[1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1.]]\n",
      "<NDArray 3x6 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "subword_padding = nlp.data.batchify.Pad(pad_val=-1)\n",
    "\n",
    "subwords = subword_padding(idx_to_subwordidxs[:3])\n",
    "subwords_mask = subwords != -1\n",
    "subwords += subwords == -1  # -1 is invalid. Change to 0\n",
    "print(subwords)\n",
    "print(subwords_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable fast training, we precompute the mapping from the  words in  our training corpus to  the  subword indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute a idx to subwordidxs mapping to support fast lookup\n",
    "idx_to_subwordidxs = list(subword_function(vocab.idx_to_token))\n",
    "max_subwordidxs_len = max(len(s) for s in idx_to_subwordidxs)\n",
    "\n",
    "# Padded max_subwordidxs_len + 1 so each row contains at least one -1\n",
    "# element which can be found by np.argmax below.\n",
    "idx_to_subwordidxs = np.stack(\n",
    "    np.pad(b.asnumpy(), (0, max_subwordidxs_len - len(b) + 1), \\\n",
    "           constant_values=-1, mode='constant')\n",
    "    for b in idx_to_subwordidxs).astype(np.float32)\n",
    "idx_to_subwordidxs = mx.nd.array(idx_to_subwordidxs)\n",
    "\n",
    "def indices_to_subwordindices_mask(indices, idx_to_subwordidxs):\n",
    "    \"\"\"Return array of subwordindices for indices.\n",
    "\n",
    "    A padded numpy array and a mask is returned. The mask is used as\n",
    "    indices map to varying length subwords.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    indices : list of int, numpy array or mxnet NDArray\n",
    "        Token indices that should be mapped to subword indices.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Array of subword indices.\n",
    "\n",
    "    \"\"\"\n",
    "    if not isinstance(indices, mx.nd.NDArray):\n",
    "        indices = mx.nd.array(indices)\n",
    "    subwords = idx_to_subwordidxs[indices]\n",
    "    mask = mx.nd.zeros_like(subwords)\n",
    "    mask += subwords != -1\n",
    "    lengths = mx.nd.argmax(subwords == -1, axis=1)\n",
    "    subwords += subwords == -1\n",
    "\n",
    "    new_length = int(max(mx.nd.max(lengths).asscalar(), 1))\n",
    "    subwords = subwords[:, :new_length]\n",
    "    mask = mask[:, :new_length]\n",
    "\n",
    "    return subwords, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model\n",
    "\n",
    "Instead of the `SimpleEmbeddingModel` we now train a `FasttextEmbeddingModel` Block which can combine the word and subword information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "emsize = 300\n",
    "embedding = nlp.model.train.FasttextEmbeddingModel(\n",
    "    token_to_idx=vocab.token_to_idx,\n",
    "    subword_function=subword_function,\n",
    "    embedding_size=emsize,\n",
    "    weight_initializer=mx.init.Uniform(scale=1 / emsize))\n",
    "embedding_out = nlp.model.train.SimpleEmbeddingModel(\n",
    "    token_to_idx=vocab.token_to_idx,\n",
    "    embedding_size=emsize,\n",
    "    weight_initializer=mx.init.Uniform(scale=1 / emsize))\n",
    "loss_function = mx.gluon.loss.SigmoidBinaryCrossEntropyLoss()\n",
    "\n",
    "embedding.initialize(ctx=context)\n",
    "embedding_out.initialize(ctx=context)\n",
    "embedding.hybridize(static_alloc=True)\n",
    "embedding_out.hybridize(static_alloc=True)\n",
    "\n",
    "params = list(embedding.collect_params().values()) + \\\n",
    "    list(embedding_out.collect_params().values())\n",
    "trainer = mx.gluon.Trainer(params, 'adagrad', dict(learning_rate=0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Compared to training the `SimpleEmbeddingModel`, we now also look up the subwords of each center word in the batch and pass the subword infor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 500/4118] loss=0.3667, throughput=89.84K wps, wc=1024.00K\n",
      "[Batch 1000/4118] loss=0.3515, throughput=91.00K wps, wc=1024.00K\n",
      "[Batch 1500/4118] loss=0.3461, throughput=113.12K wps, wc=1024.00K\n",
      "[Batch 2000/4118] loss=0.3439, throughput=115.73K wps, wc=1024.00K\n",
      "[Batch 2500/4118] loss=0.3413, throughput=121.04K wps, wc=1024.00K\n",
      "[Batch 3000/4118] loss=0.3394, throughput=120.04K wps, wc=1024.00K\n",
      "[Batch 3500/4118] loss=0.3383, throughput=120.54K wps, wc=1024.00K\n",
      "[Batch 4000/4118] loss=0.3359, throughput=127.55K wps, wc=1024.00K\n"
     ]
    }
   ],
   "source": [
    "num_batches = len(context_sampler)\n",
    "num_negatives = 5\n",
    "\n",
    "# Logging variables\n",
    "log_interval = 500\n",
    "log_wc = 0\n",
    "log_start_time = time.time()\n",
    "log_avg_loss = 0\n",
    "\n",
    "# We iterate over all batches in the context_sampler\n",
    "for i, batch in enumerate(context_sampler):\n",
    "    (center, word_context, word_context_mask) = batch\n",
    "    \n",
    "    negatives_shape = (word_context.shape[0],\n",
    "                       word_context.shape[1] * num_negatives)\n",
    "    negatives, negatives_mask = negatives_sampler(\n",
    "        negatives_shape, word_context, word_context_mask)\n",
    "\n",
    "\n",
    "    # Get subwords for all unique words in the batch\n",
    "    unique, inverse_unique_indices = np.unique(\n",
    "        center.asnumpy(), return_inverse=True)\n",
    "    unique = mx.nd.array(unique)\n",
    "    inverse_unique_indices = mx.nd.array(\n",
    "        inverse_unique_indices, ctx=context)\n",
    "    subwords, subwords_mask = indices_to_subwordindices_mask(unique, idx_to_subwordidxs)\n",
    "\n",
    "    # To GPU\n",
    "    center = center.as_in_context(context)\n",
    "    subwords = subwords.as_in_context(context)\n",
    "    subwords_mask = subwords_mask.as_in_context(context)\n",
    "    word_context_negatives = mx.nd.concat(word_context, negatives, dim=1).as_in_context(context)\n",
    "    word_context_negatives_mask = mx.nd.concat(word_context_mask, negatives_mask, dim=1).as_in_context(context)\n",
    "    word_context_mask = word_context_mask.as_in_context(context)\n",
    "\n",
    "    with mx.autograd.record():\n",
    "        emb_in = embedding(center, subwords, subwordsmask=subwords_mask, \n",
    "                           words_to_unique_subwords_indices=inverse_unique_indices)\n",
    "        emb_out = embedding_out(word_context_negatives, word_context_negatives_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        pred = mx.nd.batch_dot(emb_in, emb_out.swapaxes(1, 2))\n",
    "        pred = pred.squeeze() * word_context_negatives_mask\n",
    "        label = mx.nd.concat(word_context_mask, mx.nd.zeros(negatives.shape, ctx=context), dim=1)\n",
    "\n",
    "        loss = loss_function(pred, label)\n",
    "\n",
    "    loss.backward()\n",
    "    trainer.step(batch_size=1)\n",
    "    \n",
    "    # Logging\n",
    "    log_wc += loss.shape[0]\n",
    "    log_avg_loss += loss.mean()\n",
    "    if (i + 1) % log_interval == 0:\n",
    "        wps = log_wc / (time.time() - log_start_time)\n",
    "        # Forces waiting for computation by computing loss value\n",
    "        log_avg_loss = log_avg_loss.asscalar() / log_interval\n",
    "        print('[Batch {}/{}] loss={:.4f}, '\n",
    "                     'throughput={:.2f}K wps, wc={:.2f}K'.format(\n",
    "                         i + 1, num_batches, log_avg_loss,\n",
    "                         wps / 1000, log_wc / 1000))\n",
    "        log_start_time = time.time()\n",
    "        log_avg_loss = 0\n",
    "        log_wc = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Thanks to the subword support of the `FasttextEmbeddingModel` we can now evaluate on all words in the evaluation dataset, not only the ones that we observed during training (the `SimpleEmbeddingModel` only provides vectors for words observed at training).\n",
    "\n",
    "We first find the all tokens in the evaluation dataset and then convert the `FasttextEmbeddingModel` to a `TokenEmbedding` with exactly those tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 437 unique tokens in WordSim353\n",
      "The imputed TokenEmbedding has shape (437, 300)\n"
     ]
    }
   ],
   "source": [
    "wordsim353_tokens  = list(set(itertools.chain.from_iterable((d[0], d[1]) for d in wordsim353)))\n",
    "token_embedding = nlp.embedding.TokenEmbedding(unknown_token=None)\n",
    "token_embedding[wordsim353_tokens] = embedding[wordsim353_tokens]\n",
    "\n",
    "print('There are', len(wordsim353_tokens), 'unique tokens in WordSim353')\n",
    "print('The imputed TokenEmbedding has shape', token_embedding.idx_to_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = nlp.embedding.evaluation.WordEmbeddingSimilarity(\n",
    "    idx_to_vec=token_embedding.idx_to_vec,\n",
    "    similarity_function=\"CosineSimilarity\")\n",
    "evaluator.initialize(ctx=context)\n",
    "evaluator.hybridize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "words1, words2, scores = zip(*([token_embedding.token_to_idx[d[0]],\n",
    "                                token_embedding.token_to_idx[d[1]],\n",
    "                                d[2]] for d in wordsim353))\n",
    "words1 = mx.nd.array(words1, ctx=context)\n",
    "words2 = mx.nd.array(words2, ctx=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman rank correlation on 455 pairs of WordSim353: 0.472\n"
     ]
    }
   ],
   "source": [
    "pred_similarity = evaluator(words1, words2)\n",
    "sr = stats.spearmanr(pred_similarity.asnumpy(), np.array(scores))\n",
    "print('Spearman rank correlation on {} pairs of {}: {}'.format(\n",
    "    len(words1), wordsim353.__class__.__name__, sr.correlation.round(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pretrained FastText models with subword information\n",
    "\n",
    "As the `FasttextEmbeddingModel` in `gluonnlp` uses the same structure as the models provided by `facebookresearch/fasttext` it is possible to load models trained by `facebookresearch/fasttext` into the `FasttextEmbeddingModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman rank correlation on 455 pairs of WordSim353: 0.584\n"
     ]
    }
   ],
   "source": [
    "embedding = nlp.model.train.FasttextEmbeddingModel.load_fasttext_format('/home/ubuntu/skipgram-text8.bin')\n",
    "token_embedding = nlp.embedding.TokenEmbedding(unknown_token=None)\n",
    "token_embedding[wordsim353_tokens] = embedding[wordsim353_tokens]\n",
    "\n",
    "evaluator = nlp.embedding.evaluation.WordEmbeddingSimilarity(\n",
    "    idx_to_vec=token_embedding.idx_to_vec,\n",
    "    similarity_function=\"CosineSimilarity\")\n",
    "evaluator.initialize(ctx=context)\n",
    "evaluator.hybridize()\n",
    "\n",
    "pred_similarity = evaluator(words1, words2)\n",
    "sr = stats.spearmanr(pred_similarity.asnumpy(), np.array(scores))\n",
    "print('Spearman rank correlation on {} pairs of {}: {}'.format(\n",
    "    len(words1), wordsim353.__class__.__name__, sr.correlation.round(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_python3)",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
