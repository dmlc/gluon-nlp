{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings Training and Evaluation\n",
    "\n",
    "## Evaluating Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous example has introduced how to load pre-trained word embeddings\n",
    "from a set of sources included in the GluonNLP toolkit. It was shown how make\n",
    "use of the word vectors to find the top most similar words of a given words or\n",
    "to solve the analogy task.\n",
    "\n",
    "Besides manually investigating similar words or the predicted analogous words,\n",
    "we can facilitate word embedding evaluation datasets to quantify the\n",
    "evaluation.\n",
    "\n",
    "Datasets for the *similarity* task come with a list of word pairs together with\n",
    "a human similarity judgement. The task is to recover the order of most-similar\n",
    "to least-similar pairs.\n",
    "\n",
    "Datasets for the *analogy* tasks supply a set of analogy quadruples of the form\n",
    "‘a : b :: c : d’ and the task is to recover find the correct ‘d’ in as many\n",
    "cases as possible given just ‘a’, ‘b’, ‘c’. For instance, “man : woman :: son :\n",
    "daughter” is an analogy.\n",
    "\n",
    "The GluonNLP toolkit includes a set of popular *similarity* and *analogy* task\n",
    "datasets as well as helpers for computing the evaluation scores. Here we show\n",
    "how to make use of them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T20:27:24.991031Z",
     "start_time": "2018-03-28T20:27:23.669296Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import itertools\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "import random\n",
    "\n",
    "import mxnet as mx\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# context = mx.cpu()  # Enable this to run on CPU\n",
    "context = mx.gpu(0)  # Enable this to run on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load pre-trained FastText word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nlp.embedding.create('fasttext', source='crawl-300d-2M')\n",
    "\n",
    "vocab = nlp.Vocab(nlp.data.Counter(embedding.idx_to_token))\n",
    "vocab.set_embedding(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T20:27:25.000472Z",
     "start_time": "2018-03-28T20:27:24.993155Z"
    }
   },
   "source": [
    "### Word Similarity and Relatedness Task\n",
    "\n",
    "Word embeddings should capture the relationsship between words in natural language.\n",
    "In the Word Similarity and Relatedness Task word embeddings are evaluated by comparing word similarity scores computed from a pair of words with human labels for the similarity or relatedness of the pair.\n",
    "\n",
    "`gluonnlp` includes a number of common datasets for the Word Similarity and Relatedness Task. The included datasets are listed in the [API documentation](http://gluon-nlp.mxnet.io/api/data.html#word-embedding-evaluation-datasets). We use several of them in the evaluation example below.\n",
    "\n",
    "We first show a few samples from the WordSim353 dataset, to get an overall feeling of the Dataset structur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "announcement, warning, 6.0\n",
      "population, development, 3.75\n",
      "king, cabbage, 0.23\n",
      "century, nation, 3.16\n",
      "luxury, car, 6.47\n",
      "disability, death, 5.47\n",
      "mile, kilometer, 8.66\n",
      "listing, category, 6.38\n",
      "money, deposit, 7.73\n",
      "fertility, egg, 6.69\n",
      "planet, astronomer, 7.94\n",
      "psychology, anxiety, 7.0\n",
      "media, radio, 7.42\n",
      "closet, clothes, 8.0\n",
      "dollar, buck, 9.22\n"
     ]
    }
   ],
   "source": [
    "wordsim353 = nlp.data.WordSim353()\n",
    "for i in range(15):\n",
    "    print(*wordsim353[i], sep=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity evaluator\n",
    "\n",
    "The GluonNLP toolkit includes a `WordEmbeddingSimilarity`  block, which predicts similarity score between word pairs given an embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = nlp.embedding.evaluation.WordEmbeddingSimilarity(\n",
    "    idx_to_vec=vocab.embedding.idx_to_vec,\n",
    "    similarity_function=\"CosineSimilarity\")\n",
    "evaluator.initialize(ctx=context)\n",
    "evaluator.hybridize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation: Running the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words1, words2, scores = zip(*([vocab[d[0]], vocab[d[1]], d[2]] for d in wordsim353))\n",
    "words1 = mx.nd.array(words1, ctx=context)\n",
    "words2 = mx.nd.array(words2, ctx=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarities can be predicted by passing the two arrays of words through the evaluator. Thereby the *ith* word in `words1` will be compared with the *ith* word in `words2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ 0.30671987  0.19214539  0.18280022  0.29696473  0.2911745 ]\n",
      "<NDArray 5 @gpu(0)>\n"
     ]
    }
   ],
   "source": [
    "pred_similarity = evaluator(words1, words2)\n",
    "print(pred_similarity[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the predicted similarities, and thereby the word embeddings, by computing the Spearman Rank Correlation between the predicted similarities and the groundtruth, human, similarity scores from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman rank correlation on WordSim353: 0.79\n"
     ]
    }
   ],
   "source": [
    "sr = stats.spearmanr(pred_similarity.asnumpy(), np.array(scores))\n",
    "print('Spearman rank correlation on {}: {}'.format(wordsim353.__class__.__name__,\n",
    "                                                   sr.correlation.round(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T20:27:25.000472Z",
     "start_time": "2018-03-28T20:27:24.993155Z"
    }
   },
   "source": [
    "### Word Analogy Task\n",
    "\n",
    "In the Word Analogy Task word embeddings are evaluated by inferring an analogous word `D`, which is related to a given word `C` in the same way as a given pair of words `A, B` are related.\n",
    "\n",
    "`gluonnlp` includes a number of common datasets for the Word Analogy Task. The included datasets are listed in the [API documentation](http://gluon-nlp.mxnet.io/api/data.html#word-embedding-evaluation-datasets). In this notebook we use the GoogleAnalogyTestSet dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_analogy = nlp.data.GoogleAnalogyTestSet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first demonstrate the structure of the dataset by printing a few examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing every 1000st analogy question from the 19544 questionsin the Google Analogy Test Set:\n",
      "\n",
      "athens greece baghdad iraq\n",
      "baku azerbaijan dushanbe tajikistan\n",
      "dublin ireland kathmandu nepal\n",
      "lusaka zambia tehran iran\n",
      "rome italy windhoek namibia\n",
      "zagreb croatia astana kazakhstan\n",
      "philadelphia pennsylvania tampa florida\n",
      "wichita kansas shreveport louisiana\n",
      "shreveport louisiana oxnard california\n",
      "complete completely lucky luckily\n",
      "comfortable uncomfortable clear unclear\n",
      "good better high higher\n",
      "young younger tight tighter\n",
      "weak weakest bright brightest\n",
      "slow slowing describe describing\n",
      "ireland irish greece greek\n",
      "feeding fed sitting sat\n",
      "slowing slowed decreasing decreased\n",
      "finger fingers onion onions\n",
      "play plays sing sings\n"
     ]
    }
   ],
   "source": [
    "sample = []\n",
    "print(('Printing every 1000st analogy question '\n",
    "       'from the {} questions'\n",
    "        'in the Google Analogy Test Set:').format(len(google_analogy)))\n",
    "print('')\n",
    "for i in range(0, 19544, 1000):\n",
    "    print(*google_analogy[i])\n",
    "    sample.append(google_analogy[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We restrict ourselves here to the first (most frequent) 300000 words of the pre-trained embedding as well as all tokens that occur in the evaluation datasets as possible answers to the analogy questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using most frequent 300000 + 96 extra words\n",
      "Dropped 1781 pairs from 19544 as they were OOV.\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "most_freq = 300000\n",
    "counter = nlp.data.utils.Counter(embedding.idx_to_token[:most_freq])\n",
    "google_analogy_tokens = set(itertools.chain.from_iterable((d[0], d[1], d[2], d[3]) for d in google_analogy))\n",
    "counter.update(t for t in google_analogy_tokens if t in embedding)\n",
    "\n",
    "vocab = nlp.vocab.Vocab(counter)\n",
    "vocab.set_embedding(embedding)\n",
    "\n",
    "print(\"Using most frequent {} + {} extra words\".format(most_freq, len(vocab) - most_freq))\n",
    "\n",
    "\n",
    "google_analogy_subset = [\n",
    "    d for i, d in enumerate(google_analogy) if\n",
    "    d[0] in vocab and d[1] in vocab and d[2] in vocab and d[3] in vocab\n",
    "]\n",
    "print('Dropped {} pairs from {} as they were OOV.'.format(\n",
    "    len(google_analogy) - len(google_analogy_subset),\n",
    "    len(google_analogy)))\n",
    "\n",
    "google_analogy_coded = [[vocab[d[0]], vocab[d[1]], vocab[d[2]], vocab[d[3]]]\n",
    "                 for d in google_analogy_subset]\n",
    "google_analogy_coded_batched = mx.gluon.data.DataLoader(\n",
    "    google_analogy_coded, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = nlp.embedding.evaluation.WordEmbeddingAnalogy(\n",
    "    idx_to_vec=vocab.embedding.idx_to_vec,\n",
    "    exclude_question_words=True,\n",
    "    analogy_function=\"ThreeCosMul\")\n",
    "evaluator.initialize(ctx=context)\n",
    "evaluator.hybridize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on GoogleAnalogyTestSet: 0.772\n"
     ]
    }
   ],
   "source": [
    "acc = mx.metric.Accuracy()\n",
    "\n",
    "for i, batch in enumerate(google_analogy_coded_batched):\n",
    "    batch = batch.as_in_context(context)\n",
    "    words1, words2, words3, words4 = (batch[:, 0], batch[:, 1],\n",
    "                                      batch[:, 2], batch[:, 3])\n",
    "    pred_idxs = evaluator(words1, words2, words3)\n",
    "    acc.update(pred_idxs[:, 0], words4.astype(np.float32))\n",
    "    \n",
    "print('Accuracy on %s: %s'% (google_analogy.__class__.__name__, acc.get()[1].round(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training word embeddings\n",
    "\n",
    "Next to making it easy to work with pre-trained word embeddings, `gluonnlp`\n",
    "also provides everything needed to train your own embeddings. Datasets as well\n",
    "as model definitions are included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the training data\n",
    "\n",
    "We first load the Text8 corpus from the [Large Text Compression\n",
    "Benchmark](http://mattmahoney.net/dc/textdata.html) which includes the first\n",
    "100 MB of cleaned text from the English Wikipedia. We follow the common practice\n",
    "of splitting every 10'000 tokens to obtain \"sentences\" for embedding training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sentences: 1701\n",
      "# tokens: 10000 ['anarchism', 'originated', 'as', 'a', 'term']\n",
      "# tokens: 10000 ['reciprocity', 'qualitative', 'impairments', 'in', 'communication']\n",
      "# tokens: 10000 ['with', 'the', 'aegis', 'of', 'zeus']\n"
     ]
    }
   ],
   "source": [
    "dataset = nlp.data.Text8(segment='train')\n",
    "print('# sentences:', len(dataset))\n",
    "for sentence in dataset[:3]:\n",
    "    print('# tokens:', len(sentence), sentence[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then build a vocabulary of all the tokens in the dataset that occur more\n",
    "than 5 times and replace the words with their indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = nlp.data.count_tokens(itertools.chain.from_iterable(dataset))\n",
    "vocab = nlp.Vocab(\n",
    "    counter,\n",
    "    unknown_token=None,\n",
    "    padding_token=None,\n",
    "    bos_token=None,\n",
    "    eos_token=None,\n",
    "    min_freq=5)\n",
    "\n",
    "def code(s):\n",
    "    return [vocab[t] for t in s if t in vocab]\n",
    "\n",
    "coded_dataset = dataset.transform(code, lazy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some words such as \"the\", \"a\", and \"in\" are very frequent.\n",
    "One important trick applied when training word2vec is to subsample the dataset\n",
    "according to the token frequencies. [1] proposes to discard individual\n",
    "occurences of words from the dataset with probability\n",
    "\n",
    "$$P(w_i) = 1 - \\sqrt{\\frac{t}{f(w_i)}}$$\n",
    "\n",
    "where $f(w_i)$ is the frequency with which a word is\n",
    "observed in a dataset and $t$ is a subsampling constant typically chosen around\n",
    "$10^{-5}$.\n",
    "\n",
    "[1] Mikolov, Tomas, et al. “Distributed representations of words and phrases and their compositionality.” Advances in neural information processing systems. 2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# tokens for sentences in coded_dataset:\n",
      "9895 [5233, 3083, 11, 5, 194]\n",
      "9858 [18214, 17356, 36672, 4, 1753]\n",
      "9926 [23, 0, 19754, 1, 4829]\n",
      "\n",
      "# tokens for sentences in subsampled_dataset:\n",
      "2958 [3083, 127, 741, 10619, 27497]\n",
      "2816 [18214, 17356, 36672, 13001, 3]\n",
      "2771 [19754, 1799, 8712, 16334, 6690]\n"
     ]
    }
   ],
   "source": [
    "subsampling_constant = 1e-5\n",
    "\n",
    "idx_to_count = [counter[w] for w in vocab.idx_to_token]\n",
    "total_count = sum(idx_to_count)\n",
    "idx_to_pdiscard = [\n",
    "    1 - math.sqrt(subsampling_constant / (count / total_count))\n",
    "    for count in idx_to_count\n",
    "]\n",
    "\n",
    "\n",
    "def subsample(s):\n",
    "    return [\n",
    "        t for t, r in zip(s, np.random.uniform(0, 1, size=len(s)))\n",
    "        if r > idx_to_pdiscard[t]\n",
    "    ]\n",
    "\n",
    "\n",
    "subsampled_dataset = coded_dataset.transform(subsample, lazy=False)\n",
    "\n",
    "print('# tokens for sentences in coded_dataset:')\n",
    "for i in range(3):\n",
    "    print(len(coded_dataset[i]), coded_dataset[i][:5])\n",
    "\n",
    "print('\\n# tokens for sentences in subsampled_dataset:')\n",
    "for i in range(3):\n",
    "    print(len(subsampled_dataset[i]), subsampled_dataset[i][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition\n",
    "\n",
    "`gluonnlp` provides model definitions for popular embedding models as Gluon Blocks.\n",
    "Here we show how to train them with the Skip-Gram objective, a\n",
    "simple and popular embedding training objective. It was introduced\n",
    "by \"Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient\n",
    "estimation of word representations in vector space. ICLR Workshop , 2013.\"\n",
    "\n",
    "The Skip-Gram objective trains word vectors such that the word vector of a word\n",
    "at some position in a sentence can best predict the surrounding words. We call\n",
    "these words *center* and *context* words.\n",
    "\n",
    "<img src=\"http://blog.aylien.com/wp-content/uploads/2016/10/skip-gram.png\" width=\"300\">\n",
    "\n",
    "Skip-Gram and picture from \"Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey\n",
    "Dean. Efficient estimation of word representations in vector space. ICLR\n",
    "Workshop , 2013.\"\n",
    "\n",
    "\n",
    "For the Skip-Gram objective, we initialize two embedding models: `embedding`\n",
    "and `embedding_out`. `embedding` is used to look up embeddings for the *center*\n",
    "words. `embedding_out` is used for the *context* words.\n",
    "\n",
    "The weights of `embedding` are the final word embedding weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "emsize = 300\n",
    "\n",
    "embedding = nlp.model.train.SimpleEmbeddingModel(\n",
    "    token_to_idx=vocab.token_to_idx,\n",
    "    embedding_size=emsize,\n",
    "    weight_initializer=mx.init.Uniform(scale=1 / emsize))\n",
    "embedding_out = nlp.model.train.SimpleEmbeddingModel(\n",
    "    token_to_idx=vocab.token_to_idx,\n",
    "    embedding_size=emsize,\n",
    "    weight_initializer=mx.init.Uniform(scale=1 / emsize))\n",
    "\n",
    "embedding.initialize(ctx=context)\n",
    "embedding_out.initialize(ctx=context)\n",
    "embedding.hybridize(static_alloc=True)\n",
    "embedding_out.hybridize(static_alloc=True)\n",
    "\n",
    "params = list(embedding.collect_params().values()) + \\\n",
    "    list(embedding_out.collect_params().values())\n",
    "trainer = mx.gluon.Trainer(params, 'adagrad', dict(learning_rate=0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start training, let's examine the quality of our randomly initialized embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "closest tokens to \"data\": icarius, pedagogy, weasels, transgression, flavonoids, moguls, unvisited, basse, nonspecific, scrolling\n"
     ]
    }
   ],
   "source": [
    "def norm_vecs_by_row(x):\n",
    "    return x / (mx.nd.sum(x * x, axis=1) + 1e-10).sqrt().reshape((-1, 1))\n",
    "\n",
    "\n",
    "def get_k_closest_tokens(vocab, embedding, k, word):\n",
    "    word_vec = embedding(mx.nd.array([vocab.token_to_idx[word]],\n",
    "                                     ctx=context)).reshape((-1, 1))\n",
    "    vocab_vecs = norm_vecs_by_row(embedding.embedding.weight.data())\n",
    "    dot_prod = mx.nd.dot(vocab_vecs, word_vec)\n",
    "    indices = mx.nd.topk(\n",
    "        dot_prod.reshape((len(vocab.idx_to_token), )),\n",
    "        k=k + 1,\n",
    "        ret_typ='indices')\n",
    "    indices = [int(i.asscalar()) for i in indices]\n",
    "    result = [vocab.idx_to_token[i] for i in indices[1:]]\n",
    "    print('closest tokens to \"%s\": %s' % (word, \", \".join(result)))\n",
    "\n",
    "\n",
    "example_token = \"data\"\n",
    "get_k_closest_tokens(vocab, embedding, 10, example_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training objective\n",
    "\n",
    "#### Naive objective\n",
    "\n",
    "To naively maximize the Skip-Gram objective, if we sample a center word we need to compute a prediction for every other word in the vocabulary if it occurs in the context of the center word or not. We can then backpropagate and update the parameters to make the prediction of the correct *context* words more likely and of all other words less likely.\n",
    "\n",
    "\n",
    "However, this naive method is computationally very expensive as it requires computing a Softmax function over all words in the vocabulary. Instead, \"Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. ICLR Workshop , 2013.\" introduced *Negative Sampling*.\n",
    "\n",
    "#### Negative sampling\n",
    "\n",
    "*Negative Sampling* means that instead of using a small number of *correct* (or *positive*) *context* and all other (*negative*) words to compute the loss and update the parameters we may choose a small, constant number of *negative* words at random. Negative words are choosen randomly based on their frequency in the training corpus. It is recommend to smoothen the frequency distribution by the factor `0.75`.\n",
    "\n",
    "\n",
    "We can use the `UnigramCandidateSampler` to sample tokens by some unigram weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_negatives = 5\n",
    "weights = mx.nd.array(idx_to_count)**0.75\n",
    "negatives_sampler = nlp.data.UnigramCandidateSampler(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Center and context words\n",
    "\n",
    "We can use `EmbeddingCenterContextBatchify` to transform a corpus into batches of center and context words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2048\n",
    "window_size = 5\n",
    "batchify = nlp.data.batchify.EmbeddingCenterContextBatchify(batch_size=batch_size, window_size=window_size)\n",
    "batches = batchify(subsampled_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the loss with negative sampling we use `SigmoidBinaryCrossEntropyLoss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = mx.gluon.loss.SigmoidBinaryCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accidental_hits(candidates, true_samples):\n",
    "    \"\"\"Compute a candidates_mask surpressing accidental hits.\n",
    "\n",
    "    Accidental hits are candidates that occur in the same batch dimension of\n",
    "    true_samples.\n",
    "\n",
    "    \"\"\"\n",
    "    candidates_np = candidates.asnumpy()\n",
    "    true_samples_np = true_samples.asnumpy()\n",
    "\n",
    "    candidates_mask = np.ones(candidates.shape, dtype=np.bool_)\n",
    "    for j in range(true_samples.shape[1]):\n",
    "        candidates_mask &= ~(candidates_np == true_samples_np[:, j:j + 1])\n",
    "\n",
    "    return candidates, mx.nd.array(candidates_mask, ctx=candidates.context)\n",
    "\n",
    "\n",
    "def skipgram_batch(data):\n",
    "    \"\"\"Create a batch for Skipgram training objective.\"\"\"\n",
    "    centers, word_context, word_context_mask = data\n",
    "    assert len(centers.shape) == 2\n",
    "    negatives_shape = (len(word_context), 2 * window_size * num_negatives)\n",
    "    negatives, negatives_mask = remove_accidental_hits(\n",
    "        negatives_sampler(negatives_shape), word_context)\n",
    "    context_negatives = mx.nd.concat(word_context, negatives, dim=1)\n",
    "    masks = mx.nd.concat(word_context_mask, negatives_mask, dim=1)\n",
    "    labels = mx.nd.concat(word_context_mask, mx.nd.zeros_like(negatives), dim=1)\n",
    "    return (centers.as_in_context(context),\n",
    "            context_negatives.as_in_context(context),\n",
    "            masks.as_in_context(context),\n",
    "            labels.as_in_context(context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_embedding(num_epochs):\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        start_time = time.time()\n",
    "        train_l_sum = 0\n",
    "        num_samples = 0\n",
    "        for i, data in enumerate(batches):\n",
    "            (center, context_and_negative, mask,\n",
    "             label) = skipgram_batch(data)\n",
    "            with mx.autograd.record():\n",
    "                emb_in = embedding(center)\n",
    "                emb_out = embedding_out(context_and_negative)\n",
    "                pred = mx.nd.batch_dot(emb_in, emb_out.swapaxes(1, 2))\n",
    "                l = (loss(pred.reshape(label.shape), label, mask) *\n",
    "                     mask.shape[1] / mask.sum(axis=1))\n",
    "            l.backward()\n",
    "            trainer.step(1)\n",
    "            train_l_sum += l.sum()\n",
    "            num_samples += center.shape[0]\n",
    "            if i % 500 == 0:\n",
    "                mx.nd.waitall()\n",
    "                wps = num_samples / (time.time() - start_time)\n",
    "                print('epoch %d, time %.2fs, iteration %d, throughput=%.2fK wps'\n",
    "                      % (epoch, time.time() - start_time, i, wps / 1000))\n",
    "\n",
    "        print('epoch %d, time %.2fs, train loss %.2f'\n",
    "              % (epoch, time.time() - start_time,\n",
    "                 train_l_sum.asscalar() / num_samples))\n",
    "        get_k_closest_tokens(vocab, embedding, 10, example_token)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, time 0.38s, iteration 0, throughput=5.42K wps\n",
      "epoch 1, time 6.49s, iteration 500, throughput=158.19K wps\n",
      "epoch 1, time 12.61s, iteration 1000, throughput=162.62K wps\n",
      "epoch 1, time 18.71s, iteration 1500, throughput=164.30K wps\n",
      "epoch 1, time 24.86s, iteration 2000, throughput=164.86K wps\n",
      "epoch 1, time 27.28s, train loss 0.35\n",
      "closest tokens to \"data\": storage, extensions, architectures, accessing, applications, packages, interoperability, identifier, interfaces, executable\n",
      "\n",
      "epoch 2, time 0.37s, iteration 0, throughput=5.53K wps\n",
      "epoch 2, time 6.52s, iteration 500, throughput=157.36K wps\n",
      "epoch 2, time 12.67s, iteration 1000, throughput=161.75K wps\n",
      "epoch 2, time 18.80s, iteration 1500, throughput=163.53K wps\n",
      "epoch 2, time 24.89s, iteration 2000, throughput=164.64K wps\n",
      "epoch 2, time 27.43s, train loss 0.31\n",
      "closest tokens to \"data\": metadata, protocols, addressable, routing, asynchronous, identifier, dynamically, encoding, packet, optimization\n",
      "\n",
      "epoch 3, time 0.37s, iteration 0, throughput=5.53K wps\n",
      "epoch 3, time 6.47s, iteration 500, throughput=158.53K wps\n",
      "epoch 3, time 12.58s, iteration 1000, throughput=162.95K wps\n",
      "epoch 3, time 18.67s, iteration 1500, throughput=164.64K wps\n",
      "epoch 3, time 24.76s, iteration 2000, throughput=165.48K wps\n",
      "epoch 3, time 27.32s, train loss 0.30\n",
      "closest tokens to \"data\": addressable, terabytes, indexing, metadata, encapsulated, encode, lossless, storage, coding, encoding\n",
      "\n",
      "epoch 4, time 0.37s, iteration 0, throughput=5.55K wps\n",
      "epoch 4, time 6.45s, iteration 500, throughput=158.97K wps\n",
      "epoch 4, time 12.55s, iteration 1000, throughput=163.40K wps\n",
      "epoch 4, time 18.64s, iteration 1500, throughput=164.94K wps\n",
      "epoch 4, time 24.72s, iteration 2000, throughput=165.79K wps\n",
      "epoch 4, time 27.19s, train loss 0.29\n",
      "closest tokens to \"data\": addressable, indexing, terabytes, encapsulated, lossless, storing, storage, lossy, coding, encode\n",
      "\n",
      "epoch 5, time 0.37s, iteration 0, throughput=5.58K wps\n",
      "epoch 5, time 6.46s, iteration 500, throughput=158.89K wps\n",
      "epoch 5, time 12.55s, iteration 1000, throughput=163.38K wps\n",
      "epoch 5, time 18.64s, iteration 1500, throughput=164.92K wps\n",
      "epoch 5, time 24.72s, iteration 2000, throughput=165.79K wps\n",
      "epoch 5, time 27.23s, train loss 0.28\n",
      "closest tokens to \"data\": lossless, addressable, terabytes, lossy, storage, storing, decoder, indexing, encapsulated, coding\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_embedding(num_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of trained embedding\n",
    "\n",
    "As we have only obtained word vectors for words that occured in the training corpus,\n",
    "we filter the evaluation dataset and exclude out of vocabulary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "words1, words2, scores = zip(*([vocab[d[0]], vocab[d[1]], d[2]]\n",
    "    for d in wordsim353  if d[0] in vocab and d[1] in vocab))\n",
    "words1 = mx.nd.array(words1, ctx=context)\n",
    "words2 = mx.nd.array(words2, ctx=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new `TokenEmbedding` object and set the embedding vectors for the words we care about for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embedding = nlp.embedding.TokenEmbedding(unknown_token=None, allow_extend=True)\n",
    "token_embedding[vocab.idx_to_token] = embedding[vocab.idx_to_token]\n",
    "\n",
    "evaluator = nlp.embedding.evaluation.WordEmbeddingSimilarity(\n",
    "    idx_to_vec=token_embedding.idx_to_vec,\n",
    "    similarity_function=\"CosineSimilarity\")\n",
    "evaluator.initialize(ctx=context)\n",
    "evaluator.hybridize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman rank correlation on 332 pairs of WordSim353 (total 352): 0.627\n"
     ]
    }
   ],
   "source": [
    "pred_similarity = evaluator(words1, words2)\n",
    "sr = stats.spearmanr(pred_similarity.asnumpy(), np.array(scores))\n",
    "print('Spearman rank correlation on {} pairs of {} (total {}): {}'.format(\n",
    "    len(words1), wordsim353.__class__.__name__, len(wordsim353), sr.correlation.round(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unknown token handling and subword information\n",
    "\n",
    "Sometimes we may run into a word for which the embedding does not include a word vector. While the `vocab` object is happy to replace it with a special index for unknown tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is \"hello\" known?  True\n",
      "Is \"likelyunknown\" known?  False\n"
     ]
    }
   ],
   "source": [
    "print('Is \"hello\" known? ', 'hello' in vocab)\n",
    "print('Is \"likelyunknown\" known? ', 'likelyunknown' in vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some embedding models such as the FastText model support computing word vectors for unknown words by taking into account their subword units.\n",
    "\n",
    "\n",
    "\n",
    "- Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. ICLR Workshop , 2013."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training word embeddings with subword information\n",
    "\n",
    "`gluonnlp` provides the concept of a SubwordFunction which maps words to a list of indices representing their subword.\n",
    "Possible SubwordFunctions include mapping a word to the sequence of it's characters/bytes or hashes of all its ngrams.\n",
    "\n",
    "FastText models use a hash function to map each ngram of a word to a number in range `[0, num_subwords)`. We include the same hash function.\n",
    "\n",
    "### Concept of a SubwordFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<the>\t[151151, 409726, 148960, 361980, 60934, 316280]\n",
      "<of>\t[497102, 164528, 228930]\n",
      "<and>\t[378080, 235020, 30390, 395046, 119624, 125443]\n"
     ]
    }
   ],
   "source": [
    "subword_function = nlp.vocab.create_subword_function(\n",
    "    'NGramHashes', ngrams=[3, 4, 5, 6], num_subwords=500000)\n",
    "\n",
    "idx_to_subwordidxs = subword_function(vocab.idx_to_token)\n",
    "for word, subwords in zip(vocab.idx_to_token[:3], idx_to_subwordidxs[:3]):\n",
    "    print('<'+word+'>', subwords, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As words are of varying length, we have to pad the lists of subwords to obtain a batch. To distinguish padded values from valid subword indices we use a mask.\n",
    "We first pad the subword arrays with `-1`, compute the mask and change the `-1` entries to some valid subword index (here `0`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 151151.  409726.  148960.  361980.   60934.  316280.]\n",
      " [ 497102.  164528.  228930.       0.       0.       0.]\n",
      " [ 378080.  235020.   30390.  395046.  119624.  125443.]]\n",
      "<NDArray 3x6 @cpu_shared(0)>\n",
      "\n",
      "[[ 1.  1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  1.]]\n",
      "<NDArray 3x6 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "subword_padding = nlp.data.batchify.Pad(pad_val=-1)\n",
    "\n",
    "subwords = subword_padding(idx_to_subwordidxs[:3])\n",
    "subwords_mask = subwords != -1\n",
    "subwords += subwords == -1  # -1 is invalid. Change to 0\n",
    "print(subwords)\n",
    "print(subwords_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model\n",
    "\n",
    "Instead of the `SimpleEmbeddingModel` we now train a `FasttextEmbeddingModel` Block which can combine the word and subword information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "emsize = 300\n",
    "embedding = nlp.model.train.FasttextEmbeddingModel(\n",
    "    token_to_idx=vocab.token_to_idx,\n",
    "    subword_function=subword_function,\n",
    "    embedding_size=emsize,\n",
    "    weight_initializer=mx.init.Uniform(scale=1 / emsize))\n",
    "embedding_out = nlp.model.train.SimpleEmbeddingModel(\n",
    "    token_to_idx=vocab.token_to_idx,\n",
    "    embedding_size=emsize,\n",
    "    weight_initializer=mx.init.Uniform(scale=1 / emsize))\n",
    "loss_function = mx.gluon.loss.SigmoidBinaryCrossEntropyLoss()\n",
    "\n",
    "embedding.initialize(ctx=context)\n",
    "embedding_out.initialize(ctx=context)\n",
    "embedding.hybridize(static_alloc=True)\n",
    "embedding_out.hybridize(static_alloc=True)\n",
    "\n",
    "params = list(embedding.collect_params().values()) + \\\n",
    "    list(embedding_out.collect_params().values())\n",
    "trainer = mx.gluon.Trainer(params, 'adagrad', dict(learning_rate=0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Compared to training the `SimpleEmbeddingModel`, we now also look up the subwords of each center word in the batch and pass the subword infor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonnlp.base import numba_jitclass, numba_types, numba_prange\n",
    "\n",
    "@numba_jitclass([('idx_to_subwordidxs',\n",
    "                  numba_types.List(numba_types.int_[::1]))])\n",
    "class SubwordLookup(object):\n",
    "    \"\"\"Just-in-time compiled helper class for fast, padded subword lookup.\n",
    "\n",
    "    SubwordLookup holds a mapping from token indices to variable length subword\n",
    "    arrays and allows fast access to padded and masked batches of subwords\n",
    "    given a list of token indices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    length : int\n",
    "         Number of tokens for which to hold subword arrays.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, length):\n",
    "        self.idx_to_subwordidxs = [\n",
    "            np.arange(1).astype(np.int_) for _ in range(length)\n",
    "        ]\n",
    "\n",
    "    def set(self, i, subwords):\n",
    "        \"\"\"Set the subword array of the i-th token.\"\"\"\n",
    "        self.idx_to_subwordidxs[i] = subwords\n",
    "\n",
    "    def get(self, indices):\n",
    "        \"\"\"Get a padded array and mask of subwords for specified indices.\"\"\"\n",
    "        subwords = [self.idx_to_subwordidxs[i] for i in indices]\n",
    "        lengths = np.array([len(s) for s in subwords])\n",
    "        length = np.max(lengths)\n",
    "        subwords_arr = np.zeros((len(subwords), length))\n",
    "        mask = np.zeros((len(subwords), length))\n",
    "        for i in numba_prange(len(subwords)):\n",
    "            s = subwords[i]\n",
    "            subwords_arr[i, :len(s)] = s\n",
    "            mask[i, :len(s)] = 1\n",
    "        return subwords_arr, mask\n",
    "\n",
    "subword_lookup = SubwordLookup(len(idx_to_subwordidxs))\n",
    "for i, subwords in enumerate(idx_to_subwordidxs):\n",
    "    subword_lookup.set(i, np.array(subwords, dtype=np.int_))\n",
    "\n",
    "def skipgram_fasttext_batch(data):\n",
    "    \"\"\"Create a batch for Skipgram training objective.\"\"\"\n",
    "    centers, word_context, word_context_mask = data\n",
    "    assert len(centers.shape) == 2\n",
    "    negatives_shape = (len(word_context), 2 * window_size * num_negatives)\n",
    "    negatives, negatives_mask = remove_accidental_hits(\n",
    "        negatives_sampler(negatives_shape), word_context)\n",
    "    context_negatives = mx.nd.concat(word_context, negatives, dim=1)\n",
    "    masks = mx.nd.concat(word_context_mask, negatives_mask, dim=1)\n",
    "    labels = mx.nd.concat(word_context_mask, mx.nd.zeros_like(negatives), dim=1)\n",
    "    \n",
    "    unique, inverse_unique_indices = np.unique(centers.asnumpy(),\n",
    "                                               return_inverse=True)\n",
    "    inverse_unique_indices = mx.nd.array(inverse_unique_indices,\n",
    "                                         ctx=context)\n",
    "    subwords, subwords_mask = subword_lookup.get(unique.astype(int))\n",
    "\n",
    "    return (centers.as_in_context(context),\n",
    "            context_negatives.as_in_context(context),\n",
    "            masks.as_in_context(context),\n",
    "            labels.as_in_context(context),\n",
    "            mx.nd.array(subwords, ctx=context),\n",
    "            mx.nd.array(subwords_mask, ctx=context),\n",
    "            inverse_unique_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fasttext_embedding(num_epochs):\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        start_time = time.time()\n",
    "        train_l_sum = 0\n",
    "        num_samples = 0\n",
    "        for i, data in enumerate(batches):\n",
    "            (center, context_negatives, mask, label, subwords,\n",
    "             subwords_mask, inverse_unique_indices) = skipgram_fasttext_batch(data)\n",
    "            with mx.autograd.record():\n",
    "                emb_in = embedding(center, subwords,\n",
    "                   subwordsmask=subwords_mask,\n",
    "                   words_to_unique_subwords_indices=\n",
    "                   inverse_unique_indices)\n",
    "                emb_out = embedding_out(context_negatives, mask)\n",
    "                pred = mx.nd.batch_dot(emb_in, emb_out.swapaxes(1, 2))\n",
    "                l = (loss(pred, label, mask) * mask.shape[1] / mask.sum(axis=1))\n",
    "            l.backward()\n",
    "            trainer.step(1)\n",
    "            train_l_sum += l.sum()\n",
    "            num_samples += center.shape[0]\n",
    "            if i % 500 == 0:\n",
    "                mx.nd.waitall()\n",
    "                wps = num_samples / (time.time() - start_time)\n",
    "                print('epoch %d, time %.2fs, iteration %d, throughput=%.2fK wps'\n",
    "                      % (epoch, time.time() - start_time, i, wps / 1000))\n",
    "\n",
    "        print('epoch %d, time %.2fs, train loss %.2f'\n",
    "              % (epoch, time.time() - start_time,\n",
    "                 train_l_sum.asscalar() / num_samples))\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, time 1.15s, iteration 0, throughput=1.79K wps\n",
      "epoch 1, time 31.93s, iteration 500, throughput=32.13K wps\n",
      "epoch 1, time 62.05s, iteration 1000, throughput=33.04K wps\n",
      "epoch 1, time 91.97s, iteration 1500, throughput=33.42K wps\n",
      "epoch 1, time 121.87s, iteration 2000, throughput=33.63K wps\n",
      "epoch 1, time 125.26s, train loss 0.29\n",
      "\n",
      "epoch 2, time 0.42s, iteration 0, throughput=4.90K wps\n",
      "epoch 2, time 30.46s, iteration 500, throughput=33.68K wps\n",
      "epoch 2, time 60.69s, iteration 1000, throughput=33.78K wps\n",
      "epoch 2, time 90.62s, iteration 1500, throughput=33.92K wps\n",
      "epoch 2, time 121.22s, iteration 2000, throughput=33.81K wps\n",
      "epoch 2, time 124.74s, train loss 0.27\n",
      "\n",
      "epoch 3, time 0.43s, iteration 0, throughput=4.76K wps\n",
      "epoch 3, time 30.75s, iteration 500, throughput=33.37K wps\n",
      "epoch 3, time 60.96s, iteration 1000, throughput=33.63K wps\n",
      "epoch 3, time 90.88s, iteration 1500, throughput=33.82K wps\n",
      "epoch 3, time 120.96s, iteration 2000, throughput=33.88K wps\n",
      "epoch 3, time 124.37s, train loss 0.27\n",
      "\n",
      "epoch 4, time 0.43s, iteration 0, throughput=4.76K wps\n",
      "epoch 4, time 30.54s, iteration 500, throughput=33.59K wps\n",
      "epoch 4, time 60.52s, iteration 1000, throughput=33.87K wps\n",
      "epoch 4, time 90.60s, iteration 1500, throughput=33.93K wps\n",
      "epoch 4, time 121.34s, iteration 2000, throughput=33.77K wps\n",
      "epoch 4, time 125.48s, train loss 0.26\n",
      "\n",
      "epoch 5, time 0.43s, iteration 0, throughput=4.75K wps\n",
      "epoch 5, time 31.41s, iteration 500, throughput=32.66K wps\n",
      "epoch 5, time 62.20s, iteration 1000, throughput=32.96K wps\n",
      "epoch 5, time 93.28s, iteration 1500, throughput=32.96K wps\n",
      "epoch 5, time 123.72s, iteration 2000, throughput=33.12K wps\n",
      "epoch 5, time 127.43s, train loss 0.26\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_fasttext_embedding(num_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Thanks to the subword support of the `FasttextEmbeddingModel` we can now evaluate on all words in the evaluation dataset, not only the ones that we observed during training (the `SimpleEmbeddingModel` only provides vectors for words observed at training).\n",
    "\n",
    "We first find the all tokens in the evaluation dataset and then convert the `FasttextEmbeddingModel` to a `TokenEmbedding` with exactly those tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 437 unique tokens in WordSim353\n",
      "The imputed TokenEmbedding has shape (437, 300)\n"
     ]
    }
   ],
   "source": [
    "wordsim353_tokens  = list(set(itertools.chain.from_iterable((d[0], d[1]) for d in wordsim353)))\n",
    "token_embedding = nlp.embedding.TokenEmbedding(unknown_token=None, allow_extend=True)\n",
    "token_embedding[wordsim353_tokens] = embedding[wordsim353_tokens]\n",
    "\n",
    "print('There are', len(wordsim353_tokens), 'unique tokens in WordSim353')\n",
    "print('The imputed TokenEmbedding has shape', token_embedding.idx_to_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = nlp.embedding.evaluation.WordEmbeddingSimilarity(\n",
    "    idx_to_vec=token_embedding.idx_to_vec,\n",
    "    similarity_function=\"CosineSimilarity\")\n",
    "evaluator.initialize(ctx=context)\n",
    "evaluator.hybridize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "words1, words2, scores = zip(*([token_embedding.token_to_idx[d[0]],\n",
    "                                token_embedding.token_to_idx[d[1]],\n",
    "                                d[2]] for d in wordsim353))\n",
    "words1 = mx.nd.array(words1, ctx=context)\n",
    "words2 = mx.nd.array(words2, ctx=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman rank correlation on 352 pairs of WordSim353: 0.569\n"
     ]
    }
   ],
   "source": [
    "pred_similarity = evaluator(words1, words2)\n",
    "sr = stats.spearmanr(pred_similarity.asnumpy(), np.array(scores))\n",
    "print('Spearman rank correlation on {} pairs of {}: {}'.format(\n",
    "    len(words1), wordsim353.__class__.__name__, sr.correlation.round(3)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
