{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis (SA) with pretrained Language Model (LM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are going to build a sentiment analysis model based on the pretrained\n",
    "language model. We are focusing on the best usability to support traditional nlp tasks in a simple fashion. The building process is really simple in three steps. Let us get started now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation and settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load mxnet and gluonnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import nd, gluon, autograd\n",
    "\n",
    "import gluonnlp as nlp\n",
    "from gluonnlp import data, Vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0\n",
    "language_model_name = 'standard_lstm_lm_200'\n",
    "pretrained = True\n",
    "num_gpus = 1\n",
    "learning_rate = 0.005 * num_gpus\n",
    "batch_size = 16 * num_gpus\n",
    "bucket_num = 10\n",
    "bucket_ratio = 0.5\n",
    "epochs = 1\n",
    "grad_clip = 2\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = [mx.gpu(i) for i in range(num_gpus)] if num_gpus else [mx.cpu()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis model with pre-trained language model encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model architecture is based on pretrained LM:\n",
    "\n",
    "![sa-model](samodel-v3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentNet(\n",
      "  (out_layer): HybridSequential(\n",
      "    (0): Dropout(p = 0, axes=())\n",
      "    (1): Dense(None -> 1, linear)\n",
      "  )\n",
      "  (embedding): HybridSequential(\n",
      "    (0): Embedding(33278 -> 200, float32)\n",
      "  )\n",
      "  (encoder): LSTM(200 -> 800, TNC, num_layers=2)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/mxnet/gluon/parameter.py:321: UserWarning: Parameter sent_net_hybridsequential0_embedding0_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/usr/local/lib/python3.5/dist-packages/mxnet/gluon/parameter.py:321: UserWarning: Parameter sent_net_lstm0_l0_i2h_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/usr/local/lib/python3.5/dist-packages/mxnet/gluon/parameter.py:321: UserWarning: Parameter sent_net_lstm0_l0_h2h_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/usr/local/lib/python3.5/dist-packages/mxnet/gluon/parameter.py:321: UserWarning: Parameter sent_net_lstm0_l0_i2h_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/usr/local/lib/python3.5/dist-packages/mxnet/gluon/parameter.py:321: UserWarning: Parameter sent_net_lstm0_l0_h2h_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/usr/local/lib/python3.5/dist-packages/mxnet/gluon/parameter.py:321: UserWarning: Parameter sent_net_lstm0_l1_i2h_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/usr/local/lib/python3.5/dist-packages/mxnet/gluon/parameter.py:321: UserWarning: Parameter sent_net_lstm0_l1_h2h_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/usr/local/lib/python3.5/dist-packages/mxnet/gluon/parameter.py:321: UserWarning: Parameter sent_net_lstm0_l1_i2h_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/usr/local/lib/python3.5/dist-packages/mxnet/gluon/parameter.py:321: UserWarning: Parameter sent_net_lstm0_l1_h2h_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n"
     ]
    }
   ],
   "source": [
    "class SentimentNet(gluon.Block):\n",
    "    def __init__(self, embedding_block, encoder_block, dropout, prefix=None, params=None):\n",
    "        super(SentimentNet, self).__init__(prefix=prefix, params=params)\n",
    "        with self.name_scope():\n",
    "            self.embedding = embedding_block\n",
    "            self.encoder = encoder_block\n",
    "            self.out_layer = gluon.nn.HybridSequential()\n",
    "            with self.out_layer.name_scope():\n",
    "                self.out_layer.add(gluon.nn.Dropout(dropout))\n",
    "                self.out_layer.add(gluon.nn.Dense(1, flatten=False))\n",
    "\n",
    "    def forward(self, data, valid_length):\n",
    "        encoded = self.encoder(nd.Dropout(self.embedding(data), 0.6, axes=(0,)))  # Shape(T, N, C)\n",
    "        masked_encoded = nd.SequenceMask(encoded,\n",
    "                                        sequence_length=valid_length,\n",
    "                                        use_sequence_length=True)\n",
    "        agg_state = nd.broadcast_div(nd.sum(masked_encoded, axis=0),\n",
    "                                     nd.expand_dims(valid_length, axis=1))\n",
    "        out = self.out_layer(agg_state)\n",
    "        return out\n",
    "\n",
    "lm_model, vocab = nlp.model.get_model(name=language_model_name,\n",
    "                                      pretrained=pretrained,\n",
    "                                      ctx=context,\n",
    "                                      dropout=dropout,\n",
    "                                      prefix='sent_net_')\n",
    "net = SentimentNet(embedding_block=lm_model.embedding, encoder_block=lm_model.encoder,\n",
    "                   dropout=dropout, prefix='sent_net_')\n",
    "net.initialize(mx.init.Xavier(), ctx=context)\n",
    "net.hybridize()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sentiment analysis dataset -- IMDB reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize using spaCy...\n",
      "Done! Tokenizing Time=5.26s, #Sentences=25000\n",
      "Done! Tokenizing Time=4.97s, #Sentences=25000\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = [data.IMDB(root='data/imdb', segment=segment) for segment in ('train', 'test')]\n",
    "print(\"Tokenize using spaCy...\")\n",
    "tokenizer = nlp.data.SpacyTokenizer('en')\n",
    "length_clip = nlp.data.ClipSequence(500)\n",
    "\n",
    "def preprocess(x):\n",
    "    data, label = x\n",
    "    label = int(label > 5)\n",
    "    data = vocab[length_clip(tokenizer(data))]\n",
    "    return data, label, float(len(data))\n",
    "\n",
    "def get_length(x):\n",
    "    return x[2]\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    start = time.time()\n",
    "    with mp.Pool(32) as pool:\n",
    "        dataset = gluon.data.SimpleDataset(pool.map(preprocess, dataset))\n",
    "        lengths = gluon.data.SimpleDataset(pool.map(get_length, dataset))\n",
    "    end = time.time()\n",
    "    print('Done! Tokenizing Time={:.2f}s, #Sentences={}'.format(end - start, len(dataset)))\n",
    "    return dataset, lengths\n",
    "\n",
    "train_dataset, train_data_lengths = preprocess_dataset(train_dataset)\n",
    "test_dataset, test_data_lengths = preprocess_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation using loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, dataloader, context):\n",
    "    loss = gluon.loss.SigmoidBCELoss()\n",
    "    total_L = 0.0\n",
    "    total_sample_num = 0\n",
    "    total_correct_num = 0\n",
    "    start_log_interval_time = time.time()\n",
    "    print('Begin Testing...')\n",
    "    for i, (data, label, valid_length) in enumerate(dataloader):\n",
    "        data = mx.nd.transpose(data.as_in_context(context))\n",
    "        valid_length = valid_length.as_in_context(context).astype(np.float32)\n",
    "        label = label.as_in_context(context)\n",
    "        output = net(data, valid_length)\n",
    "        L = loss(output, label)\n",
    "        pred = (output > 0.5).reshape(-1)\n",
    "        total_L += L.sum().asscalar()\n",
    "        total_sample_num += label.shape[0]\n",
    "        total_correct_num += (pred == label).sum().asscalar()\n",
    "        if (i + 1) % log_interval == 0:\n",
    "            print('[Batch {}/{}] elapsed {:.2f} s'.format(\n",
    "                i + 1, len(dataloader), time.time() - start_log_interval_time))\n",
    "            start_log_interval_time = time.time()\n",
    "    avg_L = total_L / float(total_sample_num)\n",
    "    acc = total_correct_num / float(total_sample_num)\n",
    "    return avg_L, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, context, epochs):\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'ftml', {'learning_rate': learning_rate})\n",
    "    loss = gluon.loss.SigmoidBCELoss()\n",
    "\n",
    "    # Construct the DataLoader\n",
    "    batchify_fn = nlp.data.batchify.Tuple(nlp.data.batchify.Pad(axis=0), \n",
    "                                          nlp.data.batchify.Stack(),\n",
    "                                          nlp.data.batchify.Stack()) # Pad data, stack label and lengths\n",
    "    batch_sampler = nlp.data.sampler.FixedBucketSampler(train_data_lengths,\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        num_buckets=bucket_num,\n",
    "                                                        ratio=bucket_ratio,\n",
    "                                                        shuffle=True)\n",
    "    print(batch_sampler.stats())\n",
    "    train_dataloader = gluon.data.DataLoader(dataset=train_dataset,\n",
    "                                             batch_sampler=batch_sampler,\n",
    "                                             batchify_fn=batchify_fn)\n",
    "    test_dataloader = gluon.data.DataLoader(dataset=test_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False,\n",
    "                                            batchify_fn=batchify_fn)\n",
    "    parameters = net.collect_params().values()\n",
    "\n",
    "    # Training/Testing\n",
    "    for epoch in range(epochs):\n",
    "        # Epoch training stats\n",
    "        start_epoch_time = time.time()\n",
    "        epoch_L = 0.0\n",
    "        epoch_sent_num = 0\n",
    "        epoch_wc = 0\n",
    "        # Log interval training stats\n",
    "        start_log_interval_time = time.time()\n",
    "        log_interval_wc = 0\n",
    "        log_interval_sent_num = 0\n",
    "        log_interval_L = 0.0\n",
    "\n",
    "        for i, (data, label, length) in enumerate(train_dataloader):\n",
    "            if data.shape[0] > len(context):\n",
    "                data_list = gluon.utils.split_and_load(data, context, batch_axis=0, even_split=False)\n",
    "                label_list = gluon.utils.split_and_load(label, context, batch_axis=0, even_split=False)\n",
    "                length_list = gluon.utils.split_and_load(length, context, batch_axis=0, even_split=False)\n",
    "            else:\n",
    "                data_list = [data.as_in_context(context[0])]\n",
    "                label_list = [label.as_in_context(context[0])]\n",
    "                length_list = [length.as_in_context(context[0])]\n",
    "            L = 0\n",
    "            wc = length.sum().asscalar()\n",
    "            log_interval_wc += wc\n",
    "            epoch_wc += wc\n",
    "            log_interval_sent_num += data.shape[1]\n",
    "            epoch_sent_num += data.shape[1]\n",
    "            for data, label, valid_length in zip(data_list, label_list, length_list):\n",
    "                valid_length = valid_length\n",
    "                with autograd.record():\n",
    "                    output = net(data.T, valid_length)\n",
    "                    L = L + loss(output, label).mean().as_in_context(context[0])\n",
    "            L.backward()\n",
    "            # Clip gradient\n",
    "            if grad_clip:\n",
    "                gluon.utils.clip_global_norm([p.grad(x.context) for p in parameters for x in data_list],\n",
    "                                             grad_clip)\n",
    "            # Update parameter\n",
    "            trainer.step(1)\n",
    "            log_interval_L += L.asscalar()\n",
    "            epoch_L += L.asscalar()\n",
    "            if (i + 1) % log_interval == 0:\n",
    "                print('[Epoch {} Batch {}/{}] elapsed {:.2f} s, avg loss {:.6f}, throughput {:.2f}K wps'.format(\n",
    "                    epoch, i + 1, len(train_dataloader), time.time() - start_log_interval_time,\n",
    "                    log_interval_L / log_interval_sent_num,\n",
    "                    log_interval_wc / 1000 / (time.time() - start_log_interval_time)))\n",
    "                # Clear log interval training stats\n",
    "                start_log_interval_time = time.time()\n",
    "                log_interval_wc = 0\n",
    "                log_interval_sent_num = 0\n",
    "                log_interval_L = 0\n",
    "        end_epoch_time = time.time()\n",
    "        test_avg_L, test_acc = evaluate(net, test_dataloader, context[0])\n",
    "        print('[Epoch {}] train avg loss {:.6f}, test acc {:.2f}, test avg loss {:.6f}, throughput {:.2f}K wps'.format(\n",
    "            epoch, epoch_L / epoch_sent_num,\n",
    "            test_acc, test_avg_L, epoch_wc / 1000 / (end_epoch_time - start_epoch_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FixedBucketSampler:\n",
      "  sample_num=25000, batch_num=1319\n",
      "  key=[14, 68, 122, 176, 230, 284, 338, 392, 446, 500]\n",
      "  cnt=[5, 976, 2353, 6662, 4470, 2661, 1836, 1385, 1012, 3640]\n",
      "  batch_size=[285, 58, 32, 22, 17, 16, 16, 16, 16, 16]\n",
      "[Epoch 0 Batch 100/1319] elapsed 9.39 s, avg loss 0.002415, throughput 50.44K wps\n",
      "[Epoch 0 Batch 200/1319] elapsed 8.70 s, avg loss 0.002434, throughput 52.44K wps\n",
      "[Epoch 0 Batch 300/1319] elapsed 8.00 s, avg loss 0.002218, throughput 61.50K wps\n",
      "[Epoch 0 Batch 400/1319] elapsed 8.24 s, avg loss 0.002379, throughput 56.03K wps\n",
      "[Epoch 0 Batch 500/1319] elapsed 7.40 s, avg loss 0.002367, throughput 61.94K wps\n",
      "[Epoch 0 Batch 600/1319] elapsed 8.13 s, avg loss 0.002169, throughput 59.47K wps\n",
      "[Epoch 0 Batch 700/1319] elapsed 8.25 s, avg loss 0.002181, throughput 58.41K wps\n",
      "[Epoch 0 Batch 800/1319] elapsed 8.37 s, avg loss 0.001969, throughput 58.19K wps\n",
      "[Epoch 0 Batch 900/1319] elapsed 8.83 s, avg loss 0.002150, throughput 50.71K wps\n",
      "[Epoch 0 Batch 1000/1319] elapsed 8.08 s, avg loss 0.001967, throughput 58.79K wps\n",
      "[Epoch 0 Batch 1100/1319] elapsed 7.90 s, avg loss 0.001866, throughput 58.90K wps\n",
      "[Epoch 0 Batch 1200/1319] elapsed 7.60 s, avg loss 0.001938, throughput 58.56K wps\n",
      "[Epoch 0 Batch 1300/1319] elapsed 7.71 s, avg loss 0.001821, throughput 57.63K wps\n",
      "Begin Testing...\n",
      "[Batch 100/1563] elapsed 5.60 s\n",
      "[Batch 200/1563] elapsed 5.89 s\n",
      "[Batch 300/1563] elapsed 6.23 s\n",
      "[Batch 400/1563] elapsed 5.77 s\n",
      "[Batch 500/1563] elapsed 6.02 s\n",
      "[Batch 600/1563] elapsed 5.81 s\n",
      "[Batch 700/1563] elapsed 6.28 s\n",
      "[Batch 800/1563] elapsed 8.65 s\n",
      "[Batch 900/1563] elapsed 11.79 s\n",
      "[Batch 1000/1563] elapsed 5.64 s\n",
      "[Batch 1100/1563] elapsed 5.47 s\n",
      "[Batch 1200/1563] elapsed 5.91 s\n",
      "[Batch 1300/1563] elapsed 5.57 s\n",
      "[Batch 1400/1563] elapsed 5.83 s\n",
      "[Batch 1500/1563] elapsed 6.24 s\n",
      "[Epoch 0] train avg loss 0.002140, test acc 0.77, test avg loss 0.445162, throughput 56.95K wps\n"
     ]
    }
   ],
   "source": [
    "train(net, context, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0.7000658]]\n",
       "<NDArray 1x1 @gpu(0)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(mx.nd.reshape(mx.nd.array(vocab[['This', 'movie', 'is', 'amazing']], ctx=context[0]), shape=(-1, 1)),\n",
    "    mx.nd.array([4], ctx=context[0])).sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, we have built a SA model using gluonnlp. It is:\n",
    "\n",
    "1) easy to use.\n",
    "\n",
    "2) simple to customize.\n",
    "\n",
    "3) fast to build the NLP prototype.\n",
    "\n",
    "Gluonnlp documentation is here: http://gluon-nlp.s3-accelerate.dualstack.amazonaws.com/index.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
