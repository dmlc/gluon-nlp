{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis (SA) with pretrained Language Model (LM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are going to build a sentiment analysis model based on the pretrained\n",
    "language model. We are focusing on the best usability to support traditional nlp tasks in a simple fashion. The building process is really simple in three steps. Let us get started now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation and settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load mxnet and gluonnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import nd, gluon, autograd\n",
    "\n",
    "import gluonnlp as nlp\n",
    "from gluonnlp import datasets, Vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.5\n",
    "language_model_name = 'standard_lstm_lm_200'\n",
    "pretrained = True\n",
    "num_gpus = 4\n",
    "learning_rate = 0.01 * num_gpus\n",
    "batch_size = 20 * num_gpus\n",
    "bucket_num = 10\n",
    "bucket_ratio = 0.5\n",
    "epochs = 15\n",
    "grad_clip = 0.25\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = [mx.gpu(i) for i in range(num_gpus)] if num_gpus else [mx.cpu()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis model with pre-trained language model encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model architecture is based on pretrained LM:\n",
    "\n",
    "![sa-model](samodel-v3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentNet(\n",
      "  (encoder): LSTM(200 -> 800, TNC, num_layers=2, dropout=0.5)\n",
      "  (out_layer): HybridSequential(\n",
      "    (0): Dropout(p = 0.5, axes=())\n",
      "    (1): Dense(None -> 1, linear)\n",
      "  )\n",
      "  (embedding): HybridSequential(\n",
      "    (0): Embedding(33279 -> 200, float32)\n",
      "    (1): Dropout(p = 0.5, axes=())\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/mxnet/gluon/parameter.py:321: UserWarning: Parameter sent_net_hybridsequential0_embedding0_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/usr/local/lib/python3.5/dist-packages/mxnet/gluon/parameter.py:321: UserWarning: Parameter sent_net_lstm0_l0_i2h_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/usr/local/lib/python3.5/dist-packages/mxnet/gluon/parameter.py:321: UserWarning: Parameter sent_net_lstm0_l0_h2h_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/usr/local/lib/python3.5/dist-packages/mxnet/gluon/parameter.py:321: UserWarning: Parameter sent_net_lstm0_l0_i2h_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/usr/local/lib/python3.5/dist-packages/mxnet/gluon/parameter.py:321: UserWarning: Parameter sent_net_lstm0_l0_h2h_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/usr/local/lib/python3.5/dist-packages/mxnet/gluon/parameter.py:321: UserWarning: Parameter sent_net_lstm0_l1_i2h_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/usr/local/lib/python3.5/dist-packages/mxnet/gluon/parameter.py:321: UserWarning: Parameter sent_net_lstm0_l1_h2h_weight is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/usr/local/lib/python3.5/dist-packages/mxnet/gluon/parameter.py:321: UserWarning: Parameter sent_net_lstm0_l1_i2h_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n",
      "/usr/local/lib/python3.5/dist-packages/mxnet/gluon/parameter.py:321: UserWarning: Parameter sent_net_lstm0_l1_h2h_bias is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  \"Set force_reinit=True to re-initialize.\"%self.name)\n"
     ]
    }
   ],
   "source": [
    "class SentimentNet(gluon.Block):\n",
    "    def __init__(self, embedding_block, encoder_block, dropout, prefix=None, params=None):\n",
    "        super(SentimentNet, self).__init__(prefix=prefix, params=params)\n",
    "        with self.name_scope():\n",
    "            self.embedding = embedding_block\n",
    "            self.encoder = encoder_block\n",
    "            self.out_layer = gluon.nn.HybridSequential()\n",
    "            with self.out_layer.name_scope():\n",
    "                self.out_layer.add(gluon.nn.Dropout(dropout))\n",
    "                self.out_layer.add(gluon.nn.Dense(1, flatten=False))\n",
    "\n",
    "    def forward(self, data, valid_length):\n",
    "        encoded = self.encoder(nd.Dropout(self.embedding(data), 0.6, axes=(0,)))  # Shape(T, N, C)\n",
    "        masked_encoded = nd.SequenceMask(encoded,\n",
    "                                        sequence_length=valid_length,\n",
    "                                        use_sequence_length=True)\n",
    "        agg_state = nd.broadcast_div(nd.sum(masked_encoded, axis=0),\n",
    "                                     nd.expand_dims(valid_length, axis=1))\n",
    "        out = self.out_layer(agg_state)\n",
    "        return out\n",
    "\n",
    "lm_model, vocab = nlp.models.get_model(name=language_model_name,\n",
    "                                       pretrained=pretrained,\n",
    "                                       ctx=context,\n",
    "                                       dropout=dropout,\n",
    "                                       prefix='sent_net_')\n",
    "net = SentimentNet(embedding_block=lm_model.embedding, encoder_block=lm_model.encoder,\n",
    "                   dropout=dropout, prefix='sent_net_')\n",
    "net.initialize(mx.init.Xavier(), ctx=context)\n",
    "net.hybridize()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sentiment analysis dataset -- IMDB reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize using spaCy...\n",
      "Done! Tokenizing Time=6.03s, #Sentences=25000\n",
      "Done! Tokenizing Time=7.48s, #Sentences=25000\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = [datasets.IMDB(root='data/imdb', segment=segment) for segment in ('train', 'test')]\n",
    "print(\"Tokenize using spaCy...\")\n",
    "tokenizer = nlp.data.SpacyTokenizer('en')\n",
    "length_clip = nlp.data.ClipSequence(500)\n",
    "\n",
    "def preprocess(x):\n",
    "    data, label = x\n",
    "    label = int(label > 5)\n",
    "    data = vocab[length_clip(tokenizer(data))]\n",
    "    return data, label, float(len(data))\n",
    "\n",
    "def get_length(x):\n",
    "    return x[2]\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    start = time.time()\n",
    "    with mp.Pool(32) as pool:\n",
    "        dataset = gluon.data.SimpleDataset(pool.map(preprocess, dataset))\n",
    "        lengths = gluon.data.SimpleDataset(pool.map(get_length, dataset))\n",
    "    end = time.time()\n",
    "    print('Done! Tokenizing Time={:.2f}s, #Sentences={}'.format(end - start, len(dataset)))\n",
    "    return dataset, lengths\n",
    "\n",
    "train_dataset, train_data_lengths = preprocess_dataset(train_dataset)\n",
    "test_dataset, test_data_lengths = preprocess_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation using loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, dataloader, context):\n",
    "    loss = gluon.loss.SigmoidBCELoss()\n",
    "    total_L = 0.0\n",
    "    total_sample_num = 0\n",
    "    total_correct_num = 0\n",
    "    start_log_interval_time = time.time()\n",
    "    print('Begin Testing...')\n",
    "    for i, (data, label, valid_length) in enumerate(dataloader):\n",
    "        data = mx.nd.transpose(data.as_in_context(context))\n",
    "        valid_length = valid_length.as_in_context(context).astype(np.float32)\n",
    "        label = label.as_in_context(context)\n",
    "        output = net(data, valid_length)\n",
    "        L = loss(output, label)\n",
    "        pred = (output > 0.5).reshape(-1)\n",
    "        total_L += L.sum().asscalar()\n",
    "        total_sample_num += label.shape[0]\n",
    "        total_correct_num += (pred == label).sum().asscalar()\n",
    "        if (i + 1) % log_interval == 0:\n",
    "            print('[Batch {}/{}] elapsed {:.2f} s'.format(\n",
    "                i + 1, len(dataloader), time.time() - start_log_interval_time))\n",
    "            start_log_interval_time = time.time()\n",
    "    avg_L = total_L / float(total_sample_num)\n",
    "    acc = total_correct_num / float(total_sample_num)\n",
    "    return avg_L, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, context, epochs):\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'adagrad', {'learning_rate': learning_rate,\n",
    "                                                              'wd': 0.001})\n",
    "    loss = gluon.loss.SigmoidBCELoss()\n",
    "\n",
    "    # Construct the DataLoader\n",
    "    batchify_fn = nlp.data.batchify.Wrap(nlp.data.batchify.Pad(axis=0), \n",
    "                                         nlp.data.batchify.Stack(),\n",
    "                                         nlp.data.batchify.Stack())  # Pad data, stack label and lengths\n",
    "    batch_sampler = nlp.data.sampler.FixedBucketSampler(train_data_lengths,\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        num_buckets=bucket_num,\n",
    "                                                        ratio=bucket_ratio,\n",
    "                                                        shuffle=True)\n",
    "    print(batch_sampler.stats())\n",
    "    train_dataloader = gluon.data.DataLoader(dataset=train_dataset,\n",
    "                                             batch_sampler=batch_sampler,\n",
    "                                             batchify_fn=batchify_fn)\n",
    "    test_dataloader = gluon.data.DataLoader(dataset=test_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False,\n",
    "                                            batchify_fn=batchify_fn)\n",
    "    parameters = net.collect_params().values()\n",
    "\n",
    "    # Training/Testing\n",
    "    for epoch in range(epochs):\n",
    "        # Epoch training stats\n",
    "        start_epoch_time = time.time()\n",
    "        epoch_L = 0.0\n",
    "        epoch_sent_num = 0\n",
    "        epoch_wc = 0\n",
    "        # Log interval training stats\n",
    "        start_log_interval_time = time.time()\n",
    "        log_interval_wc = 0\n",
    "        log_interval_sent_num = 0\n",
    "        log_interval_L = 0.0\n",
    "\n",
    "        for i, (data, label, length) in enumerate(train_dataloader):\n",
    "            if data.shape[0] > len(context):\n",
    "                data_list = gluon.utils.split_and_load(data, context, batch_axis=0, even_split=False)\n",
    "                label_list = gluon.utils.split_and_load(label, context, batch_axis=0, even_split=False)\n",
    "                length_list = gluon.utils.split_and_load(length, context, batch_axis=0, even_split=False)\n",
    "            else:\n",
    "                data_list = [data.as_in_context(context[0])]\n",
    "                label_list = [label.as_in_context(context[0])]\n",
    "                length_list = [length.as_in_context(context[0])]\n",
    "            L = 0\n",
    "            wc = length.sum().asscalar()\n",
    "            log_interval_wc += wc\n",
    "            epoch_wc += wc\n",
    "            log_interval_sent_num += data.shape[1]\n",
    "            epoch_sent_num += data.shape[1]\n",
    "            for data, label, valid_length in zip(data_list, label_list, length_list):\n",
    "                valid_length = valid_length\n",
    "                with autograd.record():\n",
    "                    output = net(data.T, valid_length)\n",
    "                    L = L + loss(output, label).mean().as_in_context(context[0])\n",
    "            L.backward()\n",
    "            # Clip gradient\n",
    "            if grad_clip:\n",
    "                gluon.utils.clip_global_norm([p.grad(x.context) for p in parameters for x in data_list],\n",
    "                                             grad_clip)\n",
    "            # Update parameter\n",
    "            trainer.step(1)\n",
    "            log_interval_L += L.asscalar()\n",
    "            epoch_L += L.asscalar()\n",
    "            if (i + 1) % log_interval == 0:\n",
    "                print('[Epoch {} Batch {}/{}] elapsed {:.2f} s, avg loss {:.6f}, throughput {:.2f}K wps'.format(\n",
    "                    epoch, i + 1, len(train_dataloader), time.time() - start_log_interval_time,\n",
    "                    log_interval_L / log_interval_sent_num,\n",
    "                    log_interval_wc / 1000 / (time.time() - start_log_interval_time)))\n",
    "                # Clear log interval training stats\n",
    "                start_log_interval_time = time.time()\n",
    "                log_interval_wc = 0\n",
    "                log_interval_sent_num = 0\n",
    "                log_interval_L = 0\n",
    "        end_epoch_time = time.time()\n",
    "        test_avg_L, test_acc = evaluate(net, test_dataloader, context[0])\n",
    "        print('[Epoch {}] train avg loss {:.6f}, test acc {:.2f}, test avg loss {:.6f}, throughput {:.2f}K wps'.format(\n",
    "            epoch, epoch_L / epoch_sent_num,\n",
    "            test_acc, test_avg_L, epoch_wc / 1000 / (end_epoch_time - start_epoch_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FixedBucketSampler:\n",
      "  sample_num=25000, batch_num=265\n",
      "  key=[14, 68, 122, 176, 230, 284, 338, 392, 446, 500]\n",
      "  cnt=[5, 976, 2353, 6662, 4470, 2661, 1836, 1385, 1012, 3640]\n",
      "  batch_size=[1428, 294, 163, 113, 86, 80, 80, 80, 80, 80]\n",
      "[Epoch 0 Batch 100/265] elapsed 20.86 s, avg loss 0.009846, throughput 109.40K wps\n",
      "[Epoch 0 Batch 200/265] elapsed 19.95 s, avg loss 0.007972, throughput 124.55K wps\n",
      "Begin Testing...\n",
      "[Batch 100/313] elapsed 12.09 s\n",
      "[Batch 200/313] elapsed 9.88 s\n",
      "[Batch 300/313] elapsed 10.35 s\n",
      "[Epoch 0] train avg loss 0.008628, test acc 0.71, test avg loss 0.534941, throughput 115.87K wps\n",
      "[Epoch 1 Batch 100/265] elapsed 18.95 s, avg loss 0.005504, throughput 158.88K wps\n",
      "[Epoch 1 Batch 200/265] elapsed 18.32 s, avg loss 0.008851, throughput 102.06K wps\n",
      "Begin Testing...\n",
      "[Batch 100/313] elapsed 12.08 s\n",
      "[Batch 200/313] elapsed 11.75 s\n",
      "[Batch 300/313] elapsed 11.34 s\n",
      "[Epoch 1] train avg loss 0.007137, test acc 0.75, test avg loss 0.480933, throughput 126.94K wps\n",
      "[Epoch 2 Batch 100/265] elapsed 18.35 s, avg loss 0.007726, throughput 107.42K wps\n",
      "[Epoch 2 Batch 200/265] elapsed 20.12 s, avg loss 0.004616, throughput 153.73K wps\n",
      "Begin Testing...\n",
      "[Batch 100/313] elapsed 12.59 s\n",
      "[Batch 200/313] elapsed 10.75 s\n",
      "[Batch 300/313] elapsed 11.51 s\n",
      "[Epoch 2] train avg loss 0.006288, test acc 0.81, test avg loss 0.422101, throughput 117.66K wps\n",
      "[Epoch 3 Batch 100/265] elapsed 20.22 s, avg loss 0.004104, throughput 159.45K wps\n",
      "[Epoch 3 Batch 200/265] elapsed 18.90 s, avg loss 0.006678, throughput 98.29K wps\n",
      "Begin Testing...\n",
      "[Batch 100/313] elapsed 11.79 s\n",
      "[Batch 200/313] elapsed 12.56 s\n",
      "[Batch 300/313] elapsed 10.64 s\n",
      "[Epoch 3] train avg loss 0.005662, test acc 0.80, test avg loss 0.418549, throughput 123.45K wps\n",
      "[Epoch 4 Batch 100/265] elapsed 18.10 s, avg loss 0.007802, throughput 102.02K wps\n",
      "[Epoch 4 Batch 200/265] elapsed 18.76 s, avg loss 0.005220, throughput 114.04K wps\n",
      "Begin Testing...\n",
      "[Batch 100/313] elapsed 12.21 s\n",
      "[Batch 200/313] elapsed 10.17 s\n",
      "[Batch 300/313] elapsed 10.87 s\n",
      "[Epoch 4] train avg loss 0.005298, test acc 0.82, test avg loss 0.488239, throughput 125.39K wps\n",
      "[Epoch 5 Batch 100/265] elapsed 18.25 s, avg loss 0.006496, throughput 97.69K wps\n",
      "[Epoch 5 Batch 200/265] elapsed 18.32 s, avg loss 0.005334, throughput 118.29K wps\n",
      "Begin Testing...\n",
      "[Batch 100/313] elapsed 9.88 s\n",
      "[Batch 200/313] elapsed 10.13 s\n",
      "[Batch 300/313] elapsed 11.12 s\n",
      "[Epoch 5] train avg loss 0.004926, test acc 0.83, test avg loss 0.451379, throughput 127.08K wps\n",
      "[Epoch 6 Batch 100/265] elapsed 18.67 s, avg loss 0.004079, throughput 149.80K wps\n",
      "[Epoch 6 Batch 200/265] elapsed 19.72 s, avg loss 0.005063, throughput 109.99K wps\n",
      "Begin Testing...\n",
      "[Batch 100/313] elapsed 10.34 s\n",
      "[Batch 200/313] elapsed 9.86 s\n",
      "[Batch 300/313] elapsed 11.09 s\n",
      "[Epoch 6] train avg loss 0.004720, test acc 0.83, test avg loss 0.391468, throughput 122.64K wps\n",
      "[Epoch 7 Batch 100/265] elapsed 17.97 s, avg loss 0.006191, throughput 99.83K wps\n",
      "[Epoch 7 Batch 200/265] elapsed 15.83 s, avg loss 0.004907, throughput 130.26K wps\n",
      "Begin Testing...\n",
      "[Batch 100/313] elapsed 11.16 s\n",
      "[Batch 200/313] elapsed 12.42 s\n",
      "[Batch 300/313] elapsed 10.98 s\n",
      "[Epoch 7] train avg loss 0.004449, test acc 0.85, test avg loss 0.420773, throughput 132.64K wps\n",
      "[Epoch 8 Batch 100/265] elapsed 17.19 s, avg loss 0.003661, throughput 157.41K wps\n",
      "[Epoch 8 Batch 200/265] elapsed 17.63 s, avg loss 0.004528, throughput 128.33K wps\n",
      "Begin Testing...\n",
      "[Batch 100/313] elapsed 8.75 s\n",
      "[Batch 200/313] elapsed 9.89 s\n",
      "[Batch 300/313] elapsed 10.56 s\n",
      "[Epoch 8] train avg loss 0.004322, test acc 0.85, test avg loss 0.383867, throughput 134.57K wps\n",
      "[Epoch 9 Batch 100/265] elapsed 17.27 s, avg loss 0.003786, throughput 157.32K wps\n",
      "[Epoch 9 Batch 200/265] elapsed 18.65 s, avg loss 0.005616, throughput 94.20K wps\n",
      "Begin Testing...\n",
      "[Batch 100/313] elapsed 10.11 s\n",
      "[Batch 200/313] elapsed 11.86 s\n",
      "[Batch 300/313] elapsed 10.07 s\n",
      "[Epoch 9] train avg loss 0.004162, test acc 0.84, test avg loss 0.417968, throughput 126.72K wps\n",
      "[Epoch 10 Batch 100/265] elapsed 19.59 s, avg loss 0.004116, throughput 113.78K wps\n",
      "[Epoch 10 Batch 200/265] elapsed 18.78 s, avg loss 0.005692, throughput 93.60K wps\n",
      "Begin Testing...\n",
      "[Batch 100/313] elapsed 11.35 s\n",
      "[Batch 200/313] elapsed 10.97 s\n",
      "[Batch 300/313] elapsed 11.89 s\n",
      "[Epoch 10] train avg loss 0.004009, test acc 0.86, test avg loss 0.432112, throughput 121.24K wps\n",
      "[Epoch 11 Batch 100/265] elapsed 16.40 s, avg loss 0.005836, throughput 101.86K wps\n",
      "[Epoch 11 Batch 200/265] elapsed 16.61 s, avg loss 0.003519, throughput 155.18K wps\n",
      "Begin Testing...\n",
      "[Batch 100/313] elapsed 9.46 s\n",
      "[Batch 200/313] elapsed 10.66 s\n",
      "[Batch 300/313] elapsed 10.56 s\n",
      "[Epoch 11] train avg loss 0.003825, test acc 0.84, test avg loss 0.462627, throughput 131.07K wps\n",
      "[Epoch 12 Batch 100/265] elapsed 18.23 s, avg loss 0.003433, throughput 148.15K wps\n",
      "[Epoch 12 Batch 200/265] elapsed 17.92 s, avg loss 0.004385, throughput 107.03K wps\n",
      "Begin Testing...\n",
      "[Batch 100/313] elapsed 11.30 s\n",
      "[Batch 200/313] elapsed 10.55 s\n",
      "[Batch 300/313] elapsed 10.46 s\n",
      "[Epoch 12] train avg loss 0.003728, test acc 0.85, test avg loss 0.411282, throughput 128.18K wps\n",
      "[Epoch 13 Batch 100/265] elapsed 18.04 s, avg loss 0.005127, throughput 95.05K wps\n",
      "[Epoch 13 Batch 200/265] elapsed 16.83 s, avg loss 0.002995, throughput 166.97K wps\n",
      "Begin Testing...\n",
      "[Batch 100/313] elapsed 11.10 s\n",
      "[Batch 200/313] elapsed 12.27 s\n",
      "[Batch 300/313] elapsed 11.20 s\n",
      "[Epoch 13] train avg loss 0.003574, test acc 0.85, test avg loss 0.382595, throughput 126.08K wps\n",
      "[Epoch 14 Batch 100/265] elapsed 18.69 s, avg loss 0.004186, throughput 105.73K wps\n",
      "[Epoch 14 Batch 200/265] elapsed 18.42 s, avg loss 0.002750, throughput 156.47K wps\n",
      "Begin Testing...\n",
      "[Batch 100/313] elapsed 10.92 s\n",
      "[Batch 200/313] elapsed 11.73 s\n",
      "[Batch 300/313] elapsed 11.69 s\n",
      "[Epoch 14] train avg loss 0.003494, test acc 0.87, test avg loss 0.393573, throughput 127.62K wps\n"
     ]
    }
   ],
   "source": [
    "train(net, context, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0.9164194]]\n",
       "<NDArray 1x1 @gpu(0)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(mx.nd.reshape(mx.nd.array(vocab[['This', 'movie', 'is', 'amazing']], ctx=context[0]), shape=(-1, 1)),\n",
    "    mx.nd.array([4], ctx=context[0])).sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, we have built a SA model using gluonnlp. It is:\n",
    "\n",
    "1) easy to use.\n",
    "\n",
    "2) simple to customize.\n",
    "\n",
    "3) fast to build the NLP prototype.\n",
    "\n",
    "Gluonnlp documentation is here: http://gluon-nlp.s3-accelerate.dualstack.amazonaws.com/index.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
