{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis (SA) with pretrained Language Model (LM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "In this notebook, we are going to build a sentiment analysis model based on the pretrained language model. We are focusing on the best usability to support traditional nlp tasks in a simple fashion. The building process is simple three steps. Let us get started now.\n",
    "\n",
    "We use movie reviews from the Large Movie Review Dataset, as known as the IMDB dataset. In this task, given a moview, the model attemps to predict its sentiment, which can be positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation and settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load mxnet and gluonnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import nd, gluon, autograd\n",
    "\n",
    "import gluonnlp as nlp\n",
    "\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "mx.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is based on a standard LSTM model. We use a hidden size of 200. We use bucketing for speeding up the processing of variable-length sequences. To enable multi-gpu training, we can simply change num_gpus to some value larger than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0\n",
    "language_model_name = 'standard_lstm_lm_200'\n",
    "pretrained = True\n",
    "num_gpus = 1\n",
    "learning_rate = 0.005 * num_gpus\n",
    "batch_size = 16 * num_gpus\n",
    "bucket_num = 10\n",
    "bucket_ratio = 0.2\n",
    "epochs = 1\n",
    "grad_clip = None\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = [mx.gpu(i) for i in range(num_gpus)] if num_gpus else [mx.cpu()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis model with pre-trained language model encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model architecture is based on pretrained LM:\n",
    "\n",
    "![sa-model](samodel-v3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is composed of a two-layer LSTM followed by an average pooling and a sigmoid output layer as illustrated in the Figure above. From the embedding layer, the new representations will be passed to LSTM cells. These will include information about the sequence of words in the data. Thus, given an input sequence, the memory cells in the LSTM layer will produce a representation sequence. This representation sequence is then averaged over all timesteps resulting in representation h. Finally, this representation is fed to a sigmoid output layer. We’re using the sigmoid  because we’re trying to predict if this text has positive or negative sentiment, and a sigmoid activation function allows the model to compute the posterior probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentNet(\n",
      "  (encoder): LSTM(200 -> 200.0, TNC, num_layers=2)\n",
      "  (embedding): HybridSequential(\n",
      "    (0): Embedding(33278 -> 200, float32)\n",
      "  )\n",
      "  (out_layer): HybridSequential(\n",
      "    (0): Dropout(p = 0, axes=())\n",
      "    (1): Dense(None -> 1, linear)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SentimentNet(gluon.Block):\n",
    "    def __init__(self, embedding_block, encoder_block, dropout, \n",
    "                 prefix=None, params=None):\n",
    "        super(SentimentNet, self).__init__(prefix=prefix, params=params)\n",
    "        with self.name_scope():\n",
    "            self.embedding = embedding_block\n",
    "            self.encoder = encoder_block\n",
    "            self.out_layer = gluon.nn.HybridSequential()\n",
    "            with self.out_layer.name_scope():\n",
    "                self.out_layer.add(gluon.nn.Dropout(dropout))\n",
    "                self.out_layer.add(gluon.nn.Dense(1, flatten=False))\n",
    "\n",
    "    def forward(self, data, valid_length):\n",
    "        encoded = self.encoder(nd.Dropout(self.embedding(data), \n",
    "                                          0.2, axes=(0,)))  # Shape(T, N, C)\n",
    "        # Zero out the values with position exceeding the valid length. \n",
    "        masked_encoded = nd.SequenceMask(encoded,\n",
    "                                         sequence_length=valid_length,\n",
    "                                         use_sequence_length=True)\n",
    "        agg_state = nd.broadcast_div(nd.sum(masked_encoded, axis=0),\n",
    "                                     nd.expand_dims(valid_length, axis=1))\n",
    "        out = self.out_layer(agg_state)\n",
    "        return out\n",
    "\n",
    "\n",
    "lm_model, vocab = nlp.model.get_model(name=language_model_name,\n",
    "                                      dataset_name='wikitext-2',\n",
    "                                      pretrained=pretrained,\n",
    "                                      ctx=context,\n",
    "                                      dropout=dropout)\n",
    "net = SentimentNet(embedding_block=lm_model.embedding, \n",
    "                   encoder_block=lm_model.encoder,\n",
    "                   dropout=dropout)\n",
    "net.out_layer.initialize(mx.init.Xavier(), ctx=context)\n",
    "net.hybridize()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, we first acquire a pretrained model on Wikitext-2 dataset using nlp.model.get_model. We then construct a SentimentNet object, which takes as input the embedding layer and encoder of the pretrained model. \n",
    "\n",
    "As we employ the pretrained embedding layer and encoder, we only need to initialize the output layer using net.out_layer.initialize(mx.init.Xavier(), ctx=context)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sentiment analysis dataset -- IMDB reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize using spaCy...\n",
      "Done! Tokenizing Time=13.36s, #Sentences=25000\n",
      "Done! Tokenizing Time=12.95s, #Sentences=25000\n"
     ]
    }
   ],
   "source": [
    "# train_dataset and test_dataset are both SimpleDataset objects, \n",
    "# which is a wrapper for lists and arrays.\n",
    "train_dataset, test_dataset = [nlp.data.IMDB(segment=segment) \n",
    "                               for segment in ('train', 'test')]\n",
    "print(\"Tokenize using spaCy...\")\n",
    "# tokenizer takes as input a string and outputs a list of tokens.\n",
    "tokenizer = nlp.data.SpacyTokenizer('en')\n",
    "# length_clip takes as input a list and outputs a list with maximum length 500.\n",
    "length_clip = nlp.data.ClipSequence(500)\n",
    "\n",
    "def preprocess(x):\n",
    "    data, label = x\n",
    "    # In the labeled train/test sets, a negative review has a score <= 4 \n",
    "    # out of 10, and a positive review has a score >= 7 out of 10. Thus \n",
    "    # reviews with more neutral ratings are not included in the train/test \n",
    "    # sets. We labeled a negative review whose score <= 4 as 0, and a \n",
    "    # positive review whose score >= 7 as 1. As the neural ratings are not \n",
    "    # included in the datasets, we can simply use 5 as our threshold. \n",
    "    label = int(label > 5)\n",
    "    # A token index or a list of token indices is \n",
    "    # returned according to the vocabulary.\n",
    "    data = vocab[length_clip(tokenizer(data))]\n",
    "    return data, label, float(len(data))\n",
    "\n",
    "def get_length(x):\n",
    "    return x[2]\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    start = time.time()\n",
    "    with mp.Pool() as pool:\n",
    "        # Each sample is processed in an asynchronous manner.\n",
    "        dataset = gluon.data.SimpleDataset(pool.map(preprocess, dataset))\n",
    "        lengths = gluon.data.SimpleDataset(pool.map(get_length, dataset))\n",
    "    end = time.time()\n",
    "    print('Done! Tokenizing Time={:.2f}s, #Sentences={}'\n",
    "          .format(end - start, len(dataset)))\n",
    "    return dataset, lengths\n",
    "\n",
    "train_dataset, train_data_lengths = preprocess_dataset(train_dataset)\n",
    "test_dataset, test_data_lengths = preprocess_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation using loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, dataloader, context):\n",
    "    loss = gluon.loss.SigmoidBCELoss()\n",
    "    total_L = 0.0\n",
    "    total_sample_num = 0\n",
    "    total_correct_num = 0\n",
    "    start_log_interval_time = time.time()\n",
    "    print('Begin Testing...')\n",
    "    for i, (data, label, valid_length) in enumerate(dataloader):\n",
    "        data = mx.nd.transpose(data.as_in_context(context))\n",
    "        valid_length = valid_length.as_in_context(context).astype(np.float32)\n",
    "        label = label.as_in_context(context)\n",
    "        output = net(data, valid_length)\n",
    "        L = loss(output, label)\n",
    "        pred = (output > 0.5).reshape(-1)\n",
    "        total_L += L.sum().asscalar()\n",
    "        total_sample_num += label.shape[0]\n",
    "        total_correct_num += (pred == label).sum().asscalar()\n",
    "        if (i + 1) % log_interval == 0:\n",
    "            print('[Batch {}/{}] elapsed {:.2f} s'.format(\n",
    "                i + 1, len(dataloader), \n",
    "                time.time() - start_log_interval_time))\n",
    "            start_log_interval_time = time.time()\n",
    "    avg_L = total_L / float(total_sample_num)\n",
    "    acc = total_correct_num / float(total_sample_num)\n",
    "    return avg_L, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, we use FixedBucketSampler, which assigns each data sample to a fixed bucket based on its length. The bucket keys are either given or generated from the input sequence lengths and number of the buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, context, epochs):\n",
    "    trainer = gluon.Trainer(net.collect_params(), \n",
    "                            'ftml', \n",
    "                            {'learning_rate': learning_rate})\n",
    "    loss = gluon.loss.SigmoidBCELoss()\n",
    "\n",
    "    # Construct the DataLoader\n",
    "    # Pad data, stack label and lengths\n",
    "    batchify_fn = nlp.data.batchify.Tuple(nlp.data.batchify.Pad(axis=0), \n",
    "                                          nlp.data.batchify.Stack(),\n",
    "                                          nlp.data.batchify.Stack())\n",
    "    batch_sampler = nlp.data.sampler.FixedBucketSampler(train_data_lengths,\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        num_buckets=bucket_num,\n",
    "                                                        ratio=bucket_ratio,\n",
    "                                                        shuffle=True)\n",
    "    print(batch_sampler.stats())\n",
    "    train_dataloader = gluon.data.DataLoader(dataset=train_dataset,\n",
    "                                             batch_sampler=batch_sampler,\n",
    "                                             batchify_fn=batchify_fn)\n",
    "    test_dataloader = gluon.data.DataLoader(dataset=test_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False,\n",
    "                                            batchify_fn=batchify_fn)\n",
    "    parameters = net.collect_params().values()\n",
    "\n",
    "    # Training/Testing\n",
    "    for epoch in range(epochs):\n",
    "        # Epoch training stats\n",
    "        start_epoch_time = time.time()\n",
    "        epoch_L = 0.0\n",
    "        epoch_sent_num = 0\n",
    "        epoch_wc = 0\n",
    "        # Log interval training stats\n",
    "        start_log_interval_time = time.time()\n",
    "        log_interval_wc = 0\n",
    "        log_interval_sent_num = 0\n",
    "        log_interval_L = 0.0\n",
    "\n",
    "        for i, (data, label, length) in enumerate(train_dataloader):\n",
    "            if data.shape[0] > len(context):\n",
    "                # Multi-gpu training. \n",
    "                data_list, label_list, length_list \\\n",
    "                = [gluon.utils.split_and_load(x, \n",
    "                                              context, \n",
    "                                              batch_axis=0, \n",
    "                                              even_split=False) \n",
    "                   for x in [data, label, length]]\n",
    "            else:\n",
    "                data_list = [data.as_in_context(context[0])]\n",
    "                label_list = [label.as_in_context(context[0])]\n",
    "                length_list = [length.as_in_context(context[0])]\n",
    "            L = 0\n",
    "            wc = length.sum().asscalar()\n",
    "            log_interval_wc += wc\n",
    "            epoch_wc += wc\n",
    "            log_interval_sent_num += data.shape[1]\n",
    "            epoch_sent_num += data.shape[1]\n",
    "            for data, label, valid_length in zip(data_list, label_list, length_list):\n",
    "                valid_length = valid_length\n",
    "                with autograd.record():\n",
    "                    output = net(data.T, valid_length)\n",
    "                    L = L + loss(output, label).mean().as_in_context(context[0])\n",
    "            L.backward()\n",
    "            # Clip gradient\n",
    "            if grad_clip:\n",
    "                gluon.utils.clip_global_norm([p.grad(x.context) \n",
    "                                              for p in parameters for x in data_list],\n",
    "                                             grad_clip)\n",
    "            # Update parameter\n",
    "            trainer.step(1)\n",
    "            log_interval_L += L.asscalar()\n",
    "            epoch_L += L.asscalar()\n",
    "            if (i + 1) % log_interval == 0:\n",
    "                print('[Epoch {} Batch {}/{}] elapsed {:.2f} s, \\\n",
    "                      avg loss {:.6f}, throughput {:.2f}K wps'.format(\n",
    "                    epoch, i + 1, len(train_dataloader), \n",
    "                    time.time() - start_log_interval_time,\n",
    "                    log_interval_L / log_interval_sent_num,\n",
    "                    log_interval_wc / 1000 / (time.time() - start_log_interval_time)))\n",
    "                # Clear log interval training stats\n",
    "                start_log_interval_time = time.time()\n",
    "                log_interval_wc = 0\n",
    "                log_interval_sent_num = 0\n",
    "                log_interval_L = 0\n",
    "        end_epoch_time = time.time()\n",
    "        test_avg_L, test_acc = evaluate(net, test_dataloader, context[0])\n",
    "        print('[Epoch {}] train avg loss {:.6f}, test acc {:.2f}, \\\n",
    "        test avg loss {:.6f}, throughput {:.2f}K wps'.format(\n",
    "            epoch, epoch_L / epoch_sent_num,\n",
    "            test_acc, test_avg_L, epoch_wc / 1000 / \n",
    "            (end_epoch_time - start_epoch_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FixedBucketSampler:\n",
      "  sample_num=25000, batch_num=1548\n",
      "  key=[68, 116, 164, 212, 260, 308, 356, 404, 452, 500]\n",
      "  cnt=[981, 1958, 5686, 4614, 2813, 2000, 1411, 1129, 844, 3564]\n",
      "  batch_size=[23, 16, 16, 16, 16, 16, 16, 16, 16, 16]\n",
      "[Epoch 0 Batch 100/1548] elapsed 4.21 s,                       avg loss 0.002488, throughput 94.89K wps\n",
      "[Epoch 0 Batch 200/1548] elapsed 4.17 s,                       avg loss 0.002098, throughput 99.37K wps\n",
      "[Epoch 0 Batch 300/1548] elapsed 4.16 s,                       avg loss 0.002196, throughput 86.69K wps\n",
      "[Epoch 0 Batch 400/1548] elapsed 4.32 s,                       avg loss 0.001733, throughput 93.43K wps\n",
      "[Epoch 0 Batch 500/1548] elapsed 4.23 s,                       avg loss 0.001605, throughput 98.33K wps\n",
      "[Epoch 0 Batch 600/1548] elapsed 4.35 s,                       avg loss 0.001525, throughput 95.50K wps\n",
      "[Epoch 0 Batch 700/1548] elapsed 4.24 s,                       avg loss 0.001423, throughput 101.45K wps\n",
      "[Epoch 0 Batch 800/1548] elapsed 4.16 s,                       avg loss 0.001371, throughput 103.64K wps\n",
      "[Epoch 0 Batch 900/1548] elapsed 4.24 s,                       avg loss 0.001391, throughput 97.95K wps\n",
      "[Epoch 0 Batch 1000/1548] elapsed 4.39 s,                       avg loss 0.001463, throughput 81.96K wps\n",
      "[Epoch 0 Batch 1100/1548] elapsed 4.26 s,                       avg loss 0.001424, throughput 88.20K wps\n",
      "[Epoch 0 Batch 1200/1548] elapsed 4.10 s,                       avg loss 0.001319, throughput 94.00K wps\n",
      "[Epoch 0 Batch 1300/1548] elapsed 4.40 s,                       avg loss 0.001346, throughput 84.93K wps\n",
      "[Epoch 0 Batch 1400/1548] elapsed 4.11 s,                       avg loss 0.001259, throughput 94.36K wps\n",
      "[Epoch 0 Batch 1500/1548] elapsed 4.26 s,                       avg loss 0.001223, throughput 93.39K wps\n",
      "Begin Testing...\n",
      "[Batch 100/1563] elapsed 4.36 s\n",
      "[Batch 200/1563] elapsed 4.21 s\n",
      "[Batch 300/1563] elapsed 4.30 s\n",
      "[Batch 400/1563] elapsed 4.37 s\n",
      "[Batch 500/1563] elapsed 4.30 s\n",
      "[Batch 600/1563] elapsed 4.72 s\n",
      "[Batch 700/1563] elapsed 4.80 s\n",
      "[Batch 800/1563] elapsed 4.80 s\n",
      "[Batch 900/1563] elapsed 5.61 s\n",
      "[Batch 1000/1563] elapsed 4.23 s\n",
      "[Batch 1100/1563] elapsed 4.15 s\n",
      "[Batch 1200/1563] elapsed 4.31 s\n",
      "[Batch 1300/1563] elapsed 4.12 s\n",
      "[Batch 1400/1563] elapsed 4.35 s\n",
      "[Batch 1500/1563] elapsed 4.16 s\n",
      "[Epoch 0] train avg loss 0.001580, test acc 0.86,         test avg loss 0.314616, throughput 93.80K wps\n"
     ]
    }
   ],
   "source": [
    "train(net, context, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0.7124313]]\n",
       "<NDArray 1x1 @gpu(0)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(mx.nd.reshape(mx.nd.array(vocab[['This', 'movie', 'is', 'amazing']], \n",
    "                              ctx=context[0]), shape=(-1, 1)),\n",
    "    mx.nd.array([4], ctx=context[0])).sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, we have built a SA model using gluonnlp. It is:\n",
    "\n",
    "1) easy to use.\n",
    "\n",
    "2) simple to customize.\n",
    "\n",
    "3) fast to build the NLP prototype.\n",
    "\n",
    "Gluonnlp documentation is here: http://gluon-nlp.s3-accelerate.dualstack.amazonaws.com/index.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
