Language Model
==============
https://gluon-nlp.mxnet.io/model_zoo/language_model/index.html



Dataset: Wikitext-2

+-------------------------------------------------------------+----------------------------------+-----------------+
| Model                                                       | Pre-trained Model                | Test Perplexity |
+=============================================================+==================================+=================+
| Standard 2-layer LSTM language model (#hidden units = 200)  | standard_lstm_lm_200_wikitext-2  | 101.64          |
+-------------------------------------------------------------+----------------------------------+-----------------+
| Standard 2-layer LSTM language model (#hidden units = 650)  | standard_lstm_lm_650_wikitext-2  | 86.91           |
+-------------------------------------------------------------+----------------------------------+-----------------+
| Standard 2-layer LSTM language model (#hidden units = 1500) | standard_lstm_lm_1500_wikitext-2 | 82.29           |
+-------------------------------------------------------------+----------------------------------+-----------------+
| 3-layer AWD-LSTM language model (#hidden units = 600)       | awd_lstm_lm_600_wikitext-2       | 80.67           |
+-------------------------------------------------------------+----------------------------------+-----------------+
| 3-layer AWD-LSTM language model (#hidden units = 1150)      | awd_lstm_lm_1150_wikitext-2      | 65.62           |
+-------------------------------------------------------------+----------------------------------+-----------------+

