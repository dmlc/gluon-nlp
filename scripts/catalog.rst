Model Catalog
=============




Language Model
--------------
`Language Model Model Zoo Index <./language_model/index.html>`_

Word Language Model
~~~~~~~~~~~~~~~~~~~

Dataset: Wikitext-2

+---------------------------------------+-----------------+-----------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------+
| Pre-trained Model                     | Test Perplexity |Training Command                                                                                                             | log                                                                                                                         |
+=======================================+=================+=============================================================================================================================+=============================================================================================================================+
| standard_lstm_lm_200_wikitext-2  [1]_ | 101.64          |`command <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/standard_lstm_lm_200_wikitext-2.sh>`__   |  `log <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/standard_lstm_lm_200_wikitext-2.log>`__    |
+---------------------------------------+-----------------+-----------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------+
| standard_lstm_lm_650_wikitext-2  [1]_ | 86.91           |`command <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/standard_lstm_lm_650_wikitext-2.sh>`__   |  `log <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/standard_lstm_lm_650_wikitext-2.log>`__    |
+---------------------------------------+-----------------+-----------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------+
| standard_lstm_lm_1500_wikitext-2 [1]_ | 82.29           |`command <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/standard_lstm_lm_1500_wikitext-2.sh>`__  |  `log <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/standard_lstm_lm_1500_wikitext-2.log>`__   |
+---------------------------------------+-----------------+-----------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------+
| awd_lstm_lm_600_wikitext-2       [1]_ | 80.67           |`command <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/awd_lstm_lm_600_wikitext-2.sh>`__        |  `log <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/awd_lstm_lm_600_wikitext-2.log>`__         |
+---------------------------------------+-----------------+-----------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------+
| awd_lstm_lm_1150_wikitext-2      [1]_ | 65.62           |`command <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/awd_lstm_lm_1150_wikitext-2.sh>`__       |  `log <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/awd_lstm_lm_1150_wikitext-2.log>`__        |
+---------------------------------------+-----------------+-----------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------+


Cache Language Model
~~~~~~~~~~~~~~~~~~~~

Dataset: Wikitext-2

+---------------------------------------------+-----------------+----------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------+
| Pre-trained Model                           | Test Perplexity |Training Command                                                                                                                  | log                                                                                                                           |
+=============================================+=================+==================================================================================================================================+===============================================================================================================================+
| cache_awd_lstm_lm_1150_wikitext-2      [2]_ | 51.46           |`command <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/cache_awd_lstm_lm_1150_wikitext-2.sh>`__      |`log <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/cache_awd_lstm_lm_1150_wikitext-2.log>`__      |
+---------------------------------------------+-----------------+----------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------+
| cache_awd_lstm_lm_600_wikitext-2       [2]_ | 62.19           |`command <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/cache_awd_lstm_lm_600_wikitext-2.sh>`__       |`log <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/cache_awd_lstm_lm_600_wikitext-2.log>`__       |
+---------------------------------------------+-----------------+----------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------+
| cache_standard_lstm_lm_1500_wikitext-2 [2]_ | 62.79           |`command <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/cache_standard_lstm_lm_1500_wikitext-2.sh>`__ |`log <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/cache_standard_lstm_lm_1500_wikitext-2.log>`__ |
+---------------------------------------------+-----------------+----------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------+
| cache_standard_lstm_lm_650_wikitext-2  [2]_ | 65.85           |`command <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/cache_standard_lstm_lm_650_wikitext-2.sh>`__  |`log <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/cache_standard_lstm_lm_650_wikitext-2.log>`__  |
+---------------------------------------------+-----------------+----------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------+
| cache_standard_lstm_lm_200_wikitext-2  [2]_ | 73.74           |`command <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/cache_standard_lstm_lm_200_wikitext-2.sh>`__  |`log <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/cache_standard_lstm_lm_200_wikitext-2.log>`__  |
+---------------------------------------------+-----------------+----------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------+



Large Scale Word Language Model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Dataset: Googleâ€™s 1 billion words dataset

+-------------------------+-----------------+-------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+
| Pre-trained Model       | Test Perplexity |Training Command                                                                                                   | log                                                                                                            |
+=========================+=================+===================================================================================================================+================================================================================================================+
| LSTM-2048-512      [3]_ | 43.62           |`command <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/big_rnn_lm_2048_512_gbw.sh>`__ |`log <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/big_rnn_lm_2048_512_gbw.log>`__ |
+-------------------------+-----------------+-------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+


Machine Translation
-------------------
`Machine Translation Model Zoo Index <./machine_translation/index.html>`_


Google Neural Machine Translation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Dataset: IWLST2015-en-vi

+---------------------+-----------+-------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+
| Pre-trained Model   | Test BLEU |Training Command                                                                                                   | log                                                                                                            |
+=====================+===========+===================================================================================================================+================================================================================================================+
| GNMT                | 26.2      |                                                                                                                   |                                                                                                                |
+---------------------+-----------+-------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+


Transformers
~~~~~~~~~~~~

Dataset: WMT14-en-de
Requisite: sacremoses package: pip install scaremoses --user

+------------------------------+-----------+-------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+
| Pre-trained Model            | Test BLEU |Training Command                                                                                                   | log                                                                                                            |
+==============================+===========+===================================================================================================================+================================================================================================================+
| transformer_en_de_512_WMT2014| 27.65     |                                                                                                                   |`log <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/nmt/transformer_en_de_u512.log>`__             |
+------------------------------+-----------+-------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+


.. [1] Merity, S., et al.  \
       "`Regularizing and optimizing LSTM language models <https://openreview.net/pdf?id=SyyGPP0TZ>`_". \
       ICLR 2018
.. [2] Grave, E., et al. \
       "`Improving neural language models with a continuous cache <https://openreview.net/pdf?id=B184E5qee>`_".\
       ICLR 2017
.. [3] Jozefowicz, Rafal, et al. \
       "`Exploring the limits of language modeling <https://arxiv.org/abs/1602.02410>`_".\
       arXiv preprint arXiv:1602.02410 (2016).
