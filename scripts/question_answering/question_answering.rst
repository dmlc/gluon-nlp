Question Answer
-------------------

QANet Model
~~~~~~~~~~~~~~~~~~~~

Reference: A. Yu, D. Dohan et al. "`QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension <https://arxiv.org/pdf/1804.09541.pdf>`_". ICLR 2018

The dataset used for training and testing the models is `SQuAD <https://arxiv.org/pdf/1606.05250.pdf>`_ v1.0 dataset.

The key features used to reproduce the results for pre-trained models are listed in the following tables.

+--------------------------------------+---------------+---------------+--------------------------------------------+--------------------------------------------+--------------+
|                 Model                | QANet_MXNet_1 | QANet_MXNet_2 |                 QANet_tf_1                 |                 QANet_tf_2                 | origin_paper |
+--------------------------------------+---------------+---------------+--------------------------------------------+--------------------------------------------+--------------+
|     Embedding     |     Word size    |      300      |      300      |                     300                    |                     300                    |      300     |
+                   +------------------+---------------+---------------+--------------------------------------------+--------------------------------------------+--------------+
|                   |     Char size    |      200      |      200      |                     200                    |                     200                    |      200     |
+-------------------+------------------+---------------+---------------+--------------------------------------------+--------------------------------------------+--------------+
| Embedding encoder |   Conv channels  |      128      |      128      |                     128                    |                     128                    |      128     |
+                   +------------------+---------------+---------------+--------------------------------------------+--------------------------------------------+--------------+
|                   | Conv kernel size |       7       |       7       |                      7                     |                      7                     |       7      |
+                   +------------------+---------------+---------------+--------------------------------------------+--------------------------------------------+--------------+
|                   |    Conv layers   |       4       |       4       |                      4                     |                      4                     |       4      |
+                   +------------------+---------------+---------------+--------------------------------------------+--------------------------------------------+--------------+
|                   |  Attention heads |       1       |       2       |                      1                     |                      8                     |       8      |
+                   +------------------+---------------+---------------+--------------------------------------------+--------------------------------------------+--------------+
|                   |      Blocks      |       1       |       1       |                      1                     |                      1                     |       1      |
+-------------------+------------------+---------------+---------------+--------------------------------------------+--------------------------------------------+--------------+
|   Model encoder   |   Conv channels  |      128      |      128      |                     128                    |                     128                    |      128     |
+                   +------------------+---------------+---------------+--------------------------------------------+--------------------------------------------+--------------+
|                   |    Conv layers   |       2       |       2       |                      2                     |                      2                     |       2      |
+                   +------------------+---------------+---------------+--------------------------------------------+--------------------------------------------+--------------+
|                   | Conv kernel size |       5       |       5       |                      5                     |                      5                     |       5      |
+                   +------------------+---------------+---------------+--------------------------------------------+--------------------------------------------+--------------+
|                   |  Attention heads |       1       |       2       |                      1                     |                      8                     |       8      |
+                   +------------------+---------------+---------------+--------------------------------------------+--------------------------------------------+--------------+
|                   |      Blocks      |       7       |       7       |                      7                     |                      7                     |       7      |
+                   +------------------+---------------+---------------+--------------------------------------------+--------------------------------------------+--------------+
|                   |    Conv layers   |       2       |       2       |                      2                     |                      2                     |       2      |
+-------------------+------------------+---------------+---------------+--------------------------------------------+--------------------------------------------+--------------+
|                   |        F1        |      79.0     |      79.5     |                    79.8                    |                    80.1                    |     82.7     |
+                   +------------------+---------------+---------------+--------------------------------------------+--------------------------------------------+--------------+
|                   |        EM        |      69.8     |      70.2     |                    70.7                    |                    70.8                    |     73.6     |
+                   +------------------+---------------+---------------+--------------------------------------------+--------------------------------------------+--------------+
|                   |      Command     |      [1]      |      [2]      | `[3] <https://github.com/NLPLearn/QANet>`_ | `[4] <https://github.com/NLPLearn/QANet>`_ |       /      |
+-------------------+------------------+---------------+---------------+--------------------------------------------+--------------------------------------------+--------------+


Usage
~~~~~~~~~~~~~~~~~~~~
To preprocess the data, you need to download `dataset <https://rajpurkar.github.io/SQuAD-explorer/>`_ and `glove <https://nlp.stanford.edu/projects/glove/>`_ pretrained word vector.
And put all three files in the same directory with data_process.py and run the following code: 
.. code-block:: console

    $ python data_process.py

[1] QANet_MXNet_1 (Val set F1= 79.0 EM= 69.8)

Remember to modify the hyperparameter in config.py file to the value listed in the above table.

.. code-block:: console

   $ python train.py

[2] QANet_MXNet_2 (Val set F1= 79.5 EM= 70.2)

Remember to modify the hyperparameter in config.py file to the value listed in the above table.

.. code-block:: console

   $ python train.py
