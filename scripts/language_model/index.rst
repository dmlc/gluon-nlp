Language Model
--------------

:download:`Download scripts </model_zoo/language_model.zip>`

Word Language Model
~~~~~~~~~~~~~~~~~~~~

Reference: Merity, S., et al. "`Regularizing and optimizing LSTM language models <https://openreview.net/pdf?id=SyyGPP0TZ>`_". ICLR 2018


The key features used to reproduce the results for pre-trained models are listed in the following tables.

.. editing URL for the following table: https://bit.ly/2PHSHvc

The dataset used for training the models is wikitext-2.

+---------------+----------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------+
| Model         | awd_lstm_lm_1150_wikitext-2                                                                                                | awd_lstm_lm_600_wikitext-2                                                                                                | standard_lstm_lm_1500_wikitext-2                                                                                                | standard_lstm_lm_650_wikitext-2                                                                                                | standard_lstm_lm_200_wikitext-2                                                                                                |
+===============+============================================================================================================================+===========================================================================================================================+=================================================================================================================================+================================================================================================================================+================================================================================================================================+
| Val PPL       | 68.71                                                                                                                      | 84.89                                                                                                                     | 86.51                                                                                                                           | 90.96                                                                                                                          | 107.59                                                                                                                         |
+---------------+----------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------+
| Test PPL      | 65.62                                                                                                                      | 80.67                                                                                                                     | 82.29                                                                                                                           | 86.91                                                                                                                          | 101.64                                                                                                                         |
+---------------+----------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------+
| Command       | `command <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/awd_lstm_lm_1150_wikitext-2.sh>`__     | `command <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/awd_lstm_lm_600_wikitext-2.sh>`__     | `command <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/standard_lstm_lm_1500_wikitext-2.sh>`__     | `command <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/standard_lstm_lm_650_wikitext-2.sh>`__     | `command <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/standard_lstm_lm_200_wikitext-2.sh>`__     |
+---------------+----------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------+
| Training logs | `log <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/awd_lstm_lm_1150_wikitext-2.log>`__        | `log <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/awd_lstm_lm_600_wikitext-2.log>`__        | `log <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/standard_lstm_lm_1500_wikitext-2.log>`__        | `log <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/standard_lstm_lm_650_wikitext-2.log>`__        | `log <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/standard_lstm_lm_200_wikitext-2.log>`__        |
+---------------+----------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------+

For all the above model settings, we set Tied = True and NTASGD = True .

Cache Language Model
~~~~~~~~~~~~~~~~~~~~~

Reference: Grave, E., et al. "`Improving neural language models with a continuous cache <https://openreview.net/pdf?id=B184E5qee>`_". ICLR 2017

The key features used to reproduce the results based on the corresponding pre-trained models are listed in the following tables.

.. editing URL for the following table: https://bit.ly/2NkpklU

The dataset used for training the models is wikitext-2.

+---------------------+-----------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+
| Model               | cache_awd_lstm_lm_1150_wikitext-2                                                                                                 | cache_awd_lstm_lm_600_wikitext-2                                                                                                 | cache_standard_lstm_lm_1500_wikitext-2                                                                                                 | cache_standard_lstm_lm_650_wikitext-2                                                                                                 | cache_standard_lstm_lm_200_wikitext-2                                                                                                 |
+=====================+===================================================================================================================================+==================================================================================================================================+========================================================================================================================================+=======================================================================================================================================+=======================================================================================================================================+
| Pre-trained setting | Refer to: awd_lstm_lm_1150_wikitext-2                                                                                             | Refer to: awd_lstm_lm_600_wikitext-2                                                                                             | Refer to: standard_lstm_lm_1500_wikitext-2                                                                                             | Refer to: standard_lstm_lm_650_wikitext-2                                                                                             | Refer to: standard_lstm_lm_200_wikitext-2                                                                                             |
+---------------------+-----------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+
| Val PPL             | 53.41                                                                                                                             | 64.51                                                                                                                            | 65.54                                                                                                                                  | 68.47                                                                                                                                 | 77.51                                                                                                                                 |
+---------------------+-----------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+
| Test PPL            | 51.46                                                                                                                             | 62.19                                                                                                                            | 62.79                                                                                                                                  | 65.85                                                                                                                                 | 73.74                                                                                                                                 |
+---------------------+-----------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+
| Command             | `command <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/cache_awd_lstm_lm_1150_wikitext-2.sh>`__      | `command <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/cache_awd_lstm_lm_600_wikitext-2.sh>`__      | `command <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/cache_standard_lstm_lm_1500_wikitext-2.sh>`__      | `command <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/cache_standard_lstm_lm_650_wikitext-2.sh>`__      | `command <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/cache_standard_lstm_lm_200_wikitext-2.sh>`__      |
+---------------------+-----------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+
| Training logs       | `log <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/cache_awd_lstm_lm_1150_wikitext-2.log>`__         | `log <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/cache_awd_lstm_lm_600_wikitext-2.log>`__         | `log <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/cache_standard_lstm_lm_1500_wikitext-2.log>`__         | `log <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/cache_standard_lstm_lm_650_wikitext-2.log>`__         | `log <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/cache_standard_lstm_lm_200_wikitext-2.log>`__         |
+---------------------+-----------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+

For all the above model settings, we set lambdas = 0.1279, theta = 0.662, window = 2000 and bptt= 2000 .

Large Scale Word Language Model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Reference: Jozefowicz, Rafal, et al. "`Exploring the limits of language modeling <https://arxiv.org/abs/1602.02410>`_". arXiv preprint arXiv:1602.02410 (2016).

The key features used to reproduce the results for pre-trained models are listed in the following tables.

.. editing URL for the following table: https://bit.ly/2w28VXS

The dataset used for training the models is Google's 1 billion words dataset.

+-----------------+------------------------------------------------------------------------------------------------------------------------------+
| Model           | LSTM-2048-512                                                                                                                |
+=================+==============================================================================================================================+
| Test perplexity | 43.62                                                                                                                        |
+-----------------+------------------------------------------------------------------------------------------------------------------------------+
| Command         | `command <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/big_rnn_lm_2048_512_gbw.sh>`__           |
+-----------------+------------------------------------------------------------------------------------------------------------------------------+
| Command         | `command <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/big_rnn_lm_2048_512_gbw-eval.sh>`__      |
+-----------------+------------------------------------------------------------------------------------------------------------------------------+
| Training logs   | `log <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/big_rnn_lm_2048_512_gbw.log>`__              |
+-----------------+------------------------------------------------------------------------------------------------------------------------------+
| Evaluation logs | `log <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/big_rnn_lm_2048_512_gbw-eval.log>`__         |
+-----------------+------------------------------------------------------------------------------------------------------------------------------+


XLNet: Generalized Autoregressive Pretraining for Language Understanding
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Reference: Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R., &
Le, Q. V. "`XLNet: Generalized Autoregressive Pretraining for Language
Understanding. <https://arxiv.org/abs/1906.08237>`_" arXiv preprint
arXiv:1906.08237 (2019).


The following pre-trained XLNet models are available from the **get_model** API:

+-------------------+--------------------------+-----------------------------+
|                   | xlnet_cased_l12_h768_a12 | xlnet_cased_l24_h1024_a16   |
+===================+==========================+=============================+
| 126gb             | ✓                        | ✓                           |
+-------------------+--------------------------+-----------------------------+

where **126gb** refers to the 126 GB large training dataset used by the XLNet
paper authors.

.. code-block:: python

    import gluonnlp as nlp; import mxnet as mx
    from transformer import get_model, XLNetTokenizer
    model, vocab, tokenizer = get_model('xlnet_cased_l12_h768_a12', dataset_name='126gb', use_decoder=True)
    indices = mx.nd.array([vocab.to_indices(tokenizer('Hello world'))])
    token_types = mx.nd.ones_like(indices)
    mems = model.begin_mems(batch_size=1, mem_len=500, context=indices.context)
    output, new_mems = model(indices, token_types, mems)

Sentence Classification
~~~~~~~~~~~~~~~~~~~~~~~

GluonNLP provides the following example script to fine-tune sentence classification with pre-trained
XLNet model.

Results using `xlnet_12_768_12`:

+-----------------+---------------------+-----------------------+--------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+
|Task Name        |Metrics              |Results on Dev Set     |log                                                                                                                                         |command                                                                                                                                                          |
+=================+=====================+=======================+============================================================================================================================================+=================================================================================================================================================================+
| CoLA            |Matthew Corr.        |59.33                  |`log <https://github.com/dmlc/web-data/tree/master/gluonnlp/logs/language_model/xlnet_l12_h768_a12_finetuned_CoLA_mxnet1.6.0rc1.log>`__     |`command <https://github.com/dmlc/web-data/tree/master/gluonnlp/logs/language_model/xlnet_l12_h768_a12_finetuned_CoLA_mxnet1.6.0rc1.sh>`__                       |
+-----------------+---------------------+-----------------------+--------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+
| SST-2           |Accuracy             |94.61                  |`log <https://github.com/dmlc/web-data/tree/master/gluonnlp/logs/language_model/xlnet_l12_h768_a12_finetuned_SST_mxnet1.6.0rc1.log>`__      |`command <https://github.com/dmlc/web-data/tree/master/gluonnlp/logs/language_model/xlnet_l12_h768_a12_finetuned_SST_mxnet1.6.0rc1.sh>`__                        |
+-----------------+---------------------+-----------------------+--------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+
| MRPC            |Accuracy/F1          |89.22/92.20            |`log <https://github.com/dmlc/web-data/tree/master/gluonnlp/logs/language_model/xlnet_l12_h768_a12_finetuned_MRPC_mxnet1.6.0rc1.log>`__     |`command <https://github.com/dmlc/web-data/tree/master/gluonnlp/logs/language_model/xlnet_l12_h768_a12_finetuned_MRPC_mxnet1.6.0rc1.sh>`__                       |
+-----------------+---------------------+-----------------------+--------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+
| STS-B           |Pearson Corr.        |89.34                  |`log <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/xlnet_l12_h768_a12_finetuned_STS-B.log>`__                  |`command <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/xlnet_l12_h768_a12_finetuned_STS-B.sh>`__                                    |
+-----------------+---------------------+-----------------------+--------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+
| QQP             |Accuracy             |91.31                  |`log <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/xlnet_l12_h768_a12_finetuned_QQP.log>`__                    |`command <https://github.com/dmlc/web-data/tree/master/gluonnlp/logs/language_model/xlnet_l12_h768_a12_finetuned_QQP.sh>`__                                      |
+-----------------+---------------------+-----------------------+--------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+
| MNLI            |Accuracy(m/mm)       |87.19/86.45            |`log <https://github.com/dmlc/web-data/tree/master/gluonnlp/logs/language_model/xlnet_l12_h768_a12_finetuned_MNLI_mxnet1.6.0rc1.log>`__     |`command <https://github.com/dmlc/web-data/tree/master/gluonnlp/logs/language_model/xlnet_l12_h768_a12_finetuned_MNLI_mxnet1.6.0rc1.sh>`__                       |
+-----------------+---------------------+-----------------------+--------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+
| QNLI            |Accuracy             |88                     |`log <https://github.com/dmlc/web-data/tree/master/gluonnlp/logs/language_model/xlnet_l12_h768_a12_finetuned_QNLI.log>`__                   |`command <https://github.com/dmlc/web-data/tree/master/gluonnlp/logs/language_model/xlnet_l12_h768_a12_finetuned_QNLI.sh>`__                                     |
+-----------------+---------------------+-----------------------+--------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+
| RTE             |Accuracy             |75.09                  |`log <https://github.com/dmlc/web-data/tree/master/gluonnlp/logs/language_model/xlnet_l12_h768_a12_finetuned_RTE_mxnet1.6.0rc1.log>`__      |`command <https://github.com/dmlc/web-data/tree/master/gluonnlp/logs/language_model/xlnet_l12_h768_a12_finetuned_RTE_mxnet1.6.0rc1.sh>`__                        |
+-----------------+---------------------+-----------------------+--------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+

Results using `xlnet_24_1024_16`:
We followed the hyperparameters reported by the paper authors.

+-----------------+---------------------+-----------------------+--------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+
|Task Name        |Metrics              |Results on Dev Set     |log                                                                                                                                         |command                                                                                                                                                          |
+=================+=====================+=======================+============================================================================================================================================+=================================================================================================================================================================+
| CoLA            |Matthew Corr.        |67                     |`log <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/xlnet_l24_h1024_a16_finetuned_CoLA.log>`__                  |`command <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/xlnet_l24_h1024_a16_finetuned_CoLA.sh>`__                                    |
+-----------------+---------------------+-----------------------+--------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+
| SST-2           |Accuracy             |94                     |`log <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/xlnet_l24_h1024_a16_finetuned_SST.log>`__                   |`command <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/xlnet_l24_h1024_a16_finetuned_SST.sh>`__                                     |
+-----------------+---------------------+-----------------------+--------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+
| MRPC            |Accuracy/F1          |90.2/93                |`log <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/xlnet_l24_h1024_a16_finetuned_MRPC.log>`__                  |`command <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/xlnet_l24_h1024_a16_finetuned_MRPC.sh>`__                                    |
+-----------------+---------------------+-----------------------+--------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+
| STS-B           |Pearson Corr.        |91.37                  |`log <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/xlnet_l24_h1024_a16_finetuned_STS-B.log>`__                 |`command <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/xlnet_l24_h1024_a16_finetuned_STS-B.sh>`__                                   |
+-----------------+---------------------+-----------------------+--------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+
| QQP             |Accuracy             |91.94                  |`log <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/xlnet_l24_h1024_a16_finetuned_QQP.log>`__                   |`command <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/xlnet_l24_h1024_a16_finetuned_QQP.sh>`__                                     |
+-----------------+---------------------+-----------------------+--------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+
| MNLI            |Accuracy(m/mm)       |89.93/89.91            |`log <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/xlnet_l24_h1024_a16_finetuned_MNLI.log>`__                  |`command <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/xlnet_l24_h1024_a16_finetuned_MNLI.sh>`__                                    |
+-----------------+---------------------+-----------------------+--------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+
| RTE             |Accuracy             |84.12                  |`log <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/xlnet_l24_h1024_a16_finetuned_RTE.log>`__                   |`command <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/xlnet_l24_h1024_a16_finetuned_RTE.sh>`__                                     |
+-----------------+---------------------+-----------------------+--------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+

Question Answering on SQuAD
~~~~~~~~~~~~~~~~~~~~~~~~~~~

+-----------+---------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Dataset   | SQuAD 1.1                                                                                                                                               | SQuAD 1.1                                                                                                                                                | SQuAD 2.0                                                                                                                                                                                                                                                                                                        | SQuAD 2.0                                                                                                                                                                                                                                                                                                        |
+===========+=========================================================================================================================================================+==========================================================================================================================================================+==================================================================================================================================================================================================================================================================================================================+==================================================================================================================================================================================================================================================================================================================+
| Model     | xlnet_12_768_12                                                                                                                                         | xlnet_24_1024_16                                                                                                                                         | xlnet_12_768_12                                                                                                                                                                                                                                                                                                  | xlnet_24_1024_16                                                                                                                                                                                                                                                                                                 |
+-----------+---------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| EM / F1   | 85.50 / 91.77                                                                                                                                           | 89.08 / 94.52                                                                                                                                            | 80.47 / 83.22                                                                                                                                                                                                                                                                                                    | 86.08 / 86.69                                                                                                                                                                                                                                                                                                    |
+-----------+---------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Log       | `log <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/xlnet_finetune_squad1.1_base_mx1.6.0rc1.log>`__                         | `log <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/xlnet_finetune_squad1.1_large_mx1.6.0rc1.log>`__                         | `log <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/xlnet_finetune_squad2.0_base_mx1.6.0rc1.log>`__                                                                                                                                                                                  | `log <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/xlnet_finetune_squad2.0_large_mx1.6.0rc1.log>`__                                                                                                                                                                                 |
+-----------+---------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Command   | `command <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/xlnet_finetune_squad1.1_base_mx1.6.0rc1.sh>`__                      | `command <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/xlnet_finetune_squad1.1_large_mx1.6.0rc1.sh>`__                      | `command <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/xlnet_finetune_squad2.0_base_mx1.6.0rc1.sh>`__                                                                                                                                                                               | `command <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/xlnet_finetune_squad2.0_large_mx1.6.0rc1.sh>`__                                                                                                                                                                              |
+-----------+---------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Prediction| `predictions.json <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/xlnet_finetune_squad1.1_base_mx1.6.0rc1_pred.json>`__      | `predictions.json <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/xlnet_finetune_squad1.1_large_mx1.6.0rc1_pred.json>`__      | `predictions.json <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/xlnet_finetune_squad2.0_base_mx1.6.0rc1_pred.json>`__  `null_odds.json <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/xlnet_finetune_squad2.0_base_mx1.6.0rc1_null.json>`__             | `predictions.json <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/xlnet_finetune_squad2.0_large_mx1.6.0rc1_pred.json>`__  `null_odds.json <https://github.com/dmlc/web-data/blob/master/gluonnlp/logs/language_model/xlnet_finetune_squad2.0_large_mx1.6.0rc1_null.json>`__           |
+-----------+---------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

For xlnet_24_1024_16, we used hyperparameters reported by the paper authors.


To get the score of the dev data, you need to download the evaluate script (`evaluate-2.0.py <https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/>`_).
You can either put the evaluate script under the same folder with run_squad.py to let our script run it automatically,
or run it manually by yourself. To run the evaluate script, you can use the following commands:

SQuAD1.1:

.. code-block:: console

    $ python evaluate-v2.0.py dev-v2.0.json predictions.json

SQuAD2.0:

.. code-block:: console

    $ python evaluate-v2.0.py dev-v2.0.json predictions.json --na-prob-file null_odds.json
